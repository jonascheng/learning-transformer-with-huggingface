{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {},
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# transformers not support NumPy 2.0 yet\n",
    "!pip install -q numpy~=1.26.4 transformers~=4.46.2 accelerate~=1.1.1\n",
    "# for facebook/mask2former-swin-small-coco-instance\n",
    "!pip install -q pillow~=11.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 檢查是否有 GPU 可以使用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face 初體驗\n",
    "\n",
    "[Hugging Face](https://huggingface.co/) 是一個專注於機器學習和自然語言處理的開源平台。它提供了大量的模型、數據集和工具，支持多種任務如文本生成、圖像分類和語音識別。用戶可以輕鬆地搜索、下載和使用這些資源來開發和部署AI應用。這個平台對研究人員和開發者來說非常有用，因為它促進了技術共享和協作。\n",
    "\n",
    "平台提供各類開源模型及資料集，也針對不同型態的[任務](https://huggingface.co/tasks)進行分類，便於查找使用．\n",
    "\n",
    "![Hugging Face](https://raw.githubusercontent.com/jonascheng/learning-transformer-with-huggingface/refs/heads/main/assets/images/huggingface.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們來體驗幾個簡單的任務\n",
    "\n",
    "## [Token Classification](https://huggingface.co/tasks/token-classification)\n",
    "\n",
    "Token classification 是自然語言處理中的一種任務，旨在對文本中的每個 token（通常是單詞或子詞）進行分類。這種技術可以用來解決多種問題，以下是幾個例子：\n",
    "\n",
    "* 命名實體識別（NER）：識別文本中的人名、地名、組織名等實體。例如，從句子「John is working in New York」中識別出「John」是人名，「New York」是地名。\n",
    "* 詞性標註（POS tagging）：標註每個單詞的詞性，如名詞、動詞、形容詞等。例如，從句子「The quick brown fox jumps over the lazy dog」中標註出「quick」是形容詞，「jumps」是動詞。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonas/.pyenv/versions/learning-transformer-with-huggingface-3.11.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER',\n",
       "  'score': 0.99778074,\n",
       "  'index': 1,\n",
       "  'word': 'John',\n",
       "  'start': 0,\n",
       "  'end': 4},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9995432,\n",
       "  'index': 5,\n",
       "  'word': 'New',\n",
       "  'start': 19,\n",
       "  'end': 22},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.999273,\n",
       "  'index': 6,\n",
       "  'word': 'York',\n",
       "  'start': 23,\n",
       "  'end': 27}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 讓 pipeline 自動選擇模型\n",
    "ner_classifier = pipeline(\n",
    "  task=\"token-classification\",\n",
    "  device=device,)\n",
    "ner_classifier(\"John is working in New York\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "從回傳的結構化資料，我們可以看到每個 token 的 `word` 和 `entity`，這裡的 `entity` 是指該 token 的分類結果。\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    'entity': 'I-PER',\n",
    "    'word': 'John'\n",
    "  }, {\n",
    "    'entity': 'I-LOC',\n",
    "    'word': 'New'\n",
    "  }, {\n",
    "    'entity': 'I-LOC',\n",
    "    'word': 'York'\n",
    "  }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vblagoje/bert-english-uncased-finetuned-pos were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'DET',\n",
       "  'score': 0.9993881,\n",
       "  'index': 1,\n",
       "  'word': 'the',\n",
       "  'start': 0,\n",
       "  'end': 3},\n",
       " {'entity': 'ADJ',\n",
       "  'score': 0.9961695,\n",
       "  'index': 2,\n",
       "  'word': 'quick',\n",
       "  'start': 4,\n",
       "  'end': 9},\n",
       " {'entity': 'ADJ',\n",
       "  'score': 0.9015418,\n",
       "  'index': 3,\n",
       "  'word': 'brown',\n",
       "  'start': 10,\n",
       "  'end': 15},\n",
       " {'entity': 'NOUN',\n",
       "  'score': 0.99535537,\n",
       "  'index': 4,\n",
       "  'word': 'fox',\n",
       "  'start': 16,\n",
       "  'end': 19},\n",
       " {'entity': 'VERB',\n",
       "  'score': 0.99944025,\n",
       "  'index': 5,\n",
       "  'word': 'jumps',\n",
       "  'start': 20,\n",
       "  'end': 25},\n",
       " {'entity': 'ADP',\n",
       "  'score': 0.99929345,\n",
       "  'index': 6,\n",
       "  'word': 'over',\n",
       "  'start': 26,\n",
       "  'end': 30},\n",
       " {'entity': 'DET',\n",
       "  'score': 0.9995018,\n",
       "  'index': 7,\n",
       "  'word': 'the',\n",
       "  'start': 31,\n",
       "  'end': 34},\n",
       " {'entity': 'ADJ',\n",
       "  'score': 0.9978807,\n",
       "  'index': 8,\n",
       "  'word': 'lazy',\n",
       "  'start': 35,\n",
       "  'end': 39},\n",
       " {'entity': 'NOUN',\n",
       "  'score': 0.9988996,\n",
       "  'index': 9,\n",
       "  'word': 'dog',\n",
       "  'start': 40,\n",
       "  'end': 43}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 詞性標註（POS tagging）, 指定模型\n",
    "model_name = \"vblagoje/bert-english-uncased-finetuned-pos\"\n",
    "pos_classifier = pipeline(\n",
    "  task=\"token-classification\",\n",
    "  model=model_name,\n",
    "  device=device,)\n",
    "pos_classifier(\"The quick brown fox jumps over the lazy dog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "[\n",
    "  {\n",
    "    'entity': 'DET', # 限定詞\n",
    "    'word': 'the'\n",
    "  }, {\n",
    "    'entity': 'ADJ', # 形容詞\n",
    "    'word': 'quick'\n",
    "  }, {\n",
    "    'entity': 'ADJ',\n",
    "    'word': 'brown'\n",
    "  }, {\n",
    "    'entity': 'NOUN', # 名詞\n",
    "    'word': 'fox'\n",
    "  }, {\n",
    "    'entity': 'VERB', # 動詞\n",
    "    'word': 'jumps'\n",
    "  }, {\n",
    "    'entity': 'ADP', # 介系詞\n",
    "    'word': 'over'\n",
    "  },\n",
    "  ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Zero-Shot Classification](https://huggingface.co/tasks/zero-shot-classification)\n",
    "\n",
    "Zero-Shot Classification 旨在不需要特定訓練數據的情況下，對文本進行分類。這意味著模型可以在沒有見過特定類別的訓練樣本的情況下，直接對新類別進行分類。這種技術依賴於預訓練模型的強大語言理解能力。以下是幾個可以透過 Zero-Shot Classification 解決的例子：\n",
    "\n",
    "* 情感分析：在沒有特定情感標註數據的情況下，對文本進行正面、負面或中性情感分類。例如，對一條新的社交媒體評論進行情感分類。\n",
    "* 主題分類：對新聞文章進行主題分類，如政治、經濟、科技等，而不需要針對每個主題進行單獨訓練。例如，將一篇新的新聞文章分類為「科技」。\n",
    "\n",
    "Zero-Shot Classification 的優勢在於其靈活性和廣泛應用，特別是在缺乏標註數據的情況下。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'I have a problem with my iphone that needs to be resolved asap!',\n",
       " 'labels': ['urgent', 'phone', 'computer', 'not urgent', 'tablet'],\n",
       " 'scores': [0.5227580666542053,\n",
       "  0.45813998579978943,\n",
       "  0.014264924451708794,\n",
       "  0.002684991341084242,\n",
       "  0.00215207040309906]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"facebook/bart-large-mnli\"\n",
    "pipe = pipeline(\n",
    "  task=\"zero-shot-classification\",\n",
    "  model=model_name,\n",
    "  device=device,)\n",
    "pipe(\"I have a problem with my iphone that needs to be resolved asap!\",\n",
    "    candidate_labels=[\"urgent\", \"not urgent\", \"phone\", \"tablet\", \"computer\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "從回傳的結構化資料，我們可以知道這是一個關於 `phone` 且 `urgent` 的用戶反饋。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Image Classification](https://huggingface.co/tasks/image-classification)\n",
    "\n",
    "Transformer 架構不僅適用於自然語言處理，也可以應用於圖像處理。這裡我們使用一個預訓練的圖像分類模型來識別圖像中的物體。\n",
    "\n",
    "![貓](https://live.staticflickr.com/5670/30942263876_969b93e461_b.jpg)\n",
    "\n",
    "來源: https://live.staticflickr.com/5670/30942263876_969b93e461_b.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'Egyptian cat', 'score': 0.6866689324378967},\n",
       " {'label': 'tabby, tabby cat', 'score': 0.12515884637832642},\n",
       " {'label': 'tiger cat', 'score': 0.0503462590277195},\n",
       " {'label': 'bow tie, bow-tie, bowtie', 'score': 0.027289235964417458},\n",
       " {'label': 'sliding door', 'score': 0.02348732203245163}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"google/vit-base-patch16-224\"\n",
    "pipe = pipeline(\n",
    "  task=\"image-classification\",\n",
    "  model=model_name,\n",
    "  device=device,)\n",
    "pipe(\"https://live.staticflickr.com/5670/30942263876_969b93e461_b.jpg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果對於 Vision Transformer (ViT) 與 CNN 感興趣，可以參考 [Vision Transformer vs. CNN: A Comparison of Two Image Processing Giants](https://medium.com/@hassaanidrees7/vision-transformer-vs-cnn-a-comparison-of-two-image-processing-giants-d6c85296f34f) 這篇文章。課程將專注在文本處理，所以這裡只是簡單展示圖像分類的例子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本生成策略\n",
    "\n",
    "文本生成對於許多自然語言處理（NLP）任務至關重要，例如開放式文本生成 (open-ended text generation)、摘要、翻譯等。它還在許多以文本為輸出的混合模態 (mixed-modality) 應用中發揮作用，如語音轉文本和圖像轉文本。一些常見可以生成文本的模型包括 GPT2、OpenAI GPT、Bart、T5 等。\n",
    "\n",
    "請注意，`generate()` 方法的輸入取決於模型的模態。這些輸入由模型的預處理類（如 `AutoTokenizer` 或 `AutoProcessor`）返回。如果模型的預處理器創建了多種類型的輸入，請將所有輸入傳遞給 `generate()`。\n",
    "\n",
    "選擇生成文本的輸出標記的過程稱為解碼 (decoding)，您可以自定義 `generate()` 方法使用的解碼策略。修改解碼策略不會改變任何可訓練參數的值。然而，它可以顯著影響生成輸出的質量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 預設文本生成策略\n",
    "\n",
    "模型的解碼 (decoding) 策略在其生成配置中定義。當在 `pipeline()` 中使用預訓練模型進行推理時，這些模型會調用 `PreTrainedModel.generate()` 方法，該方法在內部使用默認的生成配置。如果沒有與模型一起保存自定義配置，則使用默認配置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "/Users/jonas/.pyenv/versions/learning-transformer-with-huggingface-3.11.11/lib/python3.11/site-packages/transformers/pytorch_utils.py:325: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Once upon a time, I did not want someone to stop me, so this was the chance.\\nAfter hearing the word \\u200dO\\u200c \\u200dO\\u200c I came across a woman I was holding around my neck. I thought she'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"distilbert/distilgpt2\"\n",
    "\n",
    "pipe = pipeline(\n",
    "  task=\"text-generation\",\n",
    "  model=model_name,\n",
    "  device=device,)\n",
    "\n",
    "pipe(\"Once upon a time\")[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "當您顯式加載模型時，可以通過 `model.generation_config` 檢查隨模型附帶的生成配置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"eos_token_id\": 50256\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generation_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model.generation_config` 只會顯示與默認生成配置不同的值，不會列出任何默認值。\n",
    "\n",
    "默認生成配置為\n",
    "\n",
    "* 輸出與輸入提示結合的大小限制為最多 20 個標記，以避免資源限制。\n",
    "\n",
    "* 默認的解碼策略是貪婪搜索 (greedy search)，這是最簡單的解碼策略，選擇概率最高的標記作為下一個標記。對於許多任務和小量輸出，這種方法效果很好。然而，當用於生成較長的輸出時，貪婪搜索可能會開始產生高度重複的結果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "預處理輸入時，`AutoTokenizer` 會自動檢測模型的模態 (modality)，並返回適當的輸入。這些輸入可以是 `input_ids`、`attention_mask`、`decoder_input_ids`、`decoder_attention_mask` 等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer(\"Once upon a time\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1, 1]]),\n",
      " 'input_ids': tensor([[7454, 2402,  257,  640]])}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這些輸入可以直接傳遞給 `generate()` 方法，生成文本的輸出將基於模型的模態和生成策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "/Users/jonas/.pyenv/versions/learning-transformer-with-huggingface-3.11.11/lib/python3.11/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7454, 2402,  257,  640,  286, 1175,   11,  262, 1578, 1829,  373,  262,\n",
      "          691, 1499,  287,  262,  995,  284,  423,  257]])\n"
     ]
    }
   ],
   "source": [
    "pprint(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "輸出的文本是編碼後的標記，您可以使用模型的 `tokenizer` 將其解碼為人類可讀的文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Once upon a time of war, the United States was the only country in the world to have a'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 客製化文本生成策略\n",
    "\n",
    "您可以通過將參數及其值直接傳遞給 `generate()` 方法來覆蓋任何 `generation_config`：\n",
    "\n",
    "* `num_beams`：表示使用束搜索（beam search）策略進行生成，並且束的數量為 4。束搜索是一種解碼策略，它在每一步選擇多個（這裡是 4 個）最有希望的候選標記，並在最終選擇最佳序列。這可以提高生成文本的質量和多樣性。\n",
    "\n",
    "* `do_sample`：表示在生成過程中啟用隨機抽樣。這意味著在每一步生成時，模型會根據概率分佈隨機選擇下一個標記，而不是總是選擇概率最高的標記。這可以增加生成文本的多樣性，減少重複和模式化的結果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(**inputs, num_beams=4, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Once upon a time when it was just a matter of time before we could do anything about it,'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
