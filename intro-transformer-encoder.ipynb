{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {},
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# transformers not support NumPy 2.0 yet\n",
    "!pip install -q numpy~=1.26.4 transformers~=4.46.2\n",
    "# for visualization self-attention\n",
    "!pip install -q bertviz~=1.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Encoder\n",
    "\n",
    "Transformer 由 N 個 Encoder 組成，每個 Encoder 將其輸出送到下一個 Encoder。最終的 Encoder 返回 **輸入** 的 **向量** 表示。為了說明，從現在開始，我們將使用 N=2 的值。\n",
    "\n",
    "![encoder in a nutshell](https://www.alexisalulema.com/wp-content/uploads/2022/08/encoders.png)\n",
    "\n",
    "每個解碼器區塊由兩個子層組成：\n",
    "\n",
    "1. 多頭注意力 (Multi-head attention)\n",
    "2. 前饋神經網路 (Feedforward Network)\n",
    "\n",
    "![](https://www.alexisalulema.com/wp-content/uploads/2022/08/encoders.inside-768x315.png)\n",
    "\n",
    "在開始解釋這兩個組件之前，有必要先了解自注意力 (self-attention) 機制。\n",
    "\n",
    "#### Self-Attention\n",
    "\n",
    "考慮以下句子：\n",
    "\n",
    "```\n",
    "John and Paul wrote several songs when they were inspired.\n",
    "```\n",
    "\n",
    "在這個句子中，自注意力機制計算每個詞的表示，並且與句子中其他詞的關係提供了更多關於該詞的信息。例如，「they」這個詞應該與「John」和「Paul」相關，而不是與「songs」相關。\n",
    "\n",
    "讓我們簡單以一個視覺化的方式來解釋自注意力機制：\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pprint import pprint\n",
    "\n",
    "model_name = \"google-bert/bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "text = \"John and Paul wrote several songs when they were inspired.\"\n",
    "# Tokenize input text\n",
    "text_to_token_id = tokenizer(\n",
    "    text,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_name,\n",
    "    output_attentions=True, # Whether the model returns attentions weights\n",
    "    )\n",
    "\n",
    "# Run the text through BERT\n",
    "with torch.no_grad():\n",
    "    outputs = model(**text_to_token_id)\n",
    "    # Retrieve attention from model outputs\n",
    "    attention = outputs.attentions\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(text_to_token_id['input_ids'][0])  # Convert input ids to token strings\n",
    "pprint(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from bertviz import (\n",
    "  model_view,\n",
    "  head_view,\n",
    ")\n",
    "\n",
    "# Visualize the self-attention of the model\n",
    "# Review the relationship of term 'they' along with 'John' and 'Paul'\n",
    "head_view(\n",
    "  attention,\n",
    "  tokens,\n",
    "  layer=8,\n",
    "  heads=[9],\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以一個更簡單的例子「The sky is blue」來理解自注意力機制如何運作，編碼器 (Encoder) 接收句子中每個詞的維度為 $\\ d_{model} = 512 $ 的 Word Embedding 向量，例如：\n",
    "\n",
    "$\\ x_1 = [3.23, 0.65, ..., 4.78] \\Rightarrow \\text{\"The\"} $\n",
    "\n",
    "$\\ x_2 = [1.26, 6.35, ..., 7.99] \\Rightarrow \\text{\"sky\"} $\n",
    "\n",
    "$\\ x_3 = [9.25, 1.68, ..., 4.26] \\Rightarrow \\text{\"is\"} $\n",
    "\n",
    "$\\ x_4 = [6.84, 2.98, ..., 11.48] \\Rightarrow \\text{\"blue\"} $\n",
    "\n",
    "有了這些向量，我們可以組裝 Embedding 矩陣 $\\ X $，其維度為$\\ d = [4 \\times 512] $：\n",
    "\n",
    "![](https://alexisalulema.com/wp-content/uploads/2022/08/embedding_matrix.gif)\n",
    "\n",
    "我們將從這個矩陣 $\\ X $ 中創建三個額外的矩陣，作為「自注意力機制」的一部分：\n",
    "\n",
    "* $\\ Q $, query matrix\n",
    "* $\\ K $, key matrix\n",
    "* $\\ V $, value matrix\n",
    "\n",
    "要創建這些陣列，我們還需要三個新的權重矩陣 (weight matrices)。\n",
    "\n",
    "原始論文中使用的維度是 $\\ d_k = 64 $；因此，權重向量的維度將是 $\\ d_{model} \\times d_k \\Rightarrow 512 \\times 64 $，這些矩陣會用隨機值初始化：\n",
    "\n",
    "* $\\ W^Q $, query weight matrix\n",
    "* $\\ W^K $, key weight matrix\n",
    "* $\\ W^V $, value weight matrix\n",
    "\n",
    "權重矩陣攜帶了在訓練過程中學到的最佳值，因此每個矩陣 $\\ Q $、$\\ K $ 和 $\\ V $ 是 Embedding 矩陣 $\\ X $ 與相應權重矩陣的乘積，這會生成 $\\ 4 \\times 64 $ 的矩陣：\n",
    "\n",
    "* $\\ Q = X × W^Q $\n",
    "* $\\ K = X × W^K $\n",
    "* $\\ V = X × W^V $ \n",
    "\n",
    "![](https://alexisalulema.com/wp-content/uploads/2022/08/QKV.gif)\n",
    "\n",
    "每個矩陣中的四行分別代表句子「The sky is blue」中的每個詞。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Self-attention Mechanism Process\n",
    "\n",
    "1. 計算點積 (dot product) $\\ Q \\cdot K^T $\n",
    "\n",
    "![](https://www.alexisalulema.com/wp-content/uploads/2022/08/QdotKT-1-1024x140.gif)\n",
    "\n",
    "結果陣列的元素表示詞語之間的關係。例如，$\\ q_1 \\cdot k_1 $ 是詞「The」與其自身的關係，$\\ q_1 \\cdot k_3 $ 是「The」與「is」之間的關係。「sky」與「blue」之間的關係 $\\ q_2 \\cdot k_4 $ 會有稍高的值，因為名詞和形容詞之間存在關係。例如：\n",
    "\n",
    "![](https://alexisalulema.com/wp-content/uploads/2022/08/QdotKT.2.gif)\n",
    "\n",
    "這樣，我們可以說計算查詢矩陣 $\\ Q $ 和鍵矩陣 $\\ K^T $ 之間的點積，基本上給出了相似度值，這有助於我們理解句子中每個詞與所有其他詞的相似程度。\n",
    "\n",
    "2. 計算 $\\ QK^T / \\sqrt{d_k} $。這個操作有助於獲得穩定的梯度，其中 $\\ d_k = 64 $ 是鍵向量的維度。\n",
    "\n",
    "![](https://alexisalulema.com/wp-content/uploads/2022/08/QdotKTSqrtDK.gif)\n",
    "\n",
    "結果矩陣的值必須正規化，如果我們使用函數 $\\ \\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) $，我們可以將值轉化為 0 到 1 的範圍。\n",
    "\n",
    "每行的值之和等於 1。通過這些值，我們可以理解句子中每個詞與所有其他詞的關係。這稱為分數矩陣 (score matrix)。\n",
    "\n",
    "3. 接下來，我們需要計算注意力矩陣 (attention matrix) $\\ Z $：\n",
    "\n",
    "![](https://www.alexisalulema.com/wp-content/uploads/2022/08/SoftmaxV.gif)\n",
    "\n",
    "![](https://www.alexisalulema.com/wp-content/uploads/2022/08/Z.SoftmaxV-1024x148.gif)\n",
    "\n",
    "注意力矩陣 $\\ Z $ 是一個具有 4 行和 512 列的矩陣，對於示例句子來說。每行對應於相應詞的自注意力 (self-attention) 向量。\n",
    "\n",
    "自注意力 (self-attention) 機制被稱為縮放點積注意力 (scaled dot product attention)，因為我們在計算向量 $\\ Q $ 和 $\\ K $ 之間的點積並通過 $\\ \\sqrt{d_k} $ 來縮放值。\n",
    "\n",
    "我們也可以透過視覺化的方式來理解自注意力機制中 $\\ Ｑ $ 和 $\\ Ｋ $ 的作用：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from bertviz.transformers_neuron_view import BertModel, BertTokenizer\n",
    "from bertviz.neuron_view import show\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "  'bert-base-uncased',\n",
    "  do_lower_case=True)\n",
    "model = BertModel.from_pretrained(\n",
    "  'bert-base-uncased',\n",
    "  output_attentions=True)\n",
    "show(\n",
    "  model=model,\n",
    "  model_type='bert',\n",
    "  tokenizer=tokenizer,\n",
    "  sentence_a=text,\n",
    "  layer=8,\n",
    "  head=9,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-head attention\n",
    "\n",
    "對於 Transformer，我們將計算多個注意力 (multiple attention) 矩陣。但為什麼我們需要多個陣列呢？這有助於在語境中詞義模糊的情況下，例如：\n",
    "\n",
    "```\n",
    "Tom was crying because he was blue.\n",
    "```\n",
    "\n",
    "單一的注意力機制可能會決定 Tom 哭泣是因為他的顏色是藍色，這是由於「Tom」這個詞的影響。如果大多數句子中「blue」表示顏色，只有一個「注意力頭 (attention head)」的機制將正確地學習到它是一種顏色。然而，擁有「多個注意力頭 (multiple attention heads)」，其中一個注意力機制更有可能從句子中學習到「blue」表示心情，通過串聯「多個注意力頭 (multiple attention heads)」的結果，注意力矩陣將更加準確。\n",
    "\n",
    "我們如何計算多個注意力矩陣？假設我們要計算兩個注意力矩陣：$\\ Z_1 $ 和 $\\ Z_2 $。\n",
    "\n",
    "要計算 $\\ Z_1 $，首先，我們創建三個矩陣 $\\ Q_1 $、$\\ K_1 $ 和 $\\ V_1 $，這意味著將 Embedding 矩陣與三個權重矩陣 $\\ W_1^Q $、$\\ W_1^K $ 和 $\\ W_1^V $ 相乘。現在，注意力矩陣的計算方式如下：\n",
    "\n",
    "$\\ Z_1 = \\text{Softmax}\\left(\\frac{Q_1K_1^T}{\\sqrt{d_k}} \\cdot V_1 \\right) $\n",
    "\n",
    "同樣地，對於 $\\ Z_2 $ 也是如此。\n",
    "\n",
    "$\\ Z_2 = \\text{Softmax}\\left(\\frac{Q_2K_2^T}{\\sqrt{d_k}} \\cdot V_2 \\right) $\n",
    "\n",
    "這樣，我們可以計算任意數量的注意力矩陣。假設我們需要八個注意力矩陣，在《[Attention is all you need](https://arxiv.org/abs/1706.03762?context=cs)》中提到的數值。在這種情況下，我們可以將所有的注意力頭拼接 (concatenate) 起來，並將結果乘以一個新的權重矩陣 $\\ W_0 $，該矩陣經過訓練以表示注意力機制的最佳值。\n",
    "\n",
    "Multi-head attention $\\ = Concatenate( Z_1, Z_2, Z_3, Z_4, Z_5, Z_6, Z_7, Z_8 ) \\cdot W_0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feedforward Network\n",
    "\n",
    "Feedforward Network 由兩個帶有 ReLU 激活的全連接層組成。\n",
    "\n",
    "另一個用於連接輸入和編碼器 (Encoder) 的組件是 **Add and norm** 組件，這是一種連接，隨後進行 **layer normalization**。\n",
    "\n",
    "![](https://www.alexisalulema.com/wp-content/uploads/2022/08/addNnorm.png)\n",
    "\n",
    "通過 Layer normalization 防止每層中的值發生劇烈變化來實現更快的訓練。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
