{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# transformers not support NumPy 2.0 yet\n",
    "!pip install -q numpy~=1.26.4 transformers~=4.46.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer 深入淺出\n",
    "\n",
    "Google Research 和 Google Brain 的成員最初在 2017 年的論文《[Attention is all you need](https://arxiv.org/abs/1706.03762?context=cs)》中提出了 Transformer。目前是 NLP 中最受歡迎的架構之一。\n",
    "\n",
    "Transformer 由 Encoder 和 Decoder 两部分组成，Encoder 接受 **输入** 然後生成 **向量** 表示這個输入，Decoder 接受 **向量** 然後逐一生成 **输出**。\n",
    "\n",
    "![transformer in a nutshell](https://www.alexisalulema.com/wp-content/uploads/2022/08/encoder-decoder-1-1024x275.png)\n",
    "\n",
    "Transformer 超越了基於 LSTM 和 RNN 的其他現有架構，獲得了更高評價和更快的訓練速度。Transformer 的注意力機制是一種「word-to-word」的操作，它會找出序列中每個詞與其他詞之間的關係，包括詞本身。\n",
    "\n",
    "![Original Transformer Architecture, Source: Attention is all you need](https://www.alexisalulema.com/wp-content/uploads/2022/08/image-695x1024.png)\n",
    "\n",
    "在本文中，我將以最簡單的方式解釋 Transformer 的架構和內部運作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 認識分詞器 (Tokenizer)\n",
    "\n",
    "分詞器負責為模型準備輸入。大多數分詞器有兩種版本：完整的 Python 實現和基於 Rust 庫的“快速”實現。“快速”實現具有以下優點：\n",
    "\n",
    "1. 特別是在批量分詞時顯著加速；\n",
    "\n",
    "2. 提供額外的方法在原始字符串（字符和單詞）和標記空間之間進行映射（例如，獲取包含給定字符的標記索引或對應於給定標記的字符範圍）。\n",
    "\n",
    "後面的課程會利用到這兩項優點，暫時無須深入了解。\n",
    "\n",
    "分詞器 (Tokenizer) 通過將文本正規化來將輸入轉換為 token；此外，它還會提供一個整數表示（基於現有詞彙），該表示將用於 Embedding 過程。例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入 BERT tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from pprint import pprint\n",
    "\n",
    "model_name = \"google-bert/bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizater 包含以下重要資訊\n",
    "\n",
    "* `vocab_size`：詞彙表的大小\n",
    "\n",
    "* `model_max_length`：輸入到模型的最大標記數\n",
    "\n",
    "* 特殊的 token，如\n",
    "\n",
    "  * 0: [PAD]\n",
    "  * 100: [UNK]\n",
    "  * 101: [CLS]\n",
    "  * 102: [SEP]\n",
    "  * 103: [MASK]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_szie: 30522\n",
    "# model_max_length: 512\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 字典 (Vocabulary)\n",
    "\n",
    "顯示部分字典內容，從第 1000 個詞開始到 1010 個詞："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顯示挑選的 10 個字\n",
    "pprint(list(tokenizer.vocab.keys())[1000:1010], compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 標記 (Token)\n",
    "\n",
    "標記是分詞器的輸出，它是一個整數，代表字典中的一個詞。標記是模型的輸入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a this course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "# 將原始文字轉換成標記\n",
    "inputs = tokenizer(\n",
    "  raw_inputs,\n",
    "  padding=True,         # Activates and controls padding\n",
    "  truncation=True,      # Activates and controls truncation\n",
    "  return_tensors=\"pt\",  # Return PyTorch tensor objects\n",
    ")\n",
    "\n",
    "pprint(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 第二句子長度不夠，所以會被補上 [PAD] token\n",
    "2. Attention mask 會標記出哪些是 padding 的 token\n",
    "\n",
    "我們也可以將標記轉換回原始文本，這樣我們就可以看到模型的輸入是什麼樣子的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(inputs[\"input_ids\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多語分詞器 (Multilingual Tokenizer)\n",
    "\n",
    "如果將此模型應用在中文文本上，會發生什麼狀況呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_inputs = [\n",
    "    \"我門正在學習目前正夯的變形金剛模型！\",\n",
    "    \"你們喜歡這個課程嗎？\"\n",
    "]\n",
    "# 將原始文字轉換成標記\n",
    "inputs = tokenizer(\n",
    "  raw_inputs,\n",
    "  padding=True,         # Activates and controls padding\n",
    "  truncation=True,      # Activates and controls truncation\n",
    "  return_tensors=\"pt\",  # Return PyTorch tensor objects\n",
    "  )\n",
    "# 將標記轉換回文字\n",
    "tokenizer.decode(inputs[\"input_ids\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由於 Tokenizer 是基於英文訓練的，所以它無法正確處理中文文本。這是因為中文文本的分詞方式與英文不同。因此，我們需要使用中文分詞器來處理中文文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入 Multilingual BERT tokenizer\n",
    "model_name = \"google-bert/bert-base-multilingual-uncased\"\n",
    "multilingual_tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_szie: 30522 > 105879\n",
    "# model_max_length: 512\n",
    "multilingual_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將原始文字轉換成標記\n",
    "multilingual_inputs = tokenizer(\n",
    "  raw_inputs,\n",
    "  padding=True,         # Activates and controls padding\n",
    "  truncation=True,      # Activates and controls truncation\n",
    "  return_tensors=\"pt\",  # Return PyTorch tensor objects\n",
    ")\n",
    "# 將標記轉換回文字\n",
    "multilingual_tokenizer.decode(multilingual_inputs[\"input_ids\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Embedding\n",
    "\n",
    "Transformer 架構首先是 Input Embedding 層，它將輸入序列轉換為維度 $\\ d_{model} = 512 $ 的向量。\n",
    "\n",
    "![Input Embedding](https://www.alexisalulema.com/wp-content/uploads/2022/08/input.embedding.png)\n",
    "\n",
    "> $\\ d_{model} = 512 $ 的值是由架構設計者設定的，這個維度的值可以根據目標進行修改。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分詞器 (Tokenizer)\n",
    "\n",
    "Embedding 從分詞後的文本中獲取數據。對於每個詞，生成一個大小為 $\\ d_{model} = 512 $ 的向量。對於意思接近的詞，Embedding 向量應該是相似的。\n",
    "\n",
    "透過一個小實驗來驗證這個假設：\n",
    "\n",
    "我們給定一句範例文本，包含多語義的文字 `bank`，這個詞有兩種不同的意思：\n",
    "\n",
    "```\n",
    "After stealing money from the bank vault, the bank robber was seen\n",
    "fishing on the Mississippi river bank.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 給定一句範例文本，包含多語義的文字 'bank'\n",
    "text = '''\n",
    "After stealing money from the bank vault, the bank robber was seen\n",
    "fishing on the Mississippi river bank.\n",
    "'''\n",
    "\n",
    "# 將原始文字轉換成標記\n",
    "tokenized_inputs = tokenizer(\n",
    "    text,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將標記轉換回原始文本，我們可以看到 `bank` 這個詞在兩個不同的上下文中卻有相同的標記。請注意，這裡的 token id 是對應到 vocab 的 index，此時 `bank` 這個字具有相同的 token id。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the words with their indeces.\n",
    "for id in tokenized_inputs[\"input_ids\"][0]:\n",
    "    pprint('{:<12} {:>6,}'.format(tokenizer.decode(id), id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此刻，我們已經將原始文本轉換為標記，總計有 22 個標記。這些標記將被送入 Embedding 層。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 隱藏狀態（Hidden States）\n",
    "\n",
    "為了方便我們深入觀察模型，我們透過 `output_hidden_states=True` 參數來獲取模型的隱藏狀態（hidden states），隱藏狀態是模型在每一層計算出的中間表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "# 載入 BERT model\n",
    "model = AutoModel.from_pretrained(\n",
    "  model_name,\n",
    "  output_hidden_states=True,  # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# 這個 model 有 12 層的 transformer blocks, 及第一層的 embedding 層, 共 13 層\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下程式碼使用 BERT 模型處理文本，並收集所有層生成的隱藏狀態。這樣可以幫助我們了解模型的結構和輸出數據的維度。\n",
    "\n",
    "使用 `torch.no_grad()` 禁用梯度計算，這樣可以節省內存並加快計算速度，因為我們只需要前向傳播的結果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Run the text through BERT, and collect all of the hidden states produced from all 12 layers.\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokenized_inputs)\n",
    "    hidden_states = outputs.hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根據[文件](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel)，每一層的隱藏狀態的維度是 `(batch_size, sequence_length, hidden_size)`，其中 `hidden_size` 是模型的維度，這裡是 768。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_states = len(hidden_states)\n",
    "pprint(f\"Number of layers: {n_hidden_states} (initial embeddings + 12 BERT layers)\")\n",
    "layer_i = 0\n",
    "\n",
    "n_batches = len(hidden_states[layer_i])\n",
    "pprint(f\"Number of batches: {n_batches}\")\n",
    "batch_i = 0\n",
    "\n",
    "n_tokens = len(hidden_states[layer_i][batch_i])\n",
    "pprint(f\"Number of tokens: {n_tokens}\")\n",
    "token_i = 0\n",
    "\n",
    "n_hidden_units = len(hidden_states[layer_i][batch_i][token_i])\n",
    "pprint(f\"Number of hidden units: {n_hidden_units}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這個模型的完整隱藏狀態集，存儲在 `hidden_states` 物件中，這個物件有四個維度，順序如下：\n",
    "\n",
    "1. 層數（13 層）\n",
    "2. 批次數（1 句話）\n",
    "3. 詞/標記 (Token) 數（我們句子中的 22 個 Token）\n",
    "4. 隱藏單元/特徵數（768 個特徵）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(f\"Number of unique values: {n_hidden_states * n_batches * n_tokens * n_hidden_units}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "僅僅為了表示我們的一句話，就需要 219,648 個唯一值！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 詞嵌入 (Word Embedding)\n",
    "\n",
    "將所有層的張量連接起來，並創建一個新的維度來表示這些層。這樣我們就可以看到每個詞的所有層的隱藏狀態。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor.\n",
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "# 張量的尺寸（shape）\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "移除 `token_embeddings` 張量中指定維度 `dim=1` 的單維度。相當於移除第一個維度的 `batch_size`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s get rid of the “batches” dimension since we don’t need it.\n",
    "# Remove dimension 1, the \"batches\".\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current dimensions: [# layers, # tokens, # features]\n",
    "# Desired dimensions: [# tokens, # layers, # features]\n",
    "\n",
    "# Swap dimensions 0 and 1.\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "現在，我們該如何處理這些隱藏狀態呢？我們希望為每個標記 (Token) 獲取單獨的向量，但對於我們的每個輸入標記 (Token)，我們有 13 個長度為 768 的單獨向量。\n",
    "\n",
    "為了獲取單獨的向量，我們需要結合一些層向量，但是哪一層或哪幾層的組合能提供最佳表示呢？\n",
    "\n",
    "不幸的是，沒有一個簡單的答案，不過讓我們嘗試通過將最後四層相加來創建詞向量，為每個標記 (Token) 生成一個詞向量。\n",
    "\n",
    "每個向量的長度將是維持長度 768。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 13 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [13 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "pprint(f'Shape is: {len(token_vecs_sum)} x {len(token_vecs_sum[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "為了確認這些向量的值確實是依賴於上下文的，讓我們看看在我們示例句子中「bank」這個詞的不同實例：\n",
    "\n",
    "「After stealing money from the **bank vault**, the **bank robber** was seen fishing on the Mississippi **river bank**.」\n",
    "\n",
    "讓我們找出這個示例句子中三個「bank」實例的索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, id in enumerate(tokenized_inputs[\"input_ids\"][0]):\n",
    "    pprint(f'{i} {tokenizer.decode(id)}')\n",
    "\n",
    "# They are at 6, 10, and 19.\n",
    "# We can try printing out their vectors to compare them.\n",
    "pprint('First 5 vector values for each instance of \"bank\".')\n",
    "pprint('')\n",
    "pprint(f\"bank vault  {token_vecs_sum[6][:5]}\")\n",
    "pprint(f\"bank robber {token_vecs_sum[10][:5]}\")\n",
    "pprint(f\"river bank  {token_vecs_sum[19][:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that the values differ, but let’s calculate the cosine similarity between the vectors to make a more precise comparison.\n",
    "# Cosine similarity measures the cosine of the angle between two vectors in a high-dimensional space.\n",
    "# It is a value between -1 and 1, with 1 indicating that the two vectors are identical and 0 indicating that they are orthogonal (i.e., they have no correlation).\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Calculate the cosine similarity between the word bank\n",
    "# in \"bank robber\" vs \"bank vault\" (same meaning).\n",
    "same_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[6])\n",
    "\n",
    "# Calculate the cosine similarity between the word bank\n",
    "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
    "diff_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[19])\n",
    "\n",
    "pprint('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\n",
    "pprint('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding (PE)\n",
    "\n",
    "![](https://www.alexisalulema.com/wp-content/uploads/2022/08/positional.encoding.png)\n",
    "\n",
    "Embedding 向量為 Transformer 提供了大量有關序列中詞語之間關係的信息。然而，仍然需要信息來指示詞語在序列中的位置，這就是位置編碼過程的用途。\n",
    "\n",
    "初始序列中的每個詞都必須具有位置編碼（PE）信息，但由於 Transformer 的主要重點是注意力機制，因此這個向量的生成必須簡單。\n",
    "\n",
    "這項任務的挑戰在於為位置編碼函數的每個輸出向量生成一個維度為 $\\ d_{model} = 512 $ 的向量。架構的作者找到了一種巧妙的方法，使用正弦和餘弦值表示位置編碼。\n",
    "\n",
    "$\\ PE_{\\text{sin}}(\\text{pos}, 2i) = \\sin\\left(\\frac{\\text{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right) $\n",
    "\n",
    "$\\ PE_{\\text{cos}}(\\text{pos}, 2i+1) = \\cos\\left(\\frac{\\text{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right) $\n",
    "\n",
    "最後，詞編碼向量應該加到位置編碼向量上，然後將結果輸入到編碼器/解碼器區塊中：\n",
    "\n",
    "$\\ Embedding_{output} = Embedding_{initial} + PE_{word-position} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
