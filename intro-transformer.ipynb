{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# transformers not support NumPy 2.0 yet\n",
    "!pip install -q numpy~=1.26.4 transformers~=4.46.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer 深入淺出\n",
    "\n",
    "Google Research 和 Google Brain 的成員最初在 2017 年的論文《[Attention is all you need](https://arxiv.org/abs/1706.03762?context=cs)》中提出了 Transformer。目前是 NLP 中最受歡迎的架構之一。\n",
    "\n",
    "Transformer 由 Encoder 和 Decoder 两部分组成，Encoder 接受 **输入** 然後生成 **向量** 表示這個输入，Decoder 接受 **向量** 然後逐一生成 **输出**。\n",
    "\n",
    "![transformer in a nutshell](https://www.alexisalulema.com/wp-content/uploads/2022/08/encoder-decoder-1-1024x275.png)\n",
    "\n",
    "Transformer 超越了基於 LSTM 和 RNN 的其他現有架構，獲得了更高評價和更快的訓練速度。Transformer 的注意力機制是一種「word-to-word」的操作，它會找出序列中每個詞與其他詞之間的關係。\n",
    "\n",
    "![Original Transformer Architecture, Source: Attention is all you need](https://www.alexisalulema.com/wp-content/uploads/2022/08/image-695x1024.png)\n",
    "\n",
    "在本文中，將以最簡單的方式解釋 Transformer 的架構和內部運作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分詞器 (Tokenizer)\n",
    "\n",
    "首先，分詞器負責為模型輸入準備。大多數分詞器有兩種版本：完整的 Python 實現和基於 Rust 庫的“快速”實現。“快速”實現具有以下優點：\n",
    "\n",
    "1. 特別是在批量分詞時顯著加速；\n",
    "\n",
    "2. 提供額外的方法在原始字符串和標記空間之間進行映射（例如，獲取給定字符的標記索引或對應於給定標記的字符範圍）。\n",
    "\n",
    "後面的課程會利用到這兩項優點，暫時無須深入了解。\n",
    "\n",
    "分詞器 (Tokenizer) 通過將文本正規化來將輸入轉換為標記 (Token)；此外，它還會提供一個整數表示（基於現有詞彙庫），該表示將用於 Embedding 過程。\n",
    "\n",
    "我們先加載一個分詞器，然後觀察其包含的資訊："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonas/.pyenv/versions/learning-transformer-with-huggingface-3.11.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 載入 BERT tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from pprint import pprint\n",
    "\n",
    "model_name = \"google-bert/bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='google-bert/bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab_szie: 30522\n",
    "# model_max_length: 512\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizater 包含以下重要資訊\n",
    "\n",
    "* `vocab_size`：詞彙表的大小\n",
    "\n",
    "* `model_max_length`：輸入到模型的最大標記數\n",
    "\n",
    "* 特殊的 token，如\n",
    "\n",
    "  * 0: [PAD]\n",
    "  * 100: [UNK]\n",
    "  * 101: [CLS]\n",
    "  * 102: [SEP]\n",
    "  * 103: [MASK]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 詞彙庫 (Vocabulary)\n",
    "\n",
    "顯示詞彙庫部分內容，從第 1 個詞開始到第 10 個詞，而且有些字僅有子詞 (subword)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['piss', 'mom', '##石', 'instrumentation', 'cannabis', 'images', '[unused976]',\n",
      " '##rya', 'mohan', 'informal']\n"
     ]
    }
   ],
   "source": [
    "# 顯示挑選的前 10 個標記 (token)\n",
    "pprint(list(tokenizer.vocab.keys())[0:10], compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 標記 (Token)\n",
    "\n",
    "標記是分詞器的輸出，它是一個整數，代表字典中的一個詞。標記是 Embedding 層的輸入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]]),\n",
      " 'input_ids': tensor([[ 101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 2023, 2607, 2026, 2878,\n",
      "         2166, 1012,  102],\n",
      "        [ 101, 1045, 5223, 2023, 2061, 2172,  999,  102,    0,    0,    0,    0,\n",
      "            0,    0,    0]]),\n",
      " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a this course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "# 將原始文字轉換成標記\n",
    "inputs = tokenizer(\n",
    "  raw_inputs,\n",
    "  padding=True,         # Activates and controls padding\n",
    "  truncation=True,      # Activates and controls truncation\n",
    "  return_tensors=\"pt\",  # Return PyTorch tensor objects\n",
    ")\n",
    "\n",
    "pprint(inputs, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 因第二句子長度不夠，所以會被補上 [PAD] token\n",
    "2. Attention Mask 會註記哪些是 padding 的 token\n",
    "\n",
    "我們也可以將標記轉換回原始文本，這樣我們就可以看到模型的輸出是什麼樣子的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] i ' ve been waiting for a this course my whole life. [SEP]\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] i hate this so much! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs[\"input_ids\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中 [CLS] 和 [SEP] 是特殊 token，[CLS] 用於句子分類任務，[SEP] 用於分隔兩個句子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多語分詞器 (Multilingual Tokenizer)\n",
    "\n",
    "如果將此模型應用在中文文本上，會發生什麼狀況呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_inputs = [\n",
    "    \"我門正在學習目前正夯的變形金剛模型！\",\n",
    "    \"你們喜歡這個課程嗎？\"\n",
    "]\n",
    "# 將原始文字轉換成標記\n",
    "inputs = tokenizer(\n",
    "  raw_inputs,\n",
    "  padding=True,         # Activates and controls padding\n",
    "  truncation=True,      # Activates and controls truncation\n",
    "  return_tensors=\"pt\",  # Return PyTorch tensor objects\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由於 Tokenizer 是基於英文訓練的，所以它無法正確處理中文文本。這是因為中文文本的分詞方式與英文不同。因此，我們需要使用中文分詞器來處理中文文本。\n",
    "\n",
    "我們嘗試用多語分詞器來處理中文文本，看看會發生什麼事情。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入 Multilingual BERT tokenizer\n",
    "mbert_model_name = \"google-bert/bert-base-multilingual-uncased\"\n",
    "multilingual_tokenizer = AutoTokenizer.from_pretrained(mbert_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='google-bert/bert-base-multilingual-uncased', vocab_size=105879, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab_szie: 30522 > 105879\n",
    "# model_max_length: 512\n",
    "multilingual_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型的最大長度不變，但是詞彙庫的大小增加了。這是因為多語分詞器包含了中文詞彙。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將原始文字轉換成標記\n",
    "multilingual_inputs = multilingual_tokenizer(\n",
    "  raw_inputs,\n",
    "  padding=True,         # Activates and controls padding\n",
    "  truncation=True,      # Activates and controls truncation\n",
    "  return_tensors=\"pt\",  # Return PyTorch tensor objects\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 我 門 正 在 學 習 目 前 正 夯 的 變 形 金 剛 模 型 ！ [SEP]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 將標記轉換回文字\n",
    "multilingual_tokenizer.decode(multilingual_inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 你 們 喜 歡 這 個 課 程 嗎 ？ [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 將標記轉換回文字\n",
    "multilingual_tokenizer.decode(multilingual_inputs[\"input_ids\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Embedding\n",
    "\n",
    "Transformer 架構首先是 Input Embedding 層，它將輸入序列轉換為維度 $\\ d_{model} = 512 $ 的向量。\n",
    "\n",
    "![Input Embedding](https://www.alexisalulema.com/wp-content/uploads/2022/08/input.embedding.png)\n",
    "\n",
    "> $\\ d_{model} = 512 $ 的值是由架構設計者設定的，這個維度的值可以根據目標進行修改。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以我們使用的模型 `bert-base-uncased` 為例，它的 $\\ d_{model} = 768 $。對於每個唯一的標記 ID（即 BERT 分詞器詞彙表中的 30,522 個單詞和子詞），BERT 模型包含一個嵌入 (Embedding)，該嵌入被訓練來表示那個特定的標記。模型中的嵌入層負責將標記映射到它們對應的嵌入。\n",
    "\n",
    "![](https://cdn.prod.website-files.com/6064b31ff49a2d31e0493af1/66d06d2f219c5eab928c6b5b_AD_4nXdL2BUY6asFzNdhx_FYCFp6DNBRCwLx_XCALqjkUueNttpIa0WPQWRzUxSNvDBdyU6U3r5unc1OJu4-4DxbNAXeaXlS7Y9BW2-igGX91VVZHRSlwLYYw0dE0m5DPrw29A3RNnVp5hV9S3ljW7GcLYRMPaX0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertEmbeddings(\n",
       "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (token_type_embeddings): Embedding(2, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "# 載入 BERT model\n",
    "model = AutoModel.from_pretrained(\n",
    "  model_name,\n",
    ")\n",
    "\n",
    "# 取得 BERT embeddings\n",
    "model.embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型 `word_embeddings.weight` 屬性可以用來查看和單一對應嵌入層，該層的屬性包含標記嵌入（即 BERT 分詞器詞彙表中每個標記的嵌入）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0102, -0.0615, -0.0265,  ..., -0.0199, -0.0372, -0.0098],\n",
       "        [-0.0117, -0.0600, -0.0323,  ..., -0.0168, -0.0401, -0.0107],\n",
       "        [-0.0198, -0.0627, -0.0326,  ..., -0.0165, -0.0420, -0.0032],\n",
       "        ...,\n",
       "        [-0.0218, -0.0556, -0.0135,  ..., -0.0043, -0.0151, -0.0249],\n",
       "        [-0.0462, -0.0565, -0.0019,  ...,  0.0157, -0.0139, -0.0095],\n",
       "        [ 0.0015, -0.0821, -0.0160,  ..., -0.0081, -0.0475,  0.0753]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings.word_embeddings.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "雖然從表面上看不明顯，但每個標記 ID 的嵌入並不只是隨機數字。這些值是在訓練 BERT 模型時學習到的，這意味著每個嵌入都編碼了模型對該特定標記的理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 位置編碼向量 (Positional Encoding)\n",
    "\n",
    "![](https://www.alexisalulema.com/wp-content/uploads/2022/08/positional.encoding.png)\n",
    "\n",
    "除了前面描述的標記嵌入外，Transformer 還依賴於位置編碼 (Positional Encoding)。標記嵌入用於代表模型的每個可能的單詞或子詞，而位置嵌入則表示輸入序列中每個標記的位置。\n",
    "\n",
    "![](https://cdn.prod.website-files.com/6064b31ff49a2d31e0493af1/66d06d30496ac93f233f13a6_AD_4nXeVGAn7H8YRdRk0S9Z6PqBiiVaJ2Aykuca5ZA0rWsj7-IX9AJncsiGBZ2thKwnAxO64_fKmSrDv9dWiuonp-wSMEnVre34FMt3QjWdpwchjUXNfv0Ry23qYuvzYjmESaWiwHCUFsmqsQ0SyfLi6ub0syWEg.png)\n",
    "\n",
    "初始序列中的每個詞都必須具有位置編碼（PE）信息，但由於原論文的主要重點是注意力機制，因此這個向量的生成，作者找到了一種巧妙的方法，使用正弦和餘弦值表示位置編碼。\n",
    "\n",
    "$\\ PE_{\\text{sin}}(\\text{pos}, 2i) = \\sin\\left(\\frac{\\text{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right) $\n",
    "\n",
    "$\\ PE_{\\text{cos}}(\\text{pos}, 2i+1) = \\cos\\left(\\frac{\\text{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right) $\n",
    "\n",
    "以下是我們使用上述公式，計算第 3 個位置的位置編碼："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1411200080598672, -0.9899924966004454, 0.27837998117302903,\n",
      " -0.9604710230309418, 0.40414136603532824, -0.9146965377976998,\n",
      " 0.517305716423722, -0.8558006752482378, 0.6173581934738708,\n",
      " -0.7866821854794215, 0.704246484600274, -0.7099555541920608,\n",
      " 0.7782725224195125, -0.6279266524418035, 0.8399987525023596,\n",
      " -0.5425883299468205, 0.8901692262613478, -0.4556300567535832,\n",
      " 0.9296448405538298, -0.3684568773026832, 0.9593514800610861,\n",
      " -0.282213992751252, 0.9802395338823054, -0.19781419618975907,\n",
      " 0.993253167134793, -0.11596614150993839, 0.999307764654284,\n",
      " -0.03720203625688118, 0.9992740773014782, 0.03809617347292795,\n",
      " 0.9939677561484435, 0.10967269367180021, 0.9841431312738992,\n",
      " 0.17737614599039198, 0.9704902638019913, 0.24114030742607345,\n",
      " 0.9536344621455892, 0.3009672949147346, 0.9341376005608856, 0.3569130751574553,\n",
      " 0.9125007075335518, 0.40907512604748686, 0.8891674026610501,\n",
      " 0.45758204733687047, 0.8645278542932224, 0.5025849074048651,\n",
      " 0.8389230076891356, 0.5442501145335796, 0.8126488966420368, 0.5827536107022249,\n",
      " 0.7859609023261889, 0.6182762004271254, 0.759077863396066, 0.6509998443180024,\n",
      " 0.732185972853175, 0.6811047651845122, 0.7054424214422146, 0.7087672326157192,\n",
      " 0.6789787657027085, 0.7341579092571477, 0.652904012444876, 0.7574406580936761,\n",
      " 0.6273074213151105, 0.7787717246812357, 0.6022610340763316, 0.7982992213658409,\n",
      " 0.5778219439143785, 0.8161628520895868, 0.5540343210303282, 0.8324938264758863,\n",
      " 0.5309312124202409, 0.8474149206132572, 0.5085361344196324, 0.8610406494408598,\n",
      " 0.4868644765678738, 0.8734775220097483, 0.46592473483598323,\n",
      " 0.8848243562809619, 0.44571959141992545, 0.8951726346490126,\n",
      " 0.4262468572520416, 0.9046068851621447, 0.40750029221646605,\n",
      " 0.9132050765537251, 0.38947031683720607, 0.9210390177960575,\n",
      " 0.3721446279880737, 0.9281747550217136, 0.3555087299855293, 0.9346729604006292,\n",
      " 0.3395463912913619, 0.9405893089765657, 0.32424003598626744,\n",
      " 0.9459748406081548, 0.30957107818589435, 0.9508763050738108,\n",
      " 0.29552020666133955, 0.955336489125606, 0.28206762609668057,\n",
      " 0.9593945248479289, 0.2691932606649208, 0.9630861791203257,\n",
      " 0.25687692492720693, 0.9664441243237718, 0.24509846645384165,\n",
      " 0.9694981906852509, 0.23383788402447173, 0.9722756008431754,\n",
      " 0.22307542478359724, 0.9748011873493064, 0.21279166330095406,\n",
      " 0.9770975939125085, 0.20296756510922315, 0.9791854612450255,\n",
      " 0.19358453695898567, 0.9810835984004498, 0.1846244657382688,\n",
      " 0.9828091405002595, 0.1760697477471168, 0.984377693737656, 0.16790330979242943,\n",
      " 0.9858034685274482, 0.1601086233712497, 0.9870994016420855,\n",
      " 0.15266971303850033, 0.9882772681392312, 0.1455711599049229,\n",
      " 0.9893477838474878, 0.13879810108005056, 0.990320699135675, 0.1323362257610981,\n",
      " 0.9912048846486319, 0.12617176856958728, 0.9920084096498489,\n",
      " 0.1202915006515041, 0.9927386135690548, 0.11468271898216846,\n",
      " 0.9934021713117286, 0.10933323425233667, 0.9940051528477726,\n",
      " 0.10423135765609279, 0.9945530775585422, 0.09936588685269511,\n",
      " 0.9950509637852613, 0.09472609133274612, 0.9955033739876628,\n",
      " 0.09030169738299577, 0.9959144558895356, 0.08608287281301641,\n",
      " 0.9962879799577319, 0.08206021158024196, 0.9966273735330602,\n",
      " 0.07822471842687809, 0.9969357519053249, 0.07456779362246066,\n",
      " 0.9972159466004733, 0.07108121788893032, 0.9974705311253192,\n",
      " 0.06775713757062492, 0.9977018443945243, 0.06458805009923906,\n",
      " 0.9979120120453396, 0.06156678979327108, 0.9981029658279507,\n",
      " 0.058686514022526545, 0.9982764612430185, 0.055940689760658684,\n",
      " 0.9984340935830976, 0.05332308054230592, 0.9985773125209078,\n",
      " 0.05082773383598042, 0.9987074353748944, 0.04844896883932296,\n",
      " 0.9988256591710118, 0.046181364699546944, 0.9989330716091481,\n",
      " 0.044019749158744084, 0.9990306610329841, 0.04195918762112378,\n",
      " 0.9991193254932943, 0.039994972637127126, 0.99919988098666,\n",
      " 0.03812261379762844, 0.9992730689442385, 0.036337828030052685,\n",
      " 0.9993395630385391, 0.03463653028714342, 0.9993999753700556,\n",
      " 0.033014824618271675, 0.999454862090042, 0.031468995612542135,\n",
      " 0.9995047285106449, 0.02999550020249566, 0.9995500337489875,\n",
      " 0.028590959816901018, 0.9995911949475887, 0.02725215287094698,\n",
      " 0.9996285911096674, 0.025976007582069204, 0.9996625665843931,\n",
      " 0.024759595099657587, 0.9996934342339661, 0.02360012293697094,\n",
      " 0.9997214783115145, 0.02249492869372799, 0.9997469570761714,\n",
      " 0.021441474058032185, 0.9997701051692928, 0.020437339076514408,\n",
      " 0.9997911357736032, 0.019480216681835045, 0.9998102425750742,\n",
      " 0.01856790746696759, 0.9998276015455355, 0.01769831469598279,\n",
      " 0.9998433725623839, 0.016869439541363287, 0.9998577008802604,\n",
      " 0.016079376538197566, 0.9998707184682142, 0.015326309245924695,\n",
      " 0.999882545224637, 0.014608506108628055, 0.9998932900811337,\n",
      " 0.013924316505201042, 0.9999030520054747, 0.01327216698103058,\n",
      " 0.999911920912851, 0.01265055765316364, 0.9999199784938113,\n",
      " 0.012058058781235663, 0.999927298966494, 0.01149330749674697,\n",
      " 0.9999339497600755, 0.0109550046835738, 0.9999399921357196,\n",
      " 0.010441912002893251, 0.9999454817507422, 0.009952849055985246,\n",
      " 0.9999504691711829, 0.00948669067865079, 0.999955000337499,\n",
      " 0.009042364361252424, 0.9999591169876688, 0.008618847788640601,\n",
      " 0.9999628570415985, 0.008215166494478728, 0.9999662549503698,\n",
      " 0.00783039162471949, 0.9999693420135457, 0.0074636378052158545,\n",
      " 0.9999721466674522, 0.007114061108672371, 0.9999746947470931,\n",
      " 0.0067808571163559175, 0.9999770097241074, 0.006463259070189646,\n",
      " 0.9999791129229608, 0.0061605361110508355, 0.9999810237173625,\n",
      " 0.005871991599281737, 0.9999827597087152, 0.005596961513603089,\n",
      " 0.9999843368882411, 0.005334812924793146, 0.9999857697842792,\n",
      " 0.005084942540660737, 0.9999870715961072, 0.004846775318999259,\n",
      " 0.9999882543155231, 0.004619763145360285, 0.9999893288373035,\n",
      " 0.004403383572630533, 0.9999903050595602, 0.0041971386195343826,\n",
      " 0.9999911919749136, 0.004000553625316836, 0.9999919977533275,\n",
      " 0.0038131761579883326, 0.9999927298173663, 0.0036345749736337064,\n",
      " 0.9999933949105669, 0.0034643390244031656, 0.999993999159557,\n",
      " 0.0033020765129134545, 0.99999454813049, 0.0031474139908925732,\n",
      " 0.9999950468803183, 0.002999995500002025, 0.999995500003375,\n",
      " 0.0028594817528664997, 0.9999959116736953, 0.002725549352432373,\n",
      " 0.9999962856834657, 0.0025978900478638403, 0.9999966254779559,\n",
      " 0.002476210025268867, 0.9999969341872558, 0.0023602292316265675,\n",
      " 0.999997214655108, 0.002249680730363582, 0.9999974694651039,\n",
      " 0.002144310087099361, 0.9999977009644824, 0.0020438747841492345,\n",
      " 0.999997911285752, 0.0019481436624400321, 0.9999981023663348,\n",
      " 0.0018568963895557974, 0.9999982759664131, 0.0017699229526909525,\n",
      " 0.9999984336851441, 0.0016870231753454267, 0.9999985769753904,\n",
      " 0.00160800625665069, 0.9999987071571036, 0.001532690332267505,\n",
      " 0.9999988254294829, 0.0014609020558457519, 0.9999989328820222,\n",
      " 0.0013924762000838767, 0.9999990305045462, 0.0013272552764704644,\n",
      " 0.9999991191963277, 0.0012650891728333796, 0.9999991997743722,\n",
      " 0.0012058348078628242, 0.9999992729809438, 0.0011493558018136072,\n",
      " 0.9999993394904023, 0.0010955221626291398, 0.9999993999154155,\n",
      " 0.0010442099867651066, 0.9999994548126031, 0.0009953011740245084,\n",
      " 0.9999995046876639, 0.0009486831557480254, 0.9999995500000337,\n",
      " 0.0009042486357343382, 0.9999995911671188, 0.0008618953432942869,\n",
      " 0.9999996285681396, 0.0008215257978706854, 0.9999996625476247,\n",
      " 0.0007830470846821898, 0.9999996934185846, 0.0007463706408749593,\n",
      " 0.9999997214653944, 0.0007114120516900337, 0.9999997469464144,\n",
      " 0.0006780908561773878, 0.999999770096369, 0.0006463303620095663,\n",
      " 0.9999997911285098, 0.0006160574689687499, 0.9999998102365795,\n",
      " 0.0005872025007010591, 0.9999998275965967, 0.000559699044350901,\n",
      " 0.9999998433684776, 0.0005334837977063175, 0.9999998576975087,\n",
      " 0.0005084964235035654, 0.9999998707156853, 0.00048467941055561995,\n",
      " 0.9999998825429276, 0.00046197794138501207, 0.9999998932881852,\n",
      " 0.0004403397660563713, 0.9999999030504405, 0.00041971508191829997,\n",
      " 0.9999999119196211, 0.0004000564189778156, 0.9999999199774277,\n",
      " 0.0003813185306435608, 0.9999999272980864, 0.0003634582895863193,\n",
      " 0.9999999339490336, 0.00034643458847716816, 0.9999999399915361,\n",
      " 0.0003302082453748163, 0.9999999454812558, 0.00031474191354437416,\n",
      " 0.9999999504687627, 0.0002999999955, 0.9999999550000004, 0.000285948561073597,\n",
      " 0.9999999591167094, 0.000272555269320982, 0.9999999628568119,\n",
      " 0.0002597892940858023, 0.9999999662547607, 0.00024762125304986975,\n",
      " 0.9999999693418571, 0.00023602314010662796, 0.9999999721465382,\n",
      " 0.00022496826090210247, 0.9999999746946405, 0.00021443117139497225,\n",
      " 0.999999977009636, 0.00020438761929436332, 0.9999999791128503,\n",
      " 0.00019481448824057457, 0.9999999810236574, 0.00018568974460026014,\n",
      " 0.9999999827596592, 0.00017699238675362053, 0.9999999843368474,\n",
      " 0.000168702396756879, 0.9999999857697506, 0.00016080069426878694,\n",
      " 0.9999999870715682, 0.00015326909263512308, 0.9999999882542926,\n",
      " 0.00014609025703010706, 0.9999999893288184, 0.00013924766455838345,\n",
      " 0.9999999903050439, 0.00013272556622575347, 0.999999991191962,\n",
      " 0.0001265089506911223, 0.9999999919977427, 0.0001205835097162321,\n",
      " 0.9999999927298086, 0.00011493560523366495, 0.9999999933949033,\n",
      " 0.00010955223795731592, 0.9999999939991535, 0.00010442101746308898,\n",
      " 0.9999999945481255, 9.953013367095728e-05, 0.9999999950468762,\n",
      " 9.486832966274888e-05, 0.9999999955, 9.042487577309287e-05, 0.9999999959116709,\n",
      " 8.618954489389807e-05, 0.9999999962856811, 8.215258893552221e-05,\n",
      " 0.999999996625476, 7.830471639045358e-05, 0.9999999969341857,\n",
      " 7.463707094786836e-05, 0.9999999972146538, 7.11412111098412e-05,\n",
      " 0.999999997469464, 6.780909076229247e-05, 0.9999999977009636,\n",
      " 6.463304065595647e-05, 0.999999997911285, 6.160575075474599e-05,\n",
      " 0.9999999981023657, 5.872025341088461e-05, 0.999999998275966,\n",
      " 5.5969907328085154e-05, 0.9999999984336847, 5.3348382275862324e-05,\n",
      " 0.9999999985769751, 5.0849644519796895e-05, 0.9999999987071568,\n",
      " 4.846794293421984e-05, 0.9999999988254292, 4.619779576535192e-05,\n",
      " 0.9999999989328818, 4.4033978014431875e-05, 0.9999999990305044,\n",
      " 4.19715094117959e-05, 0.9999999991191962, 4.000564295422854e-05,\n",
      " 0.9999999991997742, 3.813185397920151e-05, 0.9999999992729809,\n",
      " 3.634582975085537e-05, 0.9999999993394904, 3.4643459533754074e-05,\n",
      " 0.9999999993999154, 3.302082513156546e-05, 0.9999999994548125,\n",
      " 3.147419186889275e-05, 0.9999999995046877, 2.99999999955e-05, 0.99999999955,\n",
      " 2.8594856493146806e-05, 0.9999999995911671, 2.725552726617606e-05,\n",
      " 0.9999999996285681, 2.5978929697879745e-05, 0.9999999996625476,\n",
      " 2.4762125555510034e-05, 0.9999999996934186, 2.3602314227606827e-05,\n",
      " 0.9999999997214654, 2.249682627807604e-05, 0.9999999997469464,\n",
      " 2.14431173021823e-05, 0.9999999997700963, 2.0438762070315797e-05,\n",
      " 0.9999999997911285, 1.948144894605405e-05, 0.9999999998102366,\n",
      " 1.856897456567071e-05, 0.9999999998275966, 1.769923876684659e-05,\n",
      " 0.9999999998433685, 1.6870239754910247e-05, 0.9999999998576975,\n",
      " 1.608006949548242e-05, 0.9999999998707156, 1.5326909322920688e-05,\n",
      " 0.999999999882543, 1.4609025754456241e-05, 0.9999999998932881,\n",
      " 1.3924766500388347e-05, 0.9999999999030504, 1.327255666115406e-05,\n",
      " 0.9999999999119196, 1.2650895102520014e-05, 0.9999999999199775,\n",
      " 1.2058351000553162e-05, 0.999999999927298, 1.1493560548418802e-05,\n",
      " 0.9999999999339491, 1.0955223817425996e-05, 0.9999999999399916,\n",
      " 1.0442101765095473e-05, 0.9999999999454813, 9.953013383364237e-06,\n",
      " 0.9999999999504687, 9.486832980362836e-06, 0.999999999955,\n",
      " 9.042487589508947e-06, 0.9999999999591167, 8.618954499954276e-06,\n",
      " 0.9999999999628568, 8.215258902700675e-06, 0.9999999999662548,\n",
      " 7.830471646967593e-06, 0.9999999999693419, 7.463707101647207e-06,\n",
      " 0.9999999999721465, 7.114121116924958e-06, 0.9999999999746947,\n",
      " 6.780909081373799e-06, 0.9999999999770096, 6.463304070050646e-06,\n",
      " 0.9999999999791128, 6.16057507933247e-06, 0.9999999999810236,\n",
      " 5.872025344429239e-06, 0.9999999999827597, 5.596990735701512e-06,\n",
      " 0.9999999999843369, 5.334838230091463e-06, 0.9999999999857697,\n",
      " 5.08496445414913e-06, 0.9999999999870716, 4.8467942953006426e-06,\n",
      " 0.9999999999882543, 4.619779578162044e-06, 0.9999999999893289,\n",
      " 4.403397802851981e-06, 0.9999999999903051, 4.197150942399555e-06,\n",
      " 0.9999999999911919, 4.000564296479301e-06, 0.9999999999919977,\n",
      " 3.8131853988349966e-06, 0.9999999999927298, 3.6345829758777604e-06,\n",
      " 0.999999999993395, 3.464345954061445e-06, 0.9999999999939991,\n",
      " 3.30208251375063e-06, 0.9999999999945481, 3.147419187403731e-06,\n",
      " 0.9999999999950469, 2.9999999999955002e-06, 0.9999999999955,\n",
      " 2.8594856497004677e-06, 0.9999999999959117, 2.725552726951684e-06,\n",
      " 0.9999999999962856, 2.5978929700772734e-06, 0.9999999999966255,\n",
      " 2.4762125558015265e-06, 0.9999999999969342, 2.360231422977627e-06,\n",
      " 0.9999999999972147, 2.24968262799547e-06, 0.9999999999974695,\n",
      " 2.144311730380915e-06, 0.999999999997701, 2.0438762071724595e-06,\n",
      " 0.9999999999979113, 1.9481448947274014e-06, 0.9999999999981024,\n",
      " 1.8568974566727157e-06, 0.9999999999982759, 1.7699238767761437e-06,\n",
      " 0.9999999999984337, 1.6870239755702469e-06, 0.999999999998577,\n",
      " 1.6080069496168458e-06, 0.9999999999987071, 1.532690932351477e-06,\n",
      " 0.9999999999988254, 1.4609025754970696e-06, 0.9999999999989329,\n",
      " 1.3924766500833845e-06, 0.9999999999990306, 1.3272556661539845e-06,\n",
      " 0.9999999999991191, 1.265089510285409e-06, 0.9999999999991998,\n",
      " 1.2058351000842461e-06, 0.999999999999273, 1.1493560548669325e-06,\n",
      " 0.9999999999993395, 1.095522381764294e-06, 0.9999999999993999,\n",
      " 1.0442101765283342e-06, 0.9999999999994548, 9.95301338352692e-07,\n",
      " 0.9999999999995047, 9.486832980503715e-07, 0.99999999999955,\n",
      " 9.042487589630944e-07, 0.9999999999995912, 8.618954500059921e-07,\n",
      " 0.9999999999996285, 8.215258902792159e-07, 0.9999999999996625,\n",
      " 7.830471647046815e-07, 0.9999999999996935, 7.463707101715811e-07,\n",
      " 0.9999999999997214, 7.114121116984364e-07, 0.999999999999747,\n",
      " 6.780909081425245e-07, 0.9999999999997701, 6.463304070095197e-07,\n",
      " 0.9999999999997912, 6.160575079371048e-07, 0.9999999999998103,\n",
      " 5.872025344462647e-07, 0.9999999999998276, 5.596990735730441e-07,\n",
      " 0.9999999999998433, 5.334838230116515e-07, 0.9999999999998577,\n",
      " 5.084964454170824e-07, 0.9999999999998708, 4.84679429531943e-07,\n",
      " 0.9999999999998825, 4.619779578178312e-07, 0.9999999999998933,\n",
      " 4.403397802866069e-07, 0.9999999999999031, 4.197150942411755e-07,\n",
      " 0.999999999999912, 4.0005642964898655e-07, 0.99999999999992,\n",
      " 3.813185398844145e-07, 0.9999999999999273, 3.634582975885683e-07,\n",
      " 0.9999999999999339, 3.464345954068305e-07, 0.9999999999999399,\n",
      " 3.3020825137565715e-07, 0.9999999999999455, 3.1474191874088754e-07,\n",
      " 0.9999999999999505, 2.999999999999955e-07, 0.999999999999955,\n",
      " 2.859485649704326e-07, 0.9999999999999591, 2.7255527269550246e-07,\n",
      " 0.9999999999999628, 2.597892970080167e-07, 0.9999999999999662,\n",
      " 2.4762125558040316e-07, 0.9999999999999694, 2.3602314229797963e-07,\n",
      " 0.9999999999999721, 2.2496826279973484e-07, 0.9999999999999747,\n",
      " 2.1443117303825417e-07, 0.999999999999977, 2.0438762071738681e-07,\n",
      " 0.9999999999999791, 1.9481448947286217e-07, 0.999999999999981,\n",
      " 1.8568974566737725e-07, 0.9999999999999828, 1.7699238767770587e-07,\n",
      " 0.9999999999999843, 1.6870239755710393e-07, 0.9999999999999858,\n",
      " 1.608006949617532e-07, 0.9999999999999871, 1.5326909323520711e-07,\n",
      " 0.9999999999999882, 1.460902575497584e-07, 0.9999999999999893,\n",
      " 1.39247665008383e-07, 0.9999999999999903, 1.3272556661543705e-07,\n",
      " 0.9999999999999912, 1.2650895102857433e-07, 0.999999999999992,\n",
      " 1.2058351000845356e-07, 0.9999999999999928, 1.149356054867183e-07,\n",
      " 0.9999999999999934, 1.0955223817645109e-07, 0.999999999999994,\n",
      " 1.0442101765285221e-07, 0.9999999999999946, 9.953013383528548e-08,\n",
      " 0.999999999999995, 9.486832980505124e-08, 0.9999999999999954,\n",
      " 9.042487589632165e-08, 0.9999999999999959, 8.618954500060977e-08,\n",
      " 0.9999999999999963, 8.215258902793075e-08, 0.9999999999999967,\n",
      " 7.830471647047607e-08, 0.9999999999999969, 7.463707101716497e-08,\n",
      " 0.9999999999999972, 7.114121116984959e-08, 0.9999999999999974,\n",
      " 6.78090908142576e-08, 0.9999999999999977, 6.463304070095643e-08,\n",
      " 0.9999999999999979, 6.160575079371434e-08, 0.9999999999999981,\n",
      " 5.872025344462981e-08, 0.9999999999999982, 5.59699073573073e-08,\n",
      " 0.9999999999999984, 5.334838230116766e-08, 0.9999999999999986,\n",
      " 5.084964454171041e-08, 0.9999999999999987, 4.846794295319617e-08,\n",
      " 0.9999999999999988, 4.619779578178475e-08, 0.9999999999999989,\n",
      " 4.40339780286621e-08, 0.999999999999999, 4.197150942411877e-08,\n",
      " 0.9999999999999991, 4.0005642964899705e-08, 0.9999999999999992,\n",
      " 3.813185398844237e-08, 0.9999999999999992, 3.634582975885762e-08,\n",
      " 0.9999999999999993, 3.464345954068374e-08, 0.9999999999999994,\n",
      " 3.30208251375663e-08, 0.9999999999999994, 3.147419187408927e-08,\n",
      " 0.9999999999999996]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "d_model = 768\n",
    "\n",
    "def positional_encoding(position):\n",
    "    pe = [None] * d_model\n",
    "\n",
    "    for i in range(0, d_model, 2):\n",
    "        pe[i] = math.sin(position / (10000 ** ((2 * i) / d_model)))\n",
    "        pe[i + 1] = math.cos(position / (10000 ** ((2 * i) / d_model)))\n",
    "\n",
    "    return pe\n",
    "\n",
    "# 計算第 3 個位置的 positional encoding\n",
    "pprint(positional_encoding(3), compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "實際上，BERT 的位置編碼是透過訓練學習到的，而不是使用上述公式計算的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-4.1949e-03, -1.1852e-02, -2.1180e-02, -9.3056e-03, -1.0164e-02,\n",
      "         1.7667e-02, -5.2579e-03, -5.9056e-03, -5.2515e-03, -1.9121e-02,\n",
      "         2.3867e-02, -2.7827e-02,  1.8384e-02, -1.1762e-02, -2.0740e-03,\n",
      "         7.1641e-03, -2.9109e-03,  1.9358e-02,  8.1694e-04, -5.6441e-03,\n",
      "        -7.8681e-03, -2.5424e-02, -8.3934e-03,  1.3958e-02,  2.4538e-03,\n",
      "        -6.5318e-03, -2.3111e-02,  1.0861e-02, -1.0973e-02,  1.7499e-02,\n",
      "         1.5851e-02, -1.4915e-02,  2.6499e-03,  3.0319e-03, -8.4172e-03,\n",
      "        -1.6653e-02, -3.2846e-03, -8.7255e-03, -4.6161e-03, -2.7917e-03,\n",
      "         1.5829e-02,  2.3712e-03, -1.5293e-02, -5.9103e-03, -9.5935e-04,\n",
      "        -1.5983e-02,  9.4848e-04,  1.0567e-02,  2.0371e-02, -2.6296e-03,\n",
      "        -3.3435e-03,  9.3798e-03,  9.9587e-03, -5.1266e-03, -3.5115e-03,\n",
      "         1.9099e-02,  1.5399e-02,  7.0687e-03,  1.6475e-02, -1.5982e-02,\n",
      "        -6.7953e-03, -1.3570e-03, -1.7052e-02, -8.2985e-03, -6.3168e-03,\n",
      "        -9.0573e-03, -8.9459e-04, -6.1750e-03,  6.3256e-03,  2.1110e-02,\n",
      "        -9.1038e-03, -2.8794e-03, -4.1760e-03,  6.5541e-03,  7.3994e-03,\n",
      "         1.0967e-02,  2.8346e-03,  1.5476e-02,  2.0370e-02, -1.1673e-02,\n",
      "        -1.3281e-02, -9.9013e-03, -1.1771e-02, -7.8748e-03, -1.6307e-02,\n",
      "        -1.5650e-02,  1.2775e-02,  2.2723e-02,  1.7722e-02, -5.3911e-03,\n",
      "         1.4607e-02, -5.9970e-03, -1.6621e-02, -8.0202e-04, -7.1502e-03,\n",
      "        -1.6571e-02, -7.2089e-02, -1.5535e-02, -5.1865e-03, -2.0458e-02,\n",
      "         8.8984e-03, -1.3545e-02,  1.0768e-02,  1.1306e-02, -9.7652e-03,\n",
      "         5.2996e-02, -1.1299e-02,  2.2500e-03, -1.7200e-02,  3.3168e-02,\n",
      "         1.3131e-02,  1.4288e-02,  5.3589e-03, -3.5638e-03,  1.2053e-02,\n",
      "        -9.7535e-03, -4.6186e-03, -1.4517e-02,  1.3427e-02,  1.0228e-02,\n",
      "         2.6085e-02, -3.4219e-02,  7.6357e-03, -4.9960e-04,  5.1462e-02,\n",
      "        -2.4102e-02, -3.6043e-03,  1.3121e-02,  9.3095e-03,  5.1484e-03,\n",
      "        -2.2224e-03, -1.6396e-03, -1.4861e-02, -1.0627e-02,  9.5762e-03,\n",
      "         9.6303e-03, -2.9251e-03, -1.4755e-02, -2.2101e-02,  2.2938e-02,\n",
      "         1.9338e-02,  1.7060e-02,  7.3671e-03, -4.9498e-03, -4.9394e-03,\n",
      "        -3.9739e-02,  4.6561e-03,  2.4625e-02,  2.3466e-02,  1.5588e-03,\n",
      "         1.5373e-02, -4.8667e-04, -9.2451e-03,  2.7125e-03, -1.0773e-03,\n",
      "         2.3912e-03, -1.3481e-02,  8.9901e-03, -6.5100e-03,  6.9211e-02,\n",
      "         1.2614e-02, -2.3043e-03, -6.3379e-03,  1.3488e-03, -6.4105e-03,\n",
      "         1.8553e-03, -1.2440e-03, -6.4761e-03, -6.3988e-02, -2.2048e-02,\n",
      "         1.5917e-03, -1.9679e-02,  7.7462e-04, -9.7771e-03,  5.3537e-03,\n",
      "         3.8568e-02,  4.7830e-03,  5.8485e-03, -5.9514e-03, -1.0554e-02,\n",
      "         1.1176e-02, -2.3172e-03, -1.1186e-02, -4.0049e-03,  2.2642e-03,\n",
      "         1.2816e-02, -1.8637e-02, -1.2915e-03, -1.6269e-03,  1.6982e-02,\n",
      "         9.2356e-04,  1.4937e-02,  1.5294e-02,  6.5220e-03,  2.6312e-03,\n",
      "         4.2886e-03,  3.4075e-03,  3.8578e-03, -1.3493e-02,  2.7329e-03,\n",
      "         8.0338e-03, -5.6290e-03, -1.2730e-03,  3.8960e-03,  3.1083e-03,\n",
      "         6.0537e-03, -2.4037e-02,  2.7409e-02, -3.3713e-03, -8.1218e-03,\n",
      "        -1.3249e-02, -7.0505e-03,  2.0818e-02, -7.4105e-03, -2.5471e-02,\n",
      "         2.9738e-02,  2.1347e-02, -4.2131e-03, -2.7219e-02, -1.0076e-02,\n",
      "        -3.4609e-03, -9.7964e-04, -1.7393e-02,  1.9384e-02, -1.1516e-02,\n",
      "        -8.1498e-02,  2.7996e-02,  1.5759e-02, -1.4122e-02, -1.2589e-02,\n",
      "         1.1823e-02, -6.3107e-03,  8.8497e-03, -8.8185e-04, -6.5677e-03,\n",
      "        -1.6338e-02, -1.2187e-03, -7.6623e-03, -4.8961e-03,  1.2718e-02,\n",
      "        -1.7168e-03, -4.9215e-03,  1.5637e-02,  5.3422e-03, -6.0968e-03,\n",
      "         3.4907e-02,  9.7800e-03,  1.4601e-03, -9.7991e-03,  4.6129e-02,\n",
      "        -2.1525e-03, -9.4858e-03,  2.1663e-02, -3.2777e-03,  3.9944e-03,\n",
      "         3.6538e-04, -3.2305e-03, -5.1902e-03,  1.3312e-03, -1.6358e-02,\n",
      "        -1.7380e-02, -5.7172e-03,  7.2367e-03,  7.3384e-04,  9.5508e-03,\n",
      "         9.5638e-03, -1.3549e-02,  3.9387e-03,  3.2314e-04, -6.1864e-03,\n",
      "         2.4195e-02, -5.4000e-03,  3.1276e-03, -1.0259e-02, -2.1563e-02,\n",
      "         2.2096e-04,  1.1184e-02,  6.4798e-04,  1.3337e-02, -2.4641e-02,\n",
      "        -1.5270e-02, -2.0656e-02, -6.9784e-03,  7.4094e-02,  2.8753e-03,\n",
      "         1.6532e-02,  2.4233e-02, -2.4051e-02, -2.4839e-03, -7.6175e-03,\n",
      "        -1.0114e-02, -6.8420e-02, -1.2383e-02, -2.0549e-02,  9.1893e-03,\n",
      "         1.0322e-02, -8.8536e-02,  7.3783e-03, -1.8980e-02,  1.0171e-02,\n",
      "         8.9116e-03, -7.2087e-03,  2.5968e-03, -1.3621e-02,  2.0677e-02,\n",
      "        -1.5982e-02,  2.0709e-02, -1.2191e-02, -1.1482e-02, -3.7854e-03,\n",
      "        -1.0001e-02,  1.1554e-01,  1.8465e-02,  6.4498e-03,  5.0884e-03,\n",
      "        -6.9591e-03, -1.1497e-02, -9.2882e-03,  1.1727e-02, -2.3593e-02,\n",
      "         8.5226e-03, -1.1746e-02,  4.5611e-03,  2.2632e-02, -6.2231e-03,\n",
      "        -1.6000e-02, -6.2604e-03, -4.5579e-03,  2.8557e-03,  1.4893e-02,\n",
      "        -1.5393e-02,  7.4873e-02,  1.4589e-03,  8.1232e-03,  1.8672e-02,\n",
      "         2.0051e-02, -1.2487e-02,  3.1753e-03, -1.8537e-02, -3.4869e-03,\n",
      "        -7.3105e-03,  1.0058e-02,  1.6617e-02, -4.1527e-03,  9.1699e-03,\n",
      "        -7.3367e-02, -1.0556e-02,  1.5128e-02,  6.7343e-03,  5.0938e-03,\n",
      "        -3.2517e-03, -1.0831e-02,  6.2242e-03, -2.8839e-02, -4.3810e-03,\n",
      "        -2.6775e-03,  1.0574e-02,  2.8676e-02,  1.1266e-02, -8.8453e-03,\n",
      "        -1.1190e-02, -7.0927e-03, -4.1633e-03,  4.0487e-03,  3.9487e-03,\n",
      "        -8.9541e-03,  1.7497e-02,  1.2789e-03,  2.5847e-03, -1.9595e-03,\n",
      "         1.4452e-02, -3.2745e-03,  7.9730e-03, -3.7476e-03,  6.4050e-03,\n",
      "         6.4015e-03, -6.4046e-03, -1.2846e-02, -8.7482e-04,  3.3049e-04,\n",
      "        -1.4156e-03, -4.8636e-03,  1.2647e-03, -1.8101e-03, -2.1004e-03,\n",
      "        -1.0685e-03,  5.2952e-03,  1.6627e-02,  2.2797e-02,  7.0236e-03,\n",
      "         3.4607e-03, -1.0940e-02, -5.8928e-04, -7.5830e-03, -7.0077e-03,\n",
      "        -1.3924e-02, -1.1082e-02, -6.6381e-03,  1.7931e-04,  8.3157e-03,\n",
      "         1.7415e-02, -1.5505e-02, -3.2878e-02,  2.3790e-03, -2.3371e-03,\n",
      "         6.7229e-03,  3.6267e-03,  4.7065e-03, -4.2512e-03,  1.1576e-02,\n",
      "         1.1978e-03,  1.2472e-02, -1.8004e-03, -2.6683e-03, -8.2733e-03,\n",
      "        -2.6739e-02,  5.8197e-03,  4.6913e-03, -4.3422e-03,  3.0608e-02,\n",
      "        -2.0029e-02, -1.1799e-02, -1.7477e-02,  1.1061e-02,  2.2800e-03,\n",
      "        -2.1850e-02, -1.0155e-02,  1.9656e-02, -1.7915e-03,  1.1194e-02,\n",
      "        -4.9104e-03,  9.3657e-04,  3.9174e-03,  4.2571e-03, -3.2359e-04,\n",
      "         7.7488e-04,  1.3528e-02,  2.7228e-02,  8.4010e-03, -9.7467e-05,\n",
      "         1.1049e-02,  6.3494e-03, -8.3382e-03, -4.6859e-02, -1.2329e-03,\n",
      "        -1.7782e-02,  6.6022e-03,  4.1794e-03,  1.5147e-03,  6.4018e-03,\n",
      "         1.5528e-02,  2.0255e-03, -9.2981e-03,  9.7039e-03,  1.7283e-02,\n",
      "        -9.0707e-03,  6.8366e-04, -3.6027e-02, -9.3410e-04, -7.5122e-03,\n",
      "        -1.3093e-02,  8.0455e-02, -5.9367e-03,  4.4772e-03, -1.7482e-02,\n",
      "         1.6453e-02, -1.0118e-02, -2.7659e-02, -9.4592e-03, -7.9770e-03,\n",
      "        -3.2315e-03,  1.1610e-02,  3.6085e-03,  1.1613e-02, -2.0489e-02,\n",
      "         8.4690e-03,  1.7354e-02,  7.2169e-03, -9.3647e-03,  8.0065e-03,\n",
      "         8.4126e-03,  4.4003e-03,  9.0566e-02, -9.6665e-03,  6.7551e-03,\n",
      "        -2.1153e-02, -3.5922e-03,  8.4954e-03, -7.7278e-03,  6.4095e-03,\n",
      "        -5.2964e-03, -3.9850e-03,  9.0709e-03,  1.2738e-02,  1.5541e-02,\n",
      "         2.1831e-04,  4.2073e-02, -1.0366e-02,  2.6370e-03,  9.0495e-03,\n",
      "         1.4075e-02,  7.8832e-04, -4.7138e-03, -5.8078e-03, -7.3162e-03,\n",
      "        -2.2824e-03, -1.1410e-02, -1.6955e-02,  2.5528e-03, -4.6226e-03,\n",
      "         3.0807e-02, -4.6339e-03, -1.5355e-02,  1.5122e-02,  4.9599e-03,\n",
      "        -7.2503e-03, -9.7185e-03,  6.5707e-02, -7.5484e-03, -3.6065e-04,\n",
      "        -5.0008e-03, -4.3922e-03, -1.0442e-02, -1.9778e-02,  7.8605e-03,\n",
      "         3.2877e-03, -7.3790e-03,  7.9625e-03,  9.1463e-03, -7.8196e-02,\n",
      "        -1.7518e-02, -1.3160e-03, -6.3479e-03,  2.6187e-03, -7.5463e-03,\n",
      "         1.4512e-02,  1.7329e-02,  6.5902e-03,  3.4602e-03, -1.0846e-02,\n",
      "        -1.4974e-02, -8.3385e-02,  1.4266e-02,  1.2455e-02,  1.5894e-02,\n",
      "         8.1541e-02, -4.7819e-03,  5.5739e-03, -1.5036e-02, -6.0994e-03,\n",
      "         1.3197e-02,  2.7727e-03,  1.4478e-02,  8.8152e-03,  3.6011e-03,\n",
      "        -1.1700e-02, -1.8622e-02, -6.6383e-03,  1.9249e-02,  7.5996e-03,\n",
      "         6.5807e-04, -1.8532e-02, -3.4945e-03,  1.7456e-02,  3.3667e-03,\n",
      "         1.0933e-02,  1.9253e-02,  5.5672e-04, -2.7529e-02,  5.0742e-03,\n",
      "         1.6119e-03,  9.1162e-03,  7.0982e-03, -2.3739e-02, -2.4275e-03,\n",
      "         9.9501e-04, -1.4626e-03, -6.6352e-02, -9.4206e-03, -1.7033e-03,\n",
      "         2.2227e-02, -3.4428e-03, -3.8230e-03,  3.0628e-03,  1.3245e-02,\n",
      "        -1.8317e-02, -3.1938e-03,  1.1050e-02, -1.1276e-03,  1.3951e-02,\n",
      "        -1.0760e-03,  6.1108e-03, -1.0552e-02, -2.1954e-03, -1.6121e-02,\n",
      "         1.2197e-02,  8.4749e-03, -2.4978e-02,  1.6126e-02,  1.5739e-02,\n",
      "         9.1623e-03, -2.3810e-02,  2.2737e-04, -6.9377e-03,  2.6044e-02,\n",
      "        -5.8073e-03, -8.9957e-04,  4.5947e-03,  2.9757e-03, -7.0408e-03,\n",
      "        -2.0193e-03, -7.8135e-03,  7.8613e-03, -2.0520e-02,  3.3048e-02,\n",
      "         7.2027e-03,  6.2123e-03, -6.7387e-03, -1.3781e-02, -7.6473e-03,\n",
      "        -8.6830e-03, -4.3862e-03,  1.3712e-02,  1.8225e-02, -2.6934e-03,\n",
      "        -4.4433e-03, -7.3494e-03, -1.2950e-02, -1.4599e-02,  2.1371e-03,\n",
      "        -9.4679e-04, -2.5021e-02, -6.5356e-03,  3.3092e-03,  7.8073e-05,\n",
      "        -4.3705e-03, -1.0033e-02,  2.6226e-03,  1.5051e-02,  4.5655e-03,\n",
      "         1.1798e-02, -2.4664e-02,  5.7241e-03,  1.2014e-02,  2.0705e-03,\n",
      "        -1.2762e-02,  8.5551e-03, -1.5021e-02,  1.5097e-02,  5.8947e-03,\n",
      "         3.3965e-04,  1.7433e-02,  3.8729e-03,  8.0355e-03, -5.9811e-02,\n",
      "        -8.9984e-03,  1.3799e-02, -5.6733e-03, -3.6037e-03, -1.3744e-02,\n",
      "         9.3995e-03,  1.9155e-03,  3.8986e-03,  7.3232e-03, -6.0419e-03,\n",
      "         1.4504e-02,  1.8735e-02, -1.3109e-03, -1.3381e-02,  7.4994e-03,\n",
      "         8.0427e-04, -1.4797e-02,  1.5467e-02, -1.1044e-02, -3.5856e-03,\n",
      "         4.2739e-03, -2.0160e-03,  3.6408e-03, -9.9176e-04, -2.1895e-02,\n",
      "        -2.1906e-02,  6.6361e-03, -7.5800e-04, -6.9395e-03,  8.1112e-03,\n",
      "         2.1375e-04,  9.5978e-03, -8.5171e-03,  3.5701e-03,  8.7939e-03,\n",
      "         7.8418e-03, -1.6319e-03,  7.9874e-03,  1.8562e-03,  1.0500e-02,\n",
      "         2.9913e-03,  1.4248e-02,  3.1845e-03,  6.3395e-03,  1.9067e-02,\n",
      "         9.6993e-03, -1.8823e-02,  1.7311e-02,  6.6716e-03, -1.0719e-02,\n",
      "        -3.1145e-02, -2.7879e-02, -4.2140e-03, -5.8257e-03,  7.3854e-03,\n",
      "        -3.4938e-03,  1.0880e-03,  3.6344e-03, -7.9768e-03,  1.9133e-02,\n",
      "        -2.6138e-03,  1.2579e-02, -1.4487e-03, -1.0768e-02, -7.3069e-03,\n",
      "        -1.4704e-02, -1.3319e-02, -3.0231e-03,  9.3542e-03,  3.2765e-03,\n",
      "         5.6641e-03, -6.7279e-03,  6.1233e-03,  6.0199e-03, -5.0938e-03,\n",
      "         2.6048e-03, -5.7787e-03,  4.3653e-03, -1.2705e-02,  7.0265e-03,\n",
      "         9.3699e-04, -1.2731e-02, -3.2921e-03, -7.4522e-03, -6.6943e-03,\n",
      "        -2.8130e-03,  9.0801e-03, -5.9985e-03, -5.2866e-03, -6.4522e-03,\n",
      "         3.5796e-03,  9.6847e-04, -2.4636e-02,  1.6907e-02,  2.3185e-02,\n",
      "         3.8363e-03,  3.5170e-03,  2.9464e-03,  1.8069e-02,  2.6780e-03,\n",
      "         8.7443e-03,  1.5816e-02,  1.0406e-02,  2.2709e-02,  6.8144e-05,\n",
      "        -4.1751e-03,  1.4319e-03,  1.9246e-04,  8.4174e-03,  6.6466e-03,\n",
      "         2.2455e-02,  5.2826e-03, -1.9723e-03], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pprint(model.embeddings.position_embeddings.weight[3], compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後，Input Embedding 產生的編碼向量應該加上位置編碼向量，然後將結果輸入到編碼器/解碼器區塊中：\n",
    "\n",
    "$\\ Embedding_{output} = Embedding_{initial} + PE_{word-position} $\n",
    "\n",
    "以上是簡化的說明，實際上 BERT 模型還額外加上 Token Type Embedding，這不在我們的討論範圍內。\n",
    "\n",
    "![](https://cdn.prod.website-files.com/6064b31ff49a2d31e0493af1/66d06d2f71f03e028aa4698f_AD_4nXeBMY4_9hogpj-S8elt4Lm0PYtwN5KWb3drhXV0mUVvG5FwCNdkfTQcvAQGwBgeYfkEaZNUKN6YKPaFrNayvn4hcDUx-FXsW9bFyPNDM86Rjt5E0kyIkKhJowh--90eICovok_fBGdLb32mSUL7pljzM518.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
