{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# transformers not support NumPy 2.0 yet\n",
    "!pip install -q numpy~=1.26.4 transformers~=4.46.2\n",
    "# for visualization self-attention\n",
    "!pip install -q bertviz~=1.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer: how do they work internally?\n",
    "\n",
    "## Transformer in a nutshell\n",
    "\n",
    "Transformer 由 Encoder 和 Decoder 两部分组成，Encoder 接受 **输入** 然後生成 **向量** 表示這個输入，Decoder 接受 **向量** 然後逐一生成 **输出**。\n",
    "![transformer in a nutshell](https://www.alexisalulema.com/wp-content/uploads/2022/08/encoder-decoder-1-1024x275.png)\n",
    "\n",
    "### Encoder\n",
    "\n",
    "Transformer 由 N 個 Encoder 組成，每個 Encoder 將其輸出送到下一個 Encoder。最終的 Encoder 返回 **輸入** 的 **向量** 表示。為了說明，從現在開始，我們將使用 N=2 的值。\n",
    "\n",
    "![encoder in a nutshell](https://www.alexisalulema.com/wp-content/uploads/2022/08/encoders.png)\n",
    "\n",
    "### Decoder\n",
    "\n",
    "同樣地，我們可以有 N 個 Decoder（假設 N=2）。Encoder 生成的 **向量** 表示是所有 Decoder 的輸入；也就是說，一個 Decoder 接收兩個輸入，一個來自前一個 Decoder，另一個來自 Encoder 生成的 **向量** 表示。\n",
    "\n",
    "![decoder in a nutshell](https://www.alexisalulema.com/wp-content/uploads/2022/08/encoderNdecoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Transformer\n",
    "\n",
    "Google Research 和 Google Brain 的成員最初在 2017 年的論文《[Attention is all you need](https://arxiv.org/abs/1706.03762?context=cs)》中提出了 Transformer。目前是 NLP 中最受歡迎的架構之一。\n",
    "\n",
    "![Original Transformer Architecture, Source: Attention is all you need](https://www.alexisalulema.com/wp-content/uploads/2022/08/image-695x1024.png)\n",
    "\n",
    "Transformer 超越了基於 LSTM 和 RNN 的其他現有架構，獲得了更高的評估結果和更快的訓練速度。Transformer 的注意力機制是一種「word-to-word」的操作，它會找出序列中每個詞與其他詞之間的關係，包括詞本身。\n",
    "\n",
    "在本文中，我將以最簡單的方式解釋 Transformer 的架構和內部運作。\n",
    "\n",
    "### Input Embedding\n",
    "\n",
    "Transformer 架構首先是 Input Embedding 層，它將輸入序列轉換為維度為 $\\ d_{model} = 512 $ 的向量。\n",
    "\n",
    "![Input Embedding](https://www.alexisalulema.com/wp-content/uploads/2022/08/input.embedding.png)\n",
    "\n",
    "> $\\ d_{model} = 512 $ 的值是由架構設計者設定的，這個維度的值可以根據目標進行修改。\n",
    "\n",
    "Tokenizer 通過將文本正規化來將輸入流轉換為 token；此外，它還會提供一個整數表示（基於現有詞彙），該表示將用於 Embedding 過程。例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from pprint import pprint\n",
    "\n",
    "model_name = \"google-bert/bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "pprint(tokenizer)\n",
    "# vocab_szie: 30522\n",
    "# model_max_length: 512\n",
    "# 0: [PAD]\n",
    "# 100: [UNK]\n",
    "# 101: [CLS]\n",
    "# 102: [SEP]\n",
    "# 103: [MASK]\n",
    "\n",
    "pprint(list(tokenizer.vocab.keys())[1000:1010])\n",
    "# display 10 vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a this course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(\n",
    "  raw_inputs,\n",
    "  padding=True,         # Activates and controls padding\n",
    "  truncation=True,      # Activates and controls truncation\n",
    "  return_tensors=\"pt\",  # Return PyTorch tensor objects\n",
    "  )\n",
    "\n",
    "pprint(inputs)\n",
    "# 1. 第二句子長度不夠，所以會被補上 [PAD] token\n",
    "# 2. Attention mask 會標記出哪些是 padding 的 token\n",
    "\n",
    "tokenizer.decode(inputs[\"input_ids\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果將此模型應用在中文文本上，會發生什麼狀況呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilingual\n",
    "# model_name = \"google-bert/bert-base-multilingual-uncased\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# pprint(tokenizer)\n",
    "\n",
    "raw_inputs = [\n",
    "    \"我門正在學習目前正夯的變形金剛模型！\",\n",
    "    \"你們喜歡這個課程嗎？\"\n",
    "]\n",
    "inputs = tokenizer(\n",
    "  raw_inputs,\n",
    "  padding=True,         # Activates and controls padding\n",
    "  truncation=True,      # Activates and controls truncation\n",
    "  return_tensors=\"pt\",  # Return PyTorch tensor objects\n",
    "  )\n",
    "\n",
    "tokenizer.decode(inputs[\"input_ids\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下來，Embedding 從分詞後的向量中獲取數據。對於每個詞，生成一個大小為 $\\ d_{model} = 512 $ 的向量。例如，對於表示意思的詞，Embedding 向量應該是相似的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pprint import pprint\n",
    "\n",
    "model_name = \"google-bert/bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
    "text = '''\n",
    "After stealing money from the bank vault, the bank robber was seen\n",
    "fishing on the Mississippi river bank.\n",
    "'''\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "text_to_token_id = tokenizer(\n",
    "    text,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Display the words with their indeces.\n",
    "# 請注意，這裡的 token id 是對應到 vocab 的 index\n",
    "# 此時 bank 這個字具有相同的 token id\n",
    "for id in text_to_token_id[\"input_ids\"][0]:\n",
    "    pprint('{:<12} {:>6,}'.format(tokenizer.decode(id), id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\n",
    "  model_name,\n",
    "  output_hidden_states=True,  # Whether the model returns all hidden-states.\n",
    "  )\n",
    "\n",
    "# 這個 model 有 12 層的 transformer blocks, 及第一層的 embedding 層, 共 13 層\n",
    "pprint(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Run the text through BERT, and collect all of the hidden states produced from all 12 layers.\n",
    "with torch.no_grad():\n",
    "    outputs = model(**text_to_token_id)\n",
    "    hidden_states = outputs.hidden_states\n",
    "\n",
    "n_hidden_states = len(hidden_states)\n",
    "pprint(f\"Number of layers: {n_hidden_states} (initial embeddings + 12 BERT layers)\")\n",
    "layer_i = 0\n",
    "\n",
    "n_batches = len(hidden_states[layer_i])\n",
    "pprint(f\"Number of batches: {n_batches}\")\n",
    "batch_i = 0\n",
    "\n",
    "n_tokens = len(hidden_states[layer_i][batch_i])\n",
    "pprint(f\"Number of tokens: {n_tokens}\")\n",
    "token_i = 0\n",
    "\n",
    "n_hidden_units = len(hidden_states[layer_i][batch_i][token_i])\n",
    "pprint(f\"Number of hidden units: {n_hidden_units}\")\n",
    "\n",
    "pprint(f\"Number of unique values: {n_hidden_states * n_batches * n_tokens * n_hidden_units}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這個模型的完整隱藏狀態集，存儲在 hidden_states 物件中，這個物件有四個維度，順序如下：\n",
    "\n",
    "1. 層數（13 層）\n",
    "2. 批次數（1 句話）\n",
    "3. 詞/標記 (Token) 數（我們句子中的 22 個 Token）\n",
    "4. 隱藏單元/特徵數（768 個特徵）\n",
    "\n",
    "僅僅為了表示我們的一句話，就需要 219,648 個唯一值！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor.\n",
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "token_embeddings.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s get rid of the “batches” dimension since we don’t need it.\n",
    "# Remove dimension 1, the \"batches\".\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current dimensions: [# layers, # tokens, # features]\n",
    "# Desired dimensions: [# tokens, # layers, # features]\n",
    "\n",
    "# Swap dimensions 0 and 1.\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "現在，我們該如何處理這些隱藏狀態呢？我們希望為每個標記 (Token) 獲取單獨的向量，但對於我們的每個輸入標記 (Token)，我們有 13 個長度為 768 的單獨向量。\n",
    "\n",
    "為了獲取單獨的向量，我們需要結合一些層向量，但是哪一層或哪幾層的組合能提供最佳表示呢？\n",
    "\n",
    "不幸的是，沒有一個簡單的答案，不過讓我們嘗試通過將最後四層相加來創建詞向量，為每個標記 (Token) 生成一個詞向量。\n",
    "\n",
    "每個向量的長度將是維持長度 768。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 13 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [13 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "pprint(f'Shape is: {len(token_vecs_sum)} x {len(token_vecs_sum[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "為了確認這些向量的值確實是依賴於上下文的，讓我們看看在我們示例句子中「bank」這個詞的不同實例：\n",
    "\n",
    "「After stealing money from the **bank vault**, the **bank robber** was seen fishing on the Mississippi **river bank**.」\n",
    "\n",
    "讓我們找出這個示例句子中三個「bank」實例的索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, id in enumerate(text_to_token_id[\"input_ids\"][0]):\n",
    "    pprint(f'{i} {tokenizer.decode(id)}')\n",
    "\n",
    "# They are at 6, 10, and 19.\n",
    "# We can try printing out their vectors to compare them.\n",
    "pprint('First 5 vector values for each instance of \"bank\".')\n",
    "pprint('')\n",
    "pprint(f\"bank vault  {token_vecs_sum[6][:5]}\")\n",
    "pprint(f\"bank robber {token_vecs_sum[10][:5]}\")\n",
    "pprint(f\"river bank  {token_vecs_sum[19][:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that the values differ, but let’s calculate the cosine similarity between the vectors to make a more precise comparison.\n",
    "# Cosine similarity measures the cosine of the angle between two vectors in a high-dimensional space.\n",
    "# It is a value between -1 and 1, with 1 indicating that the two vectors are identical and 0 indicating that they are orthogonal (i.e., they have no correlation).\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Calculate the cosine similarity between the word bank\n",
    "# in \"bank robber\" vs \"bank vault\" (same meaning).\n",
    "same_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[6])\n",
    "\n",
    "# Calculate the cosine similarity between the word bank\n",
    "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
    "diff_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[19])\n",
    "\n",
    "pprint('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\n",
    "pprint('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding (PE)\n",
    "\n",
    "![](https://www.alexisalulema.com/wp-content/uploads/2022/08/positional.encoding.png)\n",
    "\n",
    "Embedding 向量為 Transformer 提供了大量有關序列中詞語之間關係的信息。然而，仍然需要信息來指示詞語在序列中的位置，這就是位置編碼過程的用途。\n",
    "\n",
    "初始序列中的每個詞都必須具有位置編碼（PE）信息，但由於 Transformer 的主要重點是注意力機制，因此這個向量的生成必須簡單。\n",
    "\n",
    "這項任務的挑戰在於為位置編碼函數的每個輸出向量生成一個維度為 $\\ d_{model} = 512 $ 的向量。架構的作者找到了一種巧妙的方法，使用正弦和餘弦值表示位置編碼。\n",
    "\n",
    "$$ PE_{\\text{sin}}(\\text{pos}, 2i) = \\sin\\left(\\frac{\\text{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right) $$\n",
    "\n",
    "$$ PE_{\\text{cos}}(\\text{pos}, 2i+1) = \\cos\\left(\\frac{\\text{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right) $$\n",
    "\n",
    "最後，詞編碼向量應該加到位置編碼向量上，然後將結果輸入到編碼器/解碼器區塊中：\n",
    "\n",
    "$$ Embedding_{output} = Embedding_{initial} + PE_{word-position} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Encoder\n",
    "\n",
    "每個解碼器區塊由兩個子層組成：\n",
    "\n",
    "1. 多頭注意力 (Multi-head attention)\n",
    "2. 前饋神經網路 (Feedforward Network)\n",
    "\n",
    "![](https://www.alexisalulema.com/wp-content/uploads/2022/08/encoders.inside-768x315.png)\n",
    "\n",
    "在開始解釋這兩個組件之前，有必要先了解自注意力 (self-attention) 機制。\n",
    "\n",
    "#### Self-Attention\n",
    "\n",
    "考慮以下句子：\n",
    "\n",
    "```\n",
    "John and Paul wrote several songs when they were inspired.\n",
    "```\n",
    "\n",
    "在這個句子中，自注意力機制計算每個詞的表示，並且與句子中其他詞的關係提供了更多關於該詞的信息。例如，「they」這個詞應該與「John」和「Paul」相關，而不是與「songs」相關。\n",
    "\n",
    "讓我們簡單以一個視覺化的方式來解釋自注意力機制：\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = \"google-bert/bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "text = \"John and Paul wrote several songs when they were inspired.\"\n",
    "# Tokenize input text\n",
    "text_to_token_id = tokenizer(\n",
    "    text,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_name,\n",
    "    output_attentions=True, # Whether the model returns attentions weights\n",
    "    )\n",
    "\n",
    "# Run the text through BERT\n",
    "with torch.no_grad():\n",
    "    outputs = model(**text_to_token_id)\n",
    "    # Retrieve attention from model outputs\n",
    "    attention = outputs.attentions\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(text_to_token_id['input_ids'][0])  # Convert input ids to token strings\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from bertviz import (\n",
    "  model_view,\n",
    "  head_view,\n",
    ")\n",
    "\n",
    "# Visualize the self-attention of the model\n",
    "# Review the relationship of term 'they' along with 'John' and 'Paul'\n",
    "head_view(\n",
    "  attention,\n",
    "  tokens,\n",
    "  layer=8,\n",
    "  heads=[9],\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以一個更簡單的例子「The sky is blue」來理解自注意力機制如何運作，編碼器 (Encoder) 接收句子中每個詞的維度為 $\\ d_{model} = 512 $ 的 Word Embedding 向量，例如：\n",
    "\n",
    "$$ x_1 = [3.23, 0.65, ..., 4.78] \\Rightarrow \\text{\"The\"} $$\n",
    "$$ x_2 = [1.26, 6.35, ..., 7.99] \\Rightarrow \\text{\"sky\"} $$\n",
    "$$ x_3 = [9.25, 1.68, ..., 4.26] \\Rightarrow \\text{\"is\"} $$\n",
    "$$ x_4 = [6.84, 2.98, ..., 11.48] \\Rightarrow \\text{\"blue\"} $$\n",
    "\n",
    "有了這些向量，我們可以組裝 Embedding 矩陣 $\\ X $，其維度為$\\ d = [4 \\times 512] $：\n",
    "\n",
    "![](https://alexisalulema.com/wp-content/uploads/2022/08/embedding_matrix.gif)\n",
    "\n",
    "我們將從這個矩陣 $\\ X $ 中創建三個額外的矩陣，作為「自注意力機制」的一部分：\n",
    "\n",
    "* $\\ Q $, query matrix\n",
    "* $\\ K $, key matrix\n",
    "* $\\ V $, value matrix\n",
    "\n",
    "要創建這些陣列，我們還需要三個新的權重矩陣 (weight matrices)。\n",
    "\n",
    "原始論文中使用的維度是 $\\ d_k = 64 $；因此，權重向量的維度將是 $\\ d_{model} \\times d_k \\Rightarrow 512 \\times 64 $，這些矩陣會用隨機值初始化：\n",
    "\n",
    "* $\\ W^Q $, query weight matrix\n",
    "* $\\ W^K $, key weight matrix\n",
    "* $\\ W^V $, value weight matrix\n",
    "\n",
    "權重矩陣攜帶了在訓練過程中學到的最佳值，因此每個矩陣 $\\ Q $、$\\ K $ 和 $\\ V $ 是 Embedding 矩陣 $\\ X $ 與相應權重矩陣的乘積，這會生成 $\\ 4 \\times 64 $ 的矩陣：\n",
    "\n",
    "* $\\ Q = X × W^Q $\n",
    "* $\\ K = X × W^K $\n",
    "* $\\ V = X × W^V $ \n",
    "\n",
    "![](https://alexisalulema.com/wp-content/uploads/2022/08/QKV.gif)\n",
    "\n",
    "每個矩陣中的四行分別代表句子「The sky is blue」中的每個詞。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Self-attention Mechanism Process\n",
    "\n",
    "1. 計算點積 (dot product) $\\ Q \\cdot K^T $\n",
    "\n",
    "![](https://www.alexisalulema.com/wp-content/uploads/2022/08/QdotKT-1-1024x140.gif)\n",
    "\n",
    "結果陣列的元素表示詞語之間的關係。例如，$\\ q_1 \\cdot k_1 $ 是詞「The」與其自身的關係，$\\ q_1 \\cdot k_3 $ 是「The」與「is」之間的關係。「sky」與「blue」之間的關係 $\\ q_2 \\cdot k_4 $ 會有稍高的值，因為名詞和形容詞之間存在關係。例如：\n",
    "\n",
    "![](https://alexisalulema.com/wp-content/uploads/2022/08/QdotKT.2.gif)\n",
    "\n",
    "這樣，我們可以說計算查詢矩陣 $\\ Q $ 和鍵矩陣 $\\ K^T $ 之間的點積，基本上給出了相似度值，這有助於我們理解句子中每個詞與所有其他詞的相似程度。\n",
    "\n",
    "2. 計算 $\\ QK^T / \\sqrt{d_k} $。這個操作有助於獲得穩定的梯度，其中 $\\ d_k = 64 $ 是鍵向量的維度。\n",
    "\n",
    "![](https://alexisalulema.com/wp-content/uploads/2022/08/QdotKTSqrtDK.gif)\n",
    "\n",
    "結果矩陣的值必須正規化，如果我們使用函數 $\\ \\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) $，我們可以將值轉化為 0 到 1 的範圍。\n",
    "\n",
    "每行的值之和等於 1。通過這些值，我們可以理解句子中每個詞與所有其他詞的關係。這稱為分數矩陣 (score matrix)。\n",
    "\n",
    "3. 接下來，我們需要計算注意力矩陣 (attention matrix) $\\ Z $：\n",
    "\n",
    "![](https://www.alexisalulema.com/wp-content/uploads/2022/08/SoftmaxV.gif)\n",
    "\n",
    "![](https://www.alexisalulema.com/wp-content/uploads/2022/08/Z.SoftmaxV-1024x148.gif)\n",
    "\n",
    "注意力矩陣 $\\ Z $ 是一個具有 4 行和 512 列的矩陣，對於示例句子來說。每行對應於相應詞的自注意力 (self-attention) 向量。\n",
    "\n",
    "自注意力 (self-attention) 機制被稱為縮放點積注意力 (scaled dot product attention)，因為我們在計算向量 $\\ Q $ 和 $\\ K $ 之間的點積並通過 $\\ \\sqrt{d_k} $ 來縮放值。\n",
    "\n",
    "我們也可以透過視覺化的方式來理解自注意力機制中 $\\ Ｑ $ 和 $\\ Ｋ $ 的作用：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertviz.transformers_neuron_view import BertModel, BertTokenizer\n",
    "from bertviz.neuron_view import show\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "  'bert-base-uncased',\n",
    "  do_lower_case=True)\n",
    "model = BertModel.from_pretrained(\n",
    "  'bert-base-uncased',\n",
    "  output_attentions=True)\n",
    "show(\n",
    "  model=model,\n",
    "  model_type='bert',\n",
    "  tokenizer=tokenizer,\n",
    "  sentence_a=text,\n",
    "  layer=8,\n",
    "  head=9,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-head attention\n",
    "\n",
    "對於 Transformer，我們將計算多個注意力 (multiple attention) 矩陣。但為什麼我們需要多個陣列呢？這有助於在語境中詞義模糊的情況下，例如：\n",
    "\n",
    "```\n",
    "Tom was crying because he was blue.\n",
    "```\n",
    "\n",
    "單一的注意力機制可能會決定 Tom 哭泣是因為他的顏色是藍色，這是由於「Tom」這個詞的影響。如果大多數句子中「blue」表示顏色，只有一個「注意力頭 (attention head)」的機制將正確地學習到它是一種顏色。然而，擁有「多個注意力頭 (multiple attention heads)」，其中一個注意力機制更有可能從句子中學習到「blue」表示心情，通過串聯「多個注意力頭 (multiple attention heads)」的結果，注意力矩陣將更加準確。\n",
    "\n",
    "我們如何計算多個注意力矩陣？假設我們要計算兩個注意力矩陣：$\\ Z_1 $ 和 $\\ Z_2 $。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
