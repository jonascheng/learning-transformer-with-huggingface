{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# transformers not support NumPy 2.0 yet\n",
    "!pip install -q numpy~=1.26.4 transformers~=4.46.2\n",
    "# for visualization self-attention\n",
    "!pip install -q bertviz~=1.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer: how do they work internally?\n",
    "\n",
    "## Transformer in a nutshell\n",
    "\n",
    "Transformer 由 Encoder 和 Decoder 两部分组成，Encoder 接受 **输入** 然後生成 **向量** 表示這個输入，Decoder 接受 **向量** 然後逐一生成 **输出**。\n",
    "![transformer in a nutshell](https://www.alexisalulema.com/wp-content/uploads/2022/08/encoder-decoder-1-1024x275.png)\n",
    "\n",
    "### Encoder\n",
    "\n",
    "Transformer 由 N 個 Encoder 組成，每個 Encoder 將其輸出送到下一個 Encoder。最終的 Encoder 返回 **輸入** 的 **向量** 表示。為了說明，從現在開始，我們將使用 N=2 的值。\n",
    "\n",
    "![encoder in a nutshell](https://www.alexisalulema.com/wp-content/uploads/2022/08/encoders.png)\n",
    "\n",
    "### Decoder\n",
    "\n",
    "同樣地，我們可以有 N 個 Decoder（假設 N=2）。Encoder 生成的 **向量** 表示是所有 Decoder 的輸入；也就是說，一個 Decoder 接收兩個輸入，一個來自前一個 Decoder，另一個來自 Encoder 生成的 **向量** 表示。\n",
    "\n",
    "![decoder in a nutshell](https://www.alexisalulema.com/wp-content/uploads/2022/08/encoderNdecoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Transformer\n",
    "\n",
    "Google Research 和 Google Brain 的成員最初在 2017 年的論文《[Attention is all you need](https://arxiv.org/abs/1706.03762?context=cs)》中提出了 Transformer。目前是 NLP 中最受歡迎的架構之一。\n",
    "\n",
    "![Original Transformer Architecture, Source: Attention is all you need](https://www.alexisalulema.com/wp-content/uploads/2022/08/image-695x1024.png)\n",
    "\n",
    "Transformer 超越了基於 LSTM 和 RNN 的其他現有架構，獲得了更高的評估結果和更快的訓練速度。Transformer 的注意力機制是一種「word-to-word」的操作，它會找出序列中每個詞與其他詞之間的關係，包括詞本身。\n",
    "\n",
    "在本文中，我將以最簡單的方式解釋 Transformer 的架構和內部運作。\n",
    "\n",
    "### Input Embedding\n",
    "\n",
    "Transformer 架構首先是 Input Embedding 層，它將輸入序列轉換為維度為 $\\d_{model} = 512$ 的向量。\n",
    "\n",
    "![Input Embedding](https://www.alexisalulema.com/wp-content/uploads/2022/08/input.embedding.png)\n",
    "\n",
    "> $\\ d_{model} = 512 $ 的值是由架構設計者設定的，這個維度的值可以根據目標進行修改。\n",
    "\n",
    "Tokenizer 通過將文本正規化來將輸入流轉換為 token；此外，它還會提供一個整數表示（基於現有詞彙），該表示將用於 Embedding 過程。例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from pprint import pprint\n",
    "\n",
    "model_name = \"google-bert/bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "pprint(tokenizer)\n",
    "# vocab_szie: 30522\n",
    "# model_max_length: 512\n",
    "# 0: [PAD]\n",
    "# 100: [UNK]\n",
    "# 101: [CLS]\n",
    "# 102: [SEP]\n",
    "# 103: [MASK]\n",
    "\n",
    "pprint(list(tokenizer.vocab.keys())[1000:1010])\n",
    "# display 10 vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a this course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(\n",
    "  raw_inputs,\n",
    "  padding=True,         # Activates and controls padding\n",
    "  truncation=True,      # Activates and controls truncation\n",
    "  return_tensors=\"pt\",  # Return PyTorch tensor objects\n",
    "  )\n",
    "\n",
    "pprint(inputs)\n",
    "# 1. 第二句子長度不夠，所以會被補上 [PAD] token\n",
    "# 2. Attention mask 會標記出哪些是 padding 的 token\n",
    "\n",
    "tokenizer.decode(inputs[\"input_ids\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果將此模型應用在中文文本上，會發生什麼狀況呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilingual\n",
    "# model_name = \"google-bert/bert-base-multilingual-uncased\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# pprint(tokenizer)\n",
    "\n",
    "raw_inputs = [\n",
    "    \"我門正在學習目前正夯的變形金剛模型！\",\n",
    "    \"你們喜歡這個課程嗎？\"\n",
    "]\n",
    "inputs = tokenizer(\n",
    "  raw_inputs,\n",
    "  padding=True,         # Activates and controls padding\n",
    "  truncation=True,      # Activates and controls truncation\n",
    "  return_tensors=\"pt\",  # Return PyTorch tensor objects\n",
    "  )\n",
    "\n",
    "tokenizer.decode(inputs[\"input_ids\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下來，Embedding 從分詞後的向量中獲取數據。對於每個詞，生成一個大小為 $\\ d_{model} = 512 $ 的向量。例如，對於表示意思的詞，Embedding 向量應該是相似的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pprint import pprint\n",
    "\n",
    "model_name = \"google-bert/bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
    "text = '''\n",
    "After stealing money from the bank vault, the bank robber was seen\n",
    "fishing on the Mississippi river bank.\n",
    "'''\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "text_to_token_id = tokenizer(\n",
    "    text,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Display the words with their indeces.\n",
    "# 請注意，這裡的 token id 是對應到 vocab 的 index\n",
    "# 此時 bank 這個字具有相同的 token id\n",
    "for id in text_to_token_id[\"input_ids\"][0]:\n",
    "    pprint('{:<12} {:>6,}'.format(tokenizer.decode(id), id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\n",
    "  model_name,\n",
    "  output_hidden_states=True,  # Whether the model returns all hidden-states.\n",
    "  )\n",
    "\n",
    "# 這個 model 有 12 層的 transformer blocks, 及第一層的 embedding 層, 共 13 層\n",
    "pprint(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Run the text through BERT, and collect all of the hidden states produced from all 12 layers.\n",
    "with torch.no_grad():\n",
    "    outputs = model(**text_to_token_id)\n",
    "    hidden_states = outputs.hidden_states\n",
    "\n",
    "n_hidden_states = len(hidden_states)\n",
    "pprint(f\"Number of layers: {n_hidden_states} (initial embeddings + 12 BERT layers)\")\n",
    "layer_i = 0\n",
    "\n",
    "n_batches = len(hidden_states[layer_i])\n",
    "pprint(f\"Number of batches: {n_batches}\")\n",
    "batch_i = 0\n",
    "\n",
    "n_tokens = len(hidden_states[layer_i][batch_i])\n",
    "pprint(f\"Number of tokens: {n_tokens}\")\n",
    "token_i = 0\n",
    "\n",
    "n_hidden_units = len(hidden_states[layer_i][batch_i][token_i])\n",
    "pprint(f\"Number of hidden units: {n_hidden_units}\")\n",
    "\n",
    "pprint(f\"Number of unique values: {n_hidden_states * n_batches * n_tokens * n_hidden_units}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這個模型的完整隱藏狀態集，存儲在 hidden_states 物件中，這個物件有四個維度，順序如下：\n",
    "\n",
    "1. 層數（13 層）\n",
    "2. 批次數（1 句話）\n",
    "3. 詞/標記 (Token) 數（我們句子中的 22 個 Token）\n",
    "4. 隱藏單元/特徵數（768 個特徵）\n",
    "\n",
    "僅僅為了表示我們的一句話，就需要 219,648 個唯一值！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor.\n",
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "token_embeddings.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s get rid of the “batches” dimension since we don’t need it.\n",
    "# Remove dimension 1, the \"batches\".\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current dimensions: [# layers, # tokens, # features]\n",
    "# Desired dimensions: [# tokens, # layers, # features]\n",
    "\n",
    "# Swap dimensions 0 and 1.\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "現在，我們該如何處理這些隱藏狀態呢？我們希望為每個標記 (Token) 獲取單獨的向量，但對於我們的每個輸入標記 (Token)，我們有 13 個長度為 768 的單獨向量。\n",
    "\n",
    "為了獲取單獨的向量，我們需要結合一些層向量，但是哪一層或哪幾層的組合能提供最佳表示呢？\n",
    "\n",
    "不幸的是，沒有一個簡單的答案，不過讓我們嘗試通過將最後四層相加來創建詞向量，為每個標記 (Token) 生成一個詞向量。\n",
    "\n",
    "每個向量的長度將是維持長度 768。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 13 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [13 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "pprint(f'Shape is: {len(token_vecs_sum)} x {len(token_vecs_sum[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "為了確認這些向量的值確實是依賴於上下文的，讓我們看看在我們示例句子中「bank」這個詞的不同實例：\n",
    "\n",
    "「After stealing money from the **bank vault**, the **bank robber** was seen fishing on the Mississippi **river bank**.」\n",
    "\n",
    "讓我們找出這個示例句子中三個「bank」實例的索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, id in enumerate(text_to_token_id[\"input_ids\"][0]):\n",
    "    pprint(f'{i} {tokenizer.decode(id)}')\n",
    "\n",
    "# They are at 6, 10, and 19.\n",
    "# We can try printing out their vectors to compare them.\n",
    "pprint('First 5 vector values for each instance of \"bank\".')\n",
    "pprint('')\n",
    "pprint(f\"bank vault  {token_vecs_sum[6][:5]}\")\n",
    "pprint(f\"bank robber {token_vecs_sum[10][:5]}\")\n",
    "pprint(f\"river bank  {token_vecs_sum[19][:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that the values differ, but let’s calculate the cosine similarity between the vectors to make a more precise comparison.\n",
    "# Cosine similarity measures the cosine of the angle between two vectors in a high-dimensional space.\n",
    "# It is a value between -1 and 1, with 1 indicating that the two vectors are identical and 0 indicating that they are orthogonal (i.e., they have no correlation).\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Calculate the cosine similarity between the word bank\n",
    "# in \"bank robber\" vs \"bank vault\" (same meaning).\n",
    "same_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[6])\n",
    "\n",
    "# Calculate the cosine similarity between the word bank\n",
    "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
    "diff_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[19])\n",
    "\n",
    "pprint('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\n",
    "pprint('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding (PE)\n",
    "\n",
    "![](https://www.alexisalulema.com/wp-content/uploads/2022/08/positional.encoding.png)\n",
    "\n",
    "Embedding 向量為 Transformer 提供了大量有關序列中詞語之間關係的信息。然而，仍然需要信息來指示詞語在序列中的位置，這就是位置編碼過程的用途。\n",
    "\n",
    "初始序列中的每個詞都必須具有位置編碼（PE）信息，但由於 Transformer 的主要重點是注意力機制，因此這個向量的生成必須簡單。\n",
    "\n",
    "這項任務的挑戰在於為位置編碼函數的每個輸出向量生成一個維度為 $\\ d_{model} = 512 $ 的向量。架構的作者找到了一種巧妙的方法，使用正弦和餘弦值表示位置編碼。\n",
    "\n",
    "$$ PE_{\\text{sin}}(\\text{pos}, 2i) = \\sin\\left(\\frac{\\text{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right) $$\n",
    "\n",
    "$$ PE_{\\text{cos}}(\\text{pos}, 2i+1) = \\cos\\left(\\frac{\\text{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right) $$\n",
    "\n",
    "最後，詞編碼向量應該加到位置編碼向量上，然後將結果輸入到編碼器/解碼器區塊中：\n",
    "\n",
    "$$ Embedding_{output} = Embedding_{initial} + PE_{word-position} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The encoder\n",
    "\n",
    "每個解碼器區塊由兩個子層組成：\n",
    "\n",
    "1. 多頭注意力 (Multi-head attention)\n",
    "2. 前饋神經網路 (Feedforward Network)\n",
    "\n",
    "![](https://www.alexisalulema.com/wp-content/uploads/2022/08/encoders.inside-768x315.png)\n",
    "\n",
    "在開始解釋這兩個組件之前，有必要先了解自注意力 (self-attention) 機制。\n",
    "\n",
    "#### Self-attention Mecanism\n",
    "\n",
    "考慮以下句子：\n",
    "\n",
    "```\n",
    "John and Paul wrote several songs when they were inspired.\n",
    "```\n",
    "\n",
    "在這個句子中，自注意力機制計算每個詞的表示，並且與句子中其他詞的關係提供了更多關於該詞的信息。例如，「they」這個詞應該與「John」和「Paul」相關，而不是與「songs」相關。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = \"google-bert/bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "text = \"John and Paul wrote several songs when they were inspired.\"\n",
    "# Tokenize input text\n",
    "text_to_token_id = tokenizer(\n",
    "    text,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_name,\n",
    "    output_attentions=True, # Whether the model returns attentions weights\n",
    "    )\n",
    "\n",
    "# Run the text through BERT\n",
    "with torch.no_grad():\n",
    "    outputs = model(**text_to_token_id)\n",
    "    # Retrieve attention from model outputs\n",
    "    attention = outputs.attentions\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(text_to_token_id['input_ids'][0])  # Convert input ids to token strings\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from bertviz import (\n",
    "  model_view,\n",
    "  head_view,\n",
    ")\n",
    "\n",
    "# Visualize the self-attention of the model\n",
    "# Review the relationship of term 'they' along with 'John' and 'Paul'\n",
    "head_view(\n",
    "  attention,\n",
    "  tokens,\n",
    "  layer=8,\n",
    "  heads=[9],\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertviz.transformers_neuron_view import BertModel, BertTokenizer\n",
    "from bertviz.neuron_view import show\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "  'bert-base-uncased',\n",
    "  do_lower_case=True)\n",
    "model = BertModel.from_pretrained(\n",
    "  'bert-base-uncased',\n",
    "  output_attentions=True)\n",
    "show(\n",
    "  model=model,\n",
    "  model_type='bert',\n",
    "  tokenizer=tokenizer,\n",
    "  sentence_a=text,\n",
    "  layer=8,\n",
    "  head=9,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
