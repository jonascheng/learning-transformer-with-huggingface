{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {},
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# transformers not support NumPy 2.0 yet\n",
    "!pip install -q numpy~=1.26.4 transformers~=4.46.2\n",
    "!pip install -q datasets~=3.2.0 pydantic~=2.10.4\n",
    "!pip install -q seqeval~=1.2.2 evaluate~=0.4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練 PII 偵測模型\n",
    "\n",
    "在這個筆記本中，我們將展示如何使用 `transformers` 套件訓練 PII (個人識別資訊) 偵測模型。我們將使用 `transformers` 套件中的 [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer) 類別來微調一個 Encoder-Only 架構的 BERT 模型。我們將利用到標記分類 ([Token classification](https://huggingface.co/docs/transformers/tasks/token_classification)) 進行下游任務 (downstream task) 的訓練。\n",
    "\n",
    "標記分類為句子中的單個標記分配標籤。最常見的標記分類任務之一是命名實體識別 (NER)。NER 試圖為句子中的每個實體找到一個標籤，例如人名、地點或組織。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "from transformers import (\n",
    "  AutoTokenizer,\n",
    "  AutoModelForTokenClassification,\n",
    "  DataCollatorForTokenClassification,\n",
    "  pipeline,\n",
    "  TrainingArguments,\n",
    "  Trainer,\n",
    ")\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from pprint import pprint\n",
    "\n",
    "import ast\n",
    "import torch\n",
    "\n",
    "# 檢查是否有 GPU 可以使用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下載資料\n",
    "\n",
    "從 Kaggle 下載 PII External Dataset，並解壓縮到 `sample_data` 資料夾。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# 從 Kaggle 下載 PII External Dataset\n",
    "!curl -L -o ./sample_data/pii-external-dataset.zip \\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/alejopaullier/pii-external-dataset\n",
    "\n",
    "# 解壓縮\n",
    "!unzip -o -q ./sample_data/pii-external-dataset.zip -d ./sample_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# The full `train` split\n",
    "immutable_dataset = load_dataset('csv', data_files='sample_data/pii_dataset.csv', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料包含什麼？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顯示原始資料中包含的 features 以及筆數\n",
    "immutable_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢視資料集中的第一筆資料\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.DataFrame(immutable_dataset[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這個表個結構，包含多項資訊，在這個筆記本中，我們將使用：\n",
    "\n",
    "* `text`: 一個字串，包含 PII 資訊，將用作輸入。\n",
    "* `token`: 依據 `text` 切割後的字元及字符，將用作訓練。\n",
    "* `label`: 對應每個 `token` 的標記，其中包含 PII 資訊的類別，將用作訓練。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 保留必要 features: 'text', 'tokens', 'labels'\n",
    "dataset = immutable_dataset.remove_columns([\n",
    "  'document', 'trailing_whitespace', 'prompt', 'prompt_id', 'name',\n",
    "  'email', 'phone', 'job', 'address', 'username', 'url', 'hobby', 'len'])\n",
    "\n",
    "# 將 'tokens' 與 'labels' 從 string 轉換為 list (CSV 檔案讀取後會變成 string)\n",
    "dataset = dataset.map(lambda x: {'tokens': ast.literal_eval(x['tokens']), 'labels': ast.literal_eval(x['labels'])})\n",
    "\n",
    "# 確認 tokens 長度與 labels 長度相等，避免有缺失的情況,\n",
    "dataset = dataset.filter(lambda x: len(x['tokens']) == len(x['labels']))\n",
    "\n",
    "# 將 tokens 欄位重新命名為 words 避免與後面的 tokens 概念混淆\n",
    "dataset = dataset.rename_column('tokens', 'words')\n",
    "\n",
    "# 顯示處理後的資料\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Reserve 0.1% of the training set for testing\n",
    "test_dataset = dataset.train_test_split(\n",
    "  test_size=0.001, # 0.1% of the data is used for testing\n",
    "  shuffle=False, # Ensure that train and validation sets are the same across runs\n",
    "  )\n",
    "# Split into 80% training and 20% validation sets\n",
    "train_dataset = test_dataset['train'].train_test_split(\n",
    "  test_size=0.2, # 20% of the data is used for validation\n",
    "  shuffle=False, # Ensure that train and test sets are the same across runs\n",
    "  )\n",
    "dataset = DatasetDict({\n",
    "  'train': train_dataset['train'],\n",
    "  'validation': train_dataset['test'],\n",
    "  'test': test_dataset['test'],\n",
    "  })\n",
    "# 顯示處理後的資料\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 顯示前 first_n_data 筆資料\n",
    "first_n_data = 3\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.DataFrame(dataset['train'].select(range(first_n_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 並列顯示前 max_display 個 words 與 labels\n",
    "max_display = 50\n",
    "\n",
    "def show_nth_data(dataset, nth, max_display):\n",
    "    words = dataset[nth]['words'][:max_display]\n",
    "    labels = dataset[nth]['labels'][:max_display]\n",
    "    line1 = \"\"\n",
    "    line2 = \"\"\n",
    "    for word, label in zip(words, labels): # 逐一取出 word 與 label\n",
    "        # 計算 word 與 label 的最大長度, 並將 word 與 label 用空白補齊至相同長度\n",
    "        max_length = max(len(word), len(label))\n",
    "        line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "        line2 += label + \" \" * (max_length - len(label) + 1)\n",
    "    pprint(line1, width=200)\n",
    "    pprint(line2, width=200)\n",
    "    print()\n",
    "\n",
    "def show_data(dataset, first_n_data, max_display):\n",
    "    for i in range(first_n_data):\n",
    "        show_nth_data(dataset, i, max_display)\n",
    "\n",
    "show_data(dataset['test'], first_n_data, max_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "為方便檢視 `token` 與 `label` 的關係，所以並列顯示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料中的 BIO 標注\n",
    "\n",
    "IOB 格式（inside, outside, beginning 的縮寫），也常被稱為 BIO 格式，是計算語言學中用於標記任務（例如命名實體識別 NER，詞性標記 POS）的常見標記格式。\n",
    "\n",
    "* B - for the first token of a named entity\n",
    "* I - for tokens inside named entity's\n",
    "* O - for tokens outside any named entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 顯示 BIO 標注\n",
    "label_names = set()\n",
    "for data in dataset['train']:\n",
    "    # 將 labels 欄位轉換為 set 並更新 label_names\n",
    "    label_names.update(data['labels'])\n",
    "# convert set to list and sort label names\n",
    "label_names = list(label_names)\n",
    "print('=== 資料集中的標籤 ===')\n",
    "pprint(label_names, compact=True)\n",
    "print()\n",
    "print(f'標籤數量: {len(label_names)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方便後續訓練過程，我們將建立兩個對照表，一個是將 BIO 標籤映射到一個整數編碼 (One-hot encoding)，另一個是將整數編碼映射回 BIO 標籤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整數標籤到 BIO 標籤的映射\n",
    "id2tag = dict(enumerate(label_names))\n",
    "\n",
    "print('=== id2tag ===')\n",
    "pprint(sorted(id2tag.items(), key=lambda x: x[0]), compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIO 標籤到整數標籤的映射\n",
    "tag2id = dict((v, k) for k, v in id2tag.items())\n",
    "\n",
    "print('=== tag2id ===')\n",
    "pprint(sorted(tag2id.items(), key=lambda x: x[1]), compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練參數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "當我們在微調或訓練機器學習模型時，有幾個重要的參數需要設定。這些參數會影響模型的訓練效果和性能。以下是這些參數的簡單解釋：\n",
    "\n",
    "* 批次大小（Batch Size）：每次訓練迭代中使用的樣本數量。假設你有1000個數據點，批次大小為32，這意味著模型每次會使用32個數據點來更新權重。當所有數據點都被使用完一次後，這稱為一個 epoch。較大的批次大小可以加速訓練，但需要更多的內存；較小的批次大小則更穩定，但訓練速度較慢。\n",
    "* 訓練輪數（Epochs）：完整遍歷訓練數據集的次數。如果你設定 epochs 為10，這意味著模型會完整地看10次訓練數據集。更多的 epochs 可以讓模型學習得更充分，但過多的 epochs 可能會導致過擬合（Overfitting / 模型在訓練數據上表現很好，但在新數據上表現不好）。\n",
    "* 學習率（Learning Rate）：每次更新模型權重時的步伐大小。學習率決定了模型在每次更新時應該調整多少權重。較高的學習率會使模型快速學習，但可能會跳過最佳解；較低的學習率會使模型穩定學習，但訓練時間較長。學習率需要仔細調整，過高或過低都會影響模型的性能。\n",
    "* 隨機失活（Dropout）：在訓練過程中隨機忽略一些神經元的比例。Dropout 是一種正則化技術，用於防止過擬合。它通過在每次訓練步驟中隨機忽略一些神經元來強制模型學習更穩健的特徵。適當的 Dropout 可以提高模型的泛化能力，但過高的 Dropout 可能會導致模型欠擬合（Underfitting / 模型在訓練數據和新數據上都表現不好）。\n",
    "* 權重衰減（Weight Decay）：在每次更新權重時，對權重施加的正則化項。Weight Decay 是另一種正則化技術，用於防止過擬合。它通過在每次更新時對權重施加懲罰，使權重保持較小的值。適當的 Weight Decay 可以提高模型的泛化能力，但過高的 Weight Decay 可能會導致模型欠擬合。\n",
    "\n",
    "這些參數在模型訓練中扮演著重要角色，影響著模型的學習速度、穩定性和泛化能力。調整這些參數需要根據具體的數據集和任務進行實驗和調整，以找到最佳的組合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 訓練相關設定\n",
    "class Config(BaseModel):\n",
    "  model_name: str = 'dslim/distilbert-NER' # 使用蒸餾模型，降低參數量，加快訓練速度\n",
    "  saved_model_path: str = 'sample_data/saved_decoder_model' # path to save the trained model\n",
    "  train_batch_size: int = 4 # size of the input batch in training\n",
    "  eval_batch_size: int = 4 # size of the input batch in evaluation\n",
    "  epochs: int = 1 # number of times to iterate over the entire training dataset\n",
    "  lr: float = 2e-5 # learning rate, controls how fast or slow the model learns\n",
    "  weight_decay: float = 0.01 # weight decay, helps the model stay simple and avoid overfitting by penalizing large weights.\n",
    "  tags: list # BIO 標註的標籤列表\n",
    "  id2tag: dict # 整數標籤到 BIO 標籤的映射\n",
    "  tag2id: dict # BIO 標籤到整數標籤的映射\n",
    "  num_tags: int # 標籤數量\n",
    "\n",
    "config = Config(\n",
    "  tags=label_names,\n",
    "  id2tag=id2tag,\n",
    "  tag2id=tag2id,\n",
    "  num_tags=len(label_names)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning 前的表現"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入預訓練分詞器 (Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 透過預訓練模型取得 Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  config.model_name,\n",
    ")\n",
    "pprint(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確認為 fast tokenizer，方便後續做資料預處理。\n",
    "\n",
    "> [Fast tokenizers’ special powers](https://huggingface.co/learn/nlp-course/chapter6/3?fw=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確認 tokenizer 是否為 fast tokenizer\n",
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入預訓練模型\n",
    "\n",
    "透過 `AutoModelForTokenClassification` 載入預訓練模型，並確認模型的分類數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "  config.model_name,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這是一個典型的 Encoder-Only 架構，其中包含一個 BERT 模型和一個線性分類器。BERT 模型用於提取特徵，線性分類器用於將特徵映射到分類標籤。\n",
    "\n",
    "觀察預訓練模型的分類數為 9。\n",
    "\n",
    "```json\n",
    "(classifier): Linear(in_features=768, out_features=9, bias=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning 前的表現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 載入預訓練模型\n",
    "classifier = pipeline(\n",
    "  task=\"token-classification\",\n",
    "  model=model,\n",
    "  tokenizer=tokenizer,\n",
    "  device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 合併顯示預測結果\n",
    "def show_prediction(text, classifier):\n",
    "    result = classifier(text) # 預測結果\n",
    "    line1 = \"\"\n",
    "    line2 = \"\"\n",
    "    for r in result:\n",
    "        # 取出預測結果中的 word 與 entity\n",
    "        word = r['word']\n",
    "        label = r['entity']\n",
    "        # 計算 word 與 entity 的最大長度, 並將 word 與 entity 用空白補\n",
    "        max_length = max(len(word), len(label))\n",
    "        line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "        line2 += label + \" \" * (max_length - len(label) + 1)\n",
    "    pprint(line1)\n",
    "    pprint(line2)\n",
    "    print()\n",
    "\n",
    "# 顯示預訓練模型預測結果，僅顯示有被標注的部分\n",
    "for val in dataset['test']: # 逐一取出測試資料\n",
    "  print(f'輸入: {val[\"text\"]}')\n",
    "  show_prediction(val[\"text\"], classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料預處理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 了解 Tokenizer 行為\n",
    "\n",
    "分詞器添加了模型使用的特殊標記（[CLS] 在開頭和 [SEP] 在結尾），並且大多數單詞保持不變。然而，某些單詞 (word) 會被分為數個子詞 (subword)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "# 以第 data_nth 筆資料為例\n",
    "data_cat = 'test'\n",
    "data_nth = 2\n",
    "input_words = dataset[data_cat][data_nth][\"words\"]\n",
    "input_labels = dataset[data_cat][data_nth][\"labels\"]\n",
    "# 對 input_words 進行分詞\n",
    "input_token_ids = tokenizer(\n",
    "  input_words,\n",
    "  # is_split_into_words: Whether or not the input is already pre-tokenized (e.g., split into words).\n",
    "  # If set to True, the tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace) which it will tokenize.\n",
    "  # This is useful for NER or token classification.\n",
    "  is_split_into_words=True)\n",
    "\n",
    "# 如我們所見，分詞器添加了模型使用的特殊標記（[CLS] 在開頭和 [SEP] 在結尾），\n",
    "# 並且大多數單詞保持不變。然而，某些單詞 (word) 會被分為數個子詞 (subword)，如: Bar, ##eil 和 ##ly\n",
    "pprint(input_token_ids.tokens(), compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 原始資料\n",
    "print(f'length of input_words: {len(input_words)}')\n",
    "print(f'length of input_labels: {len(input_labels)}')\n",
    "# 這導致了我們的輸入和標籤之間的不匹配\n",
    "print(f'length of token id: {len(input_token_ids.tokens())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "經過 Tokenizer 後，這導致了我們的輸入和標籤之間的不匹配。\n",
    "\n",
    "感謝 fast tokenizer 我們可以輕鬆地將每個 Token 藉由 `word_ids()` 映射到其對應的單詞 Word IDs。\n",
    "\n",
    "舉例：\n",
    "\n",
    "| Word    | Token | Word ID |\n",
    "|---------|-------|---------|\n",
    "|         | [CLS] | None    |\n",
    "| My      | My    | 0       |\n",
    "| name    | name  | 1       |\n",
    "| is      | is    | 2       |\n",
    "| Ludmila | Lu    | 3       |\n",
    "|         | ##d   | 3       |\n",
    "|         | ##mi  | 3       |\n",
    "|         | ##la  | 3       |\n",
    "|         | [SEP] | None    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 資料對比\n",
    "print('=== Tokenizer 前 ===')\n",
    "show_nth_data(dataset[data_cat], data_nth, max_display)\n",
    "print('=== Tokenizer 後 ===')\n",
    "pprint(input_token_ids.tokens()[:max_display], compact=True)\n",
    "\n",
    "# `word_ids` return the list of tokens (sub-parts of the input strings after word/subword splitting and before conversion to integer indices)\n",
    "# at a given batch index (only works for the output of a fast tokenizer).\n",
    "print()\n",
    "print('=== 對應的 word_ids ===')\n",
    "pprint(input_token_ids.word_ids()[:max_display], compact=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "metadata": {}
   },
   "source": [
    "#### 重新校準 Tokenizer 與標籤\n",
    "\n",
    "我們可以擴展標籤以匹配單詞 Token。\n",
    "\n",
    "1. 首先，我們將應用的規則是特殊 Token 獲得 -100 標籤。這是因為默認情況下，-100 是我們在損失函數中被忽略的索引。\n",
    "2. 然後，每個 Token 獲得與其所在單詞相同的標籤，因為它們是同一實體的一部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(word_ids, labels):\n",
    "    new_labels = []\n",
    "    current_word_id = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None: # None 代表特殊 Token\n",
    "            label = -100\n",
    "        elif word_id != current_word_id: # Word ID 改變，代表新的 Token\n",
    "            current_word_id = word_id # 更新 Word ID\n",
    "            label = -100 if word_id is None else config.tag2id.get(labels[word_id]) # 取得對應的標籤\n",
    "        else: # 與前一個 Token 相同的字\n",
    "            label = config.tag2id.get(labels[word_id])\n",
    "        new_labels.append(label) # 附加新的標籤\n",
    "    # 返回新的標籤\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "labels = align_labels_with_tokens(input_token_ids.word_ids(), input_labels)\n",
    "\n",
    "# 顯示對齊後的標籤，兩者數量應該相同\n",
    "print(f'length of token id: {len(input_token_ids.tokens())}')\n",
    "print(f'length of labels: {len(labels)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "經過重新校準後，我們的標籤與 Token 一一對應。\n",
    "\n",
    "| Word    | Token | Word ID | Label          |\n",
    "|---------|-------|---------|----------------|\n",
    "|         | [CLS] | None    | -              |\n",
    "| My      | My    | 0       | O              |\n",
    "| name    | name  | 1       | O              |\n",
    "| is      | is    | 2       | O              |\n",
    "| Ludmila | Lu    | 3       | B-NAME_STUDENT |\n",
    "|         | ##d   | 3       | B-NAME_STUDENT |\n",
    "|         | ##mi  | 3       | B-NAME_STUDENT |\n",
    "|         | ##la  | 3       | B-NAME_STUDENT |\n",
    "|         | [SEP] | None    | -              |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print('=== 對應的標籤 ===')\n",
    "pprint(input_token_ids.tokens()[:max_display], compact=True)\n",
    "# 將整數標籤轉換為 BIO 標籤方便比較\n",
    "labels_id = [config.id2tag.get(label, '-') for label in labels]\n",
    "pprint(labels_id[:max_display], compact=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定義預處理函數\n",
    "\n",
    "要預處理整個數據集，我們需要對所有輸入進行分詞，並對所有標籤應用 `align_labels_with_tokens()`。為了利用快速分詞器的速度，最好一次分詞大量文本，因此我們將編寫一個批次處理函數。\n",
    "\n",
    "與之前的示例不同的是，當分詞器的輸入是文本列表時，`word_ids()` 函數需要獲取我們想要單詞 ID 的索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(dataset):\n",
    "    # 使用 tokenizer 對資料集進行分詞\n",
    "    tokenized_inputs = tokenizer(\n",
    "        dataset['words'], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = dataset['labels']\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i) # 取得對應的 word_ids\n",
    "        new_labels.append(align_labels_with_tokens(word_ids, labels)) # 對齊標籤\n",
    "    # 更新 labels 欄位，請留意這邊的 labels 欄位是整數標籤，而非 BIO 標籤\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenize_and_align_labels(dataset[data_cat][:1])\n",
    "input_ids = tokenized_dataset['input_ids'][0]\n",
    "labels = tokenized_dataset['labels'][0]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for input_id, label in zip(input_ids, labels): # 逐一取出 input_id 與 label\n",
    "    # 計算 input_id 與 label 的最大長度, 並將 input_id 與 label 用空白補齊至相同長度\n",
    "    str_input_id = str(input_id)\n",
    "    str_label = str(label)\n",
    "    max_length = max(len(str_input_id), len(str_label))\n",
    "    line1 += str_input_id + \" \" * (max_length - len(str_input_id) + 1)\n",
    "    line2 += str_label + \" \" * (max_length - len(str_label) + 1)\n",
    "pprint(line1, width=200)\n",
    "pprint(line2, width=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請留意經過批次轉換後，labels 欄位是整數標籤，而不是 BIO 字串標籤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 批次校準 Tokenizer 與標籤\n",
    "\n",
    "使用 `Dataset.map()` 方法，選項設置為 `batched=True`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_and_align_labels, # 對資料集進行分詞與標籤對齊\n",
    "    batched=True, # 是否以批次進行處理\n",
    "    remove_columns=dataset['train'].column_names, # 移除不必要的欄位\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 顯示前 first_n_data 筆資料\n",
    "pd.DataFrame(tokenized_dataset[data_cat].select(range(first_n_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料校對器 (Data Collator)\n",
    "\n",
    "我們不能僅使用 `DataCollatorWithPadding`，因為它只填充輸入（input IDs, attention mask）。在這裡，我們的標籤應該以與輸入完全相同的方式進行填充，以保持相同的大小，使用 -100 作為值，以便在損失計算中忽略相應的預測。\n",
    "\n",
    "這一切都由 `DataCollatorForTokenClassification` 完成。與 `DataCollatorWithPadding` 一樣，它需要使用預處理輸入的分詞器：\n",
    "\n",
    ">\n",
    "> * [DataCollatorWithPadding](https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorWithPadding) that will dynamically pad the inputs received.\n",
    "> * [DataCollatorForTokenClassification](https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorForTokenClassification) that will dynamically pad the inputs received, as well as the labels.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 展示 DataCollatorForTokenClassification 的輸出, 標籤以 -100 表示 padding\n",
    "batch = data_collator([tokenized_dataset[data_cat][i] for i in range(first_n_data)])\n",
    "pprint(batch[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型評估函數\n",
    "\n",
    "在訓練過程中包含度量標準通常有助於評估模型的性能。您可以使用 Evaluate 庫快速加載評估方法。對於這個任務，請加載 [seqeval](https://huggingface.co/docs/evaluate/a_quick_tour) 框架。Seqeval 實際上會生成多個分數：precision, recall, F1, 和 accuracy。\n",
    "\n",
    "* Precision: 精確率，是指所有被標記為正的樣本中實際為正的比例。\n",
    "\n",
    "$\\ Precision = \\frac{\\text{correctly classified actual positives}}{\\text{everything classified as positives}} = \\frac{TP}{TP + FP} $\n",
    "\n",
    "* Recall: 召回率，是指所有實際為正的樣本中被標記為正的比例。\n",
    "\n",
    "$\\ Recall = \\frac{\\text{correctly classified actual positives}}{\\text{all actual positives}} = \\frac{TP}{TP + FN} $\n",
    "\n",
    "* F1: F1 值是精確率和召回率的調和平均值，用於綜合考慮精確率和召回率。\n",
    "\n",
    "$\\ F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall} $\n",
    "\n",
    "* Accuracy: 準確率，是指所有被正確分類的樣本數量與總樣本數量之比。\n",
    "\n",
    "$\\ Accuracy = \\frac{\\text{correctly classifications}}{\\text{total classifications}} = \\frac{TP + TN}{TP + TN + FP + FN} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    # Unpack logits and labels from the input\n",
    "    logits, labels = eval_preds\n",
    "\n",
    "    # Convert logits to the index of the maximum logit value\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Map predictions and labels to their corresponding label names, ignoring padding (-100)\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    # Compute evaluation metrics using seqeval\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    # Return the computed metrics\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入預訓練模型\n",
    "\n",
    "您現在可以開始訓練您的模型了！使用 `AutoModelForTokenClassification` 加載預訓練的模型，並指定預期標籤的數量和標籤映射：\n",
    "\n",
    "需要留意，因為預訓練模型的分類數為 9，所以我們需要重新設定模型的分類數，且忽略預訓練模型的分類層。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "  config.model_name,\n",
    "  num_labels=config.num_tags, # 任務標籤數量\n",
    "  ignore_mismatched_sizes=True, # 忽略不匹配的大小，預訓練模型的標籤數量與我們的標籤數量不同\n",
    "  id2label=config.id2tag, # 整數標籤到 BIO 標籤的映射\n",
    "  label2id=config.tag2id, # BIO 標籤到整數標籤的映射\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 查看可訓練的參數量約 65M\n",
    "print('Parameters: {:,}, Trainable Parameters: {:,}'.format(\n",
    "  model.num_parameters(),\n",
    "  model.num_parameters(only_trainable=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定義訓練參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir='sample_data/train_output_pii_detection',\n",
    "  learning_rate=config.lr,\n",
    "  per_device_train_batch_size=config.train_batch_size,\n",
    "  per_device_eval_batch_size=config.eval_batch_size,\n",
    "  num_train_epochs=config.epochs,\n",
    "  weight_decay=config.weight_decay,\n",
    "  eval_strategy='epoch', # 每個 epoch 評估一次\n",
    "  save_strategy='epoch', # 每個 epoch 儲存一次\n",
    "  load_best_model_at_end=True,\n",
    "  report_to='none', # Disable wandb on colab\n",
    "  save_total_limit=5, # 最多儲存 5 個 checkpoints\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  tokenizer=tokenizer,\n",
    "  args=training_args,\n",
    "  train_dataset=tokenized_dataset['train'],\n",
    "  eval_dataset=tokenized_dataset['validation'],\n",
    "  data_collator=data_collator,\n",
    "  compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 開始訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 開始訓練，這可能需要一些時間\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練完成後，您可以通過運行 `Trainer.evaluate()` 方法在驗證集上評估模型的性能。它會計算模型的損失和其他評估指標，並返回這些結果。這對於了解模型在未見數據上的表現非常有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 儲存模型\n",
    "trainer.save_model(config.saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評估模型\n",
    "\n",
    "### 載入微調後 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  config.saved_model_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入微調後模型\n",
    "\n",
    "請留意分類數已經從預訓練模型的 9 類，變成 10 類。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "  config.saved_model_path,\n",
    ").to(device)\n",
    "pprint(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning 後的表現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 載入新模型\n",
    "classifier = pipeline(\n",
    "  task=\"token-classification\",\n",
    "  model=config.saved_model_path,\n",
    "  device=device,)\n",
    "\n",
    "# 顯示預訓練模型預測結果，僅顯示有被標注的部分\n",
    "for val in dataset['test']: # 逐一取出測試資料\n",
    "  print(f'輸入: {val[\"text\"]}')\n",
    "  show_prediction(val[\"text\"], classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
