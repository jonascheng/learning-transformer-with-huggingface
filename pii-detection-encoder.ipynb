{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# transformers not support NumPy 2.0 yet\n",
    "!pip install -q numpy~=1.26.4 transformers~=4.46.2\n",
    "!pip install -q datasets~=3.2.0 pydantic~=2.10.4\n",
    "!pip install -q seqeval~=1.2.2 evaluate~=0.4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練 PII 偵測模型\n",
    "\n",
    "在這個筆記本中，我們將展示如何使用 `transformers` 套件訓練 PII (個人識別資訊) 偵測模型。我們將使用 `transformers` 套件中的 [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer) 類別來微調一個 Encoder-Only 架構的 BERT 模型。我們將利用到標記分類 ([Token classification](https://huggingface.co/docs/transformers/tasks/token_classification)) 進行下游任務 (downstream task) 的訓練。\n",
    "\n",
    "標記分類為句子中的單詞分配標籤 (Label)。最常見的標記分類任務之一是命名實體識別 (NER)。NER 試圖為句子中的每個實體找到一個標籤，例如人名、地點或組織。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "from transformers import (\n",
    "  AutoTokenizer,\n",
    "  AutoModelForTokenClassification,\n",
    "  DataCollatorForTokenClassification,\n",
    "  pipeline,\n",
    "  TrainingArguments,\n",
    "  Trainer,\n",
    ")\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from pprint import pprint\n",
    "\n",
    "import ast\n",
    "import torch\n",
    "\n",
    "# 檢查是否有 GPU 可以使用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下載資料\n",
    "\n",
    "從 Kaggle 下載 PII External Dataset，並解壓縮到 `sample_data` 資料夾。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 從 Kaggle 下載 PII External Dataset\n",
    "!curl -L -o ./sample_data/pii-external-dataset.zip \\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/alejopaullier/pii-external-dataset\n",
    "\n",
    "# 解壓縮\n",
    "!unzip -o -q ./sample_data/pii-external-dataset.zip -d ./sample_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解壓縮後的資料為 CSV 檔案，`load_dataset()` 函數可以讀取 CSV 檔案。\n",
    "\n",
    "無論數據集存儲在哪裡，[Datasets](https://huggingface.co/docs/datasets/loading) 都可以幫助您加載它。數據可以存儲在 Hugging Face Hub、本地機器上、在 Github 中，以及在內存數據結構中，也可以在 Pandas DataFrame 之間轉換。在接續的實戰中，我們也都會用到 `load_dataset()` 函數加載數據集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# The full `train` split\n",
    "immutable_dataset = load_dataset('csv', data_files='sample_data/pii_dataset.csv', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料包含什麼？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顯示原始資料中包含的 features 以及筆數\n",
    "immutable_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢視資料集中的第一筆資料\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df = pd.DataFrame(immutable_dataset[:1])\n",
    "# 將 'text', 'tokens', labels' 三個欄位的順序移至前面\n",
    "df.insert(0, 'labels', df.pop('labels'))\n",
    "df.insert(0, 'tokens', df.pop('tokens'))\n",
    "df.insert(0, 'text', df.pop('text'))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這個表格結構，包含多項資訊，在這個筆記本中，我們將使用：\n",
    "\n",
    "* `text`: 一個完整字符串，包含 PII 資訊，將用作輸入。\n",
    "\n",
    "* `token`: 依據 `text` 切割後的字詞及字符，將用作訓練。\n",
    "\n",
    "* `label`: 對應每個 `token` 的標記，其中包含 PII 資訊的類別，將用作訓練。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料前處理\n",
    "\n",
    "保留必要 features: `text`, `tokens` 及 `labels`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 保留必要 features: 'text', 'tokens', 'labels'\n",
    "dataset = immutable_dataset.remove_columns([\n",
    "  'document', 'trailing_whitespace', 'prompt', 'prompt_id', 'name',\n",
    "  'email', 'phone', 'job', 'address', 'username', 'url', 'hobby', 'len'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "從 CSV 檔案讀取後的 `tokens` 與 `labels` 是字符串型別，我們需要將其轉換為 list 型別。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset[0]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset[0]['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下程式碼使用 `dataset.map` 方法來轉換數據集中的每個元素。具體來說，它將每個元素中的 `tokens` 和 `labels` 字段從字符串格式轉換為其對應的 Python 數據結構：\n",
    "\n",
    "* `dataset.map`: map 方法用於對數據集中的每個元素應用一個函數，並返回一個新的數據集。這個方法通常用於數據預處理和轉換。\n",
    "\n",
    "* `lambda x: { ... }`: 這是一個匿名函數（lambda 函數），它接受一個輸入 x，並返回一個字典。x 代表數據集中的一個元素。\n",
    "\n",
    "* `ast.literal_eval`: `ast.literal_eval` 是 Python 的 ast 模塊中的一個函數，它可以將字符串轉換為對應的 Python 數據結構，如列表、字典等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將 'tokens' 與 'labels' 從 string 轉換為 list (CSV 檔案讀取後會變成 string)\n",
    "dataset = dataset.map(\n",
    "  lambda x: {\n",
    "    'tokens': ast.literal_eval(x['tokens']),\n",
    "    'labels': ast.literal_eval(x['labels'])\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上程式碼的作用是對數據集中的每個元素進行轉換，將 `tokens` 和 `labels` 字段從字符串格式轉換為對應的 Python 數據結構。這樣做的目的是便於後續的數據處理和模型訓練。轉換後的數據集將包含已解析的 `tokens` 和 `labels`，而不再是原始的字符串格式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "透過過濾數據集中的元素，確保每個元素中的 `tokens` 和 `labels` 字段的長度相等，確保 `tokens` 與 `labels` 長度相等，避免有缺失的情況。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確認 tokens 長度與 labels 長度相等，避免有缺失的情況\n",
    "dataset = dataset.filter(lambda x: len(x['tokens']) == len(x['labels']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將 `tokens` 欄位重新命名為 `words` 避免與後面的 tokens 概念混淆。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將 tokens 欄位重新命名為 words 避免與後面的 tokens 概念混淆\n",
    "dataset = dataset.rename_column('tokens', 'words')\n",
    "\n",
    "# 顯示處理後的資料\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將數據集分割為訓練集、驗證集和測試集。\n",
    "\n",
    "* 保留 0.1% 的數據作為測試集，為了控制課程演示所以採用 `shuffle=False`，確保在不同運行中訓練集、驗證集和測試集保持一致。您可以試試看 `shuffle=True` 來打亂數據。\n",
    "\n",
    "* 將剩餘的數據集分割為 80% 的訓練集和 20% 的驗證集。\n",
    "\n",
    "* 創建包含訓練集、驗證集和測試集的數據集字典。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Reserve 0.1% of the training set for testing\n",
    "test_dataset = dataset.train_test_split(\n",
    "  test_size=0.001, # 0.1% of the data is used for testing\n",
    "  shuffle=False, # Ensure that train and validation sets are the same across runs\n",
    "  )\n",
    "# Split into 80% training and 20% validation sets\n",
    "train_dataset = test_dataset['train'].train_test_split(\n",
    "  test_size=0.2, # 20% of the data is used for validation\n",
    "  shuffle=False, # Ensure that train and test sets are the same across runs\n",
    "  )\n",
    "dataset = DatasetDict({\n",
    "  'train': train_dataset['train'],\n",
    "  'validation': train_dataset['test'],\n",
    "  'test': test_dataset['test'],\n",
    "  })\n",
    "# 顯示處理後的資料\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 顯示前 first_n_data 筆資料\n",
    "first_n_data = 2\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.DataFrame(dataset['train'].select(range(first_n_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "為了方便我們說明及檢視資料，所以我們期望將 `words` 與 `labels` 並列對齊顯示如下：\n",
    "\n",
    "| 1      | 2   | 3              | 4              |\n",
    "|--------|-----|----------------|----------------|\n",
    "| Hello, | I'm | Nicholas       | Moore,         |\n",
    "| O      | O   | B-NAME_STUDENT | I-NAME_STUDENT |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "於是我們定義一個函數，輸入為單一筆數據及最大列印長度，輸出為對齊後的數據。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_words_labels(x, max_display):\n",
    "    words = x['words'][:max_display]\n",
    "    labels = x['labels'][:max_display]\n",
    "    line1 = \"\"\n",
    "    line2 = \"\"\n",
    "    for word, label in zip(words, labels): # 逐一取出 word 與 label\n",
    "        # 計算 word 與 label 的最大長度, 並將 word 與 label 用空白補齊至相同長度\n",
    "        max_length = max(len(word), len(label))\n",
    "        line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "        line2 += label + \" \" * (max_length - len(label) + 1)\n",
    "    pprint(line1, width=200)\n",
    "    pprint(line2, width=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 並列顯示前 max_display 個 words 與 labels\n",
    "max_display = 50\n",
    "\n",
    "# 顯示前 first_n_data 筆資料中的前 max_display 個 words 與 labels\n",
    "for i in range(first_n_data):\n",
    "    display_words_labels(dataset['train'][i], max_display)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料中的 BIO 標注\n",
    "\n",
    "IOB 格式（Inside, Outside, Beginning 的縮寫），也常被稱為 BIO 格式，是計算語言學中用於標記任務（例如命名實體識別 NER，詞性標記 POS）的常見標記格式。\n",
    "\n",
    "* B - for the first token of a named entity\n",
    "\n",
    "* I - for tokens inside named entity's\n",
    "\n",
    "* O - for tokens outside any named entity\n",
    "\n",
    "我們需要先了解數據集中包含哪些 BIO 標注，所以我們先從訓練集中取出所有的標籤，並且去除重複值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 顯示 BIO 標注\n",
    "label_names = set()\n",
    "for data in dataset['train']:\n",
    "    # 將 labels 欄位轉換為 set 並更新 label_names\n",
    "    label_names.update(data['labels'])\n",
    "# convert set to list and sort label names\n",
    "label_names = list(label_names)\n",
    "# sort label names\n",
    "label_names.sort()\n",
    "print('=== 資料集中的標籤 ===')\n",
    "pprint(label_names, compact=True)\n",
    "print()\n",
    "print(f'標籤數量: {len(label_names)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上，我們可以獲得十個標籤，分別為：\n",
    "\n",
    "```json\n",
    "[\n",
    "  'B-EMAIL', 'B-NAME_STUDENT', 'B-PHONE_NUM', 'B-STREET_ADDRESS', 'B-URL_PERSONAL', \n",
    "  'B-USERNAME', 'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'O'\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方便後續訓練過程，我們將建立兩個對照表，一個是將 BIO 標籤映射到一個整數編碼，另一個是將整數編碼映射回 BIO 標籤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整數標籤到 BIO 標籤的映射\n",
    "id2tag = dict(enumerate(label_names))\n",
    "\n",
    "print('=== id2tag ===')\n",
    "pprint(sorted(id2tag.items(), key=lambda x: x[0]), compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一張對照表是 0 代表 `B-EMAIL`，1 代表 `B-NAME_STUDENT`，2 代表 `B-PHONE_NUM`，以此類推。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIO 標籤到整數標籤的映射\n",
    "tag2id = dict((v, k) for k, v in id2tag.items())\n",
    "\n",
    "print('=== tag2id ===')\n",
    "pprint(sorted(tag2id.items(), key=lambda x: x[1]), compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反之，建立另一個對照表，將 `B-EMAIL` 映射回 0，將 `B-NAME_STUDENT` 映射回 1，將 `B-PHONE_NUM` 映射回 2，以此類推。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練參數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "當我們在微調或訓練機器學習模型時，有幾個重要的參數需要設定。這些參數會影響模型的訓練效果和性能。以下是這些參數的簡單解釋：\n",
    "\n",
    "* 批次大小（Batch Size）：每次訓練迭代中使用的樣本數量。假設你有1000個數據點，批次大小為32，這意味著模型每次會使用32個數據點來更新權重。當所有數據點都被使用完一次後，這稱為一個 epoch。較大的批次大小可以加速訓練，但需要更多的內存；較小的批次大小則更穩定，但訓練速度較慢。\n",
    "* 訓練輪數（Epochs）：完整遍歷訓練數據集的次數。如果你設定 epochs 為10，這意味著模型會完整地看10次訓練數據集。更多的 epochs 可以讓模型學習得更充分，但過多的 epochs 可能會導致過擬合（Overfitting / 模型在訓練數據上表現很好，但在新數據上表現不好）。\n",
    "* 學習率（Learning Rate）：每次更新模型權重時的步伐大小。學習率決定了模型在每次更新時應該調整多少權重。較高的學習率會使模型快速學習，但可能會跳過最佳解；較低的學習率會使模型穩定學習，但訓練時間較長。學習率需要仔細調整，過高或過低都會影響模型的性能。\n",
    "* 隨機失活（Dropout）：在訓練過程中隨機忽略一些神經元的比例。Dropout 是一種正則化技術，用於防止過擬合。它通過在每次訓練步驟中隨機忽略一些神經元來強制模型學習更穩健的特徵。適當的 Dropout 可以提高模型的泛化能力，但過高的 Dropout 可能會導致模型欠擬合（Underfitting / 模型在訓練數據和新數據上都表現不好）。\n",
    "* 權重衰減（Weight Decay）：在每次更新權重時，對權重施加的正則化項。Weight Decay 是另一種正則化技術，用於防止過擬合。它通過在每次更新時對權重施加懲罰，使權重保持較小的值。適當的 Weight Decay 可以提高模型的泛化能力，但過高的 Weight Decay 可能會導致模型欠擬合。\n",
    "\n",
    "這些參數在模型訓練中扮演著重要角色，影響著模型的學習速度、穩定性和泛化能力。調整這些參數需要根據具體的數據集和任務進行實驗和調整，以找到最佳的組合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 訓練相關設定\n",
    "class Config(BaseModel):\n",
    "  model_name: str = 'dslim/distilbert-NER' # 使用蒸餾模型，降低參數量，加快訓練速度\n",
    "  saved_model_path: str = 'sample_data/saved_decoder_model' # path to save the trained model\n",
    "  train_batch_size: int = 4 # size of the input batch in training\n",
    "  eval_batch_size: int = 4 # size of the input batch in evaluation\n",
    "  epochs: int = 1 # number of times to iterate over the entire training dataset\n",
    "  lr: float = 2e-5 # learning rate, controls how fast or slow the model learns\n",
    "  weight_decay: float = 0.01 # weight decay, helps the model stay simple and avoid overfitting by penalizing large weights.\n",
    "  tags: list # BIO 標註的標籤列表\n",
    "  id2tag: dict # 整數標籤到 BIO 標籤的映射\n",
    "  tag2id: dict # BIO 標籤到整數標籤的映射\n",
    "  num_tags: int # 標籤數量\n",
    "\n",
    "config = Config(\n",
    "  tags=label_names,\n",
    "  id2tag=id2tag,\n",
    "  tag2id=tag2id,\n",
    "  num_tags=len(label_names)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning 前的表現"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入預訓練分詞器 (Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 透過預訓練模型取得 Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  config.model_name,\n",
    ")\n",
    "pprint(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確認為 fast tokenizer，方便後續做資料預處理。\n",
    "\n",
    "> [Fast tokenizers’ special powers](https://huggingface.co/learn/nlp-course/chapter6/3?fw=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確認 tokenizer 是否為 fast tokenizer\n",
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入預訓練模型\n",
    "\n",
    "透過 `AutoModelForTokenClassification` 載入預訓練模型，並確認模型的分類數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "  config.model_name,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這是一個典型的 Encoder-Only 架構，其中包含一個 BERT 模型和一個線性分類器。BERT 模型用於提取特徵，線性分類器用於將特徵映射到分類標籤。\n",
    "\n",
    "觀察預訓練模型的分類數為 9。\n",
    "\n",
    "```json\n",
    "(classifier): Linear(in_features=768, out_features=9, bias=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning 前的表現"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 單筆演示分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 載入預訓練模型，宣告分類器\n",
    "classifier = pipeline(\n",
    "  task=\"token-classification\",\n",
    "  model=model,\n",
    "  tokenizer=tokenizer,\n",
    "  device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 透過預訓練模型預測分類\n",
    "predict = classifier(dataset['test'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們將獲得一個句子中被標記的字詞，其中 `word` 為字詞，`entity` 為標籤分類。\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    'entity': 'B-PER', \n",
    "    'word': 'Nicholas'\n",
    "  }, {\n",
    "    'entity': 'I-PER',\n",
    "    'word': 'Moore'\n",
    "  },\n",
    "  ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 批次演示分類\n",
    "\n",
    "初步了解如何利用 `pipeline` 進行分類，我們將定義一個 `display_words_entity` 函數，輸入為一個句子，輸出為對齊後的字詞及標籤。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_words_entity(text, classifier):\n",
    "    result = classifier(text) # 預測結果\n",
    "    line1 = \"\"\n",
    "    line2 = \"\"\n",
    "    for r in result:\n",
    "        # 取出預測結果中的 word 與 entity\n",
    "        word = r['word']\n",
    "        label = r['entity']\n",
    "        # 計算 word 與 entity 的最大長度, 並將 word 與 entity 用空白補\n",
    "        max_length = max(len(word), len(label))\n",
    "        line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "        line2 += label + \" \" * (max_length - len(label) + 1)\n",
    "    pprint(line1)\n",
    "    pprint(line2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 顯示預訓練模型預測結果，僅顯示有被標注的部分\n",
    "for val in dataset['test']: # 逐一取出測試資料\n",
    "  print(f'輸入: {val[\"text\"]}')\n",
    "  display_words_entity(val[\"text\"], classifier)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以留意到，預訓練模型已經具備一定的分類能力，但是由於我們的任務是 PII 偵測，所以我們需要進行 Fine-tuning 來提高模型的性能。同時，我們希望調整分類數，以符合我們的任務需求。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 了解分詞器 (Tokenizer) 行為\n",
    "\n",
    "首先，我們先試試看透過分詞器將一個句子轉換為 tokens。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_labels = dataset['train'][0][\"labels\"]\n",
    "input_words = dataset['train'][0][\"words\"]\n",
    "# 輸入為將 'text' 切割後的字詞及字符\n",
    "pprint(input_words, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由於輸入已經預先分詞 (e.g., split into words)，所以我們將 `is_split_into_words` 設為 `True`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 對 input_words 進行分詞\n",
    "input_tokenized_ids = tokenizer(\n",
    "  input_words,\n",
    "  truncation=True, # 是否截斷序列\n",
    "  # is_split_into_words: Whether or not the input is already pre-tokenized (e.g., split into words).\n",
    "  # If set to True, the tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace) which it will tokenize.\n",
    "  # This is useful for NER or token classification.\n",
    "  is_split_into_words=True)\n",
    "pprint(input_tokenized_ids, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "經過分詞器後將得到一個包含 `attention_mask` 及 `input_ids` 的字典，其中 `attention_mask` 用於指示哪些 tokens 是模型需要關注，哪些是 padding tokens 可以忽略，`input_ids` 則是 tokens 的索引或對應到辭典 (vocab) 的 ID。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 還原編碼後的文字\n",
    "\n",
    "我們亦可以透過 `tokens()` 方法將 `input_ids` 轉換回 tokens。\n",
    "\n",
    "分詞器添加了模型使用的特殊標記（[CLS] 在開頭和 [SEP] 在結尾），並且大多數單詞保持不變。然而，某些單詞 (word) 會被分為數個子詞 (subword)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如我們所見，分詞器添加了模型使用的特殊標記（[CLS] 在開頭和 [SEP] 在結尾），\n",
    "# 並且大多數單詞保持不變。然而，某些單詞 (word) 會被分為數個子詞 (subword)，如: Bar, ##eil 和 ##ly\n",
    "pprint(input_tokenized_ids.tokens(), compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "於是，經過分詞器後，這導致了我們的輸入和標籤之間的不匹配，從兩者長度便可窺知一二。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 原始資料\n",
    "print(f'length of input_words: {len(input_words)}')\n",
    "print(f'length of input_labels: {len(input_labels)}')\n",
    "# 這導致了我們的輸入和標籤之間的不匹配\n",
    "print(f'length of token id: {len(input_tokenized_ids.tokens())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 還原編碼後的位置\n",
    "\n",
    "感謝 fast tokenizer 我們可以輕鬆地將每個 Token 藉由 `word_ids()` 映射到其對應的單詞位置或 Word IDs。\n",
    "\n",
    "舉例：\n",
    "\n",
    "| Word    | Token | Word ID |\n",
    "|---------|-------|---------|\n",
    "|         | [CLS] | None    |\n",
    "| My      | My    | 0       |\n",
    "| name    | name  | 1       |\n",
    "| is      | is    | 2       |\n",
    "| Ludmila | Lu    | 3       |\n",
    "|         | ##d   | 3       |\n",
    "|         | ##mi  | 3       |\n",
    "|         | ##la  | 3       |\n",
    "|         | [SEP] | None    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `word_ids` return the list of tokens (sub-parts of the input strings after word/subword splitting and before conversion to integer indices)\n",
    "# at a given batch index (only works for the output of a fast tokenizer).\n",
    "pprint(input_tokenized_ids.word_ids(), compact=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CLS] 和 [SEP] 並不對應到任何單詞，所以它們的 Word ID 為 None。其他則是以 0 開始的 Word ID，如遇到子詞 (subword) 則會共用相同的 Word ID。\n",
    "\n",
    "根據上述的了解，我們依序將輸入，編碼後的文字，以及編碼後的位置對比，幫助我們理解分詞器的行為。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 資料對比\n",
    "print('=== Tokenizer 前 (Words 及 Labels) ===')\n",
    "display_words_labels(dataset['train'][0], max_display)\n",
    "\n",
    "print()\n",
    "print('=== Tokenizer 後 (還原編碼後的文字) ===')\n",
    "pprint(input_tokenized_ids.tokens()[:max_display], compact=True)\n",
    "\n",
    "print()\n",
    "print('=== Tokenizer 後 (還原編碼後的位置) ===')\n",
    "pprint(input_tokenized_ids.word_ids()[:max_display], compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料預處理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "metadata": {}
   },
   "source": [
    "#### 重新校準標籤\n",
    "\n",
    "我們可以搭配 `word_id` 擴展標籤 (Label) 以匹配單詞 Token ID，將標籤與分詞後的標記對齊。\n",
    "\n",
    "1. 首先，我們將應用的規則是特殊 Token 獲得 -100 標籤。這是因為默認情況下，-100 是我們在損失函數中被忽略的索引。\n",
    "2. 然後，每個相同 Word ID 的 Token 獲得與其所在單詞相同的標籤，因為它們是同一實體的一部分。\n",
    "\n",
    "定義一個函數接受兩個參數：`word_ids` 和 `labels`，並返回重新校準後的標籤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(\n",
    "        word_ids: list,\n",
    "        labels: list,) -> list:\n",
    "    new_labels = [] # 用於存儲對齊後的新標籤\n",
    "    current_word_id = None # 用於追踪當前的單詞 ID\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None: # None 代表特殊 Token\n",
    "            label = -100 # 特殊 Token 獲得 -100 的標籤\n",
    "        elif word_id != current_word_id: # Word ID 改變，代表新的 Token\n",
    "            current_word_id = word_id # 更新 Word ID\n",
    "            label = -100 if word_id is None else config.tag2id.get(labels[word_id]) # 取得對應的標籤\n",
    "        else: # 與前一個 Token 相同的字\n",
    "            label = config.tag2id.get(labels[word_id])\n",
    "        new_labels.append(label) # 附加新的標籤\n",
    "    # 返回新的標籤\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再來檢視一次原先 `input_labels` 中的標籤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先將其轉換為標籤編碼\n",
    "input_label_ids = [config.tag2id.get(label) for label in input_labels]\n",
    "pprint(input_label_ids, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "經過重新校準後，我們可以看到標籤已經與分詞後的標記對齊，且長度相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_label_ids_aligned = align_labels_with_tokens(\n",
    "  input_tokenized_ids.word_ids(), # 透過 word_ids 取得對應的 word id\n",
    "  input_labels, # 原始標籤\n",
    ")\n",
    "pprint(input_label_ids_aligned, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 顯示對齊後的標籤，兩者數量應該相同\n",
    "print(f'length of token id: {len(input_tokenized_ids.tokens())}')\n",
    "print(f'length of labels: {len(input_label_ids_aligned)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "經過重新校準後，我們的標籤與 Token 一一對應。\n",
    "\n",
    "| Word    | Token | Word ID | Label          |\n",
    "|---------|-------|---------|----------------|\n",
    "|         | [CLS] | None    | -              |\n",
    "| My      | My    | 0       | O              |\n",
    "| name    | name  | 1       | O              |\n",
    "| is      | is    | 2       | O              |\n",
    "| Ludmila | Lu    | 3       | B-NAME_STUDENT |\n",
    "|         | ##d   | 3       | B-NAME_STUDENT |\n",
    "|         | ##mi  | 3       | B-NAME_STUDENT |\n",
    "|         | ##la  | 3       | B-NAME_STUDENT |\n",
    "|         | [SEP] | None    | -              |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def display_tokens_labels(tokens, label_ids):\n",
    "    line1 = \"\"\n",
    "    line2 = \"\"\n",
    "    for token, label_id in zip(tokens, label_ids): # 逐一取出 token 與 label_id\n",
    "        # 將整數標籤轉換為 BIO 標籤方便比較\n",
    "        label = config.id2tag.get(label_id, '-')\n",
    "        # 計算 token 與 label 的最大長度, 並將 token 與 label 用空白補齊至相同長度\n",
    "        max_length = max(len(token), len(label))\n",
    "        line1 += token + \" \" * (max_length - len(token) + 1)\n",
    "        line2 += label + \" \" * (max_length - len(label) + 1)\n",
    "    pprint(line1, width=200)\n",
    "    pprint(line2, width=200)\n",
    "\n",
    "# 顯示對齊後的 tokens 與 labels\n",
    "display_tokens_labels(\n",
    "    input_tokenized_ids.tokens()[:max_display],\n",
    "    input_label_ids_aligned[:max_display])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定義預處理函數\n",
    "\n",
    "要預處理整個數據集，我們需要對所有輸入進行分詞，並對所有標籤應用 `align_labels_with_tokens()`。為了利用快速分詞器的速度，最好一次分詞大量文本，因此我們將編寫一個批次處理函數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def preprocess_function(dataset):\n",
    "    # 使用分詞器對資料集進行分詞\n",
    "    tokenized_inputs = tokenizer(\n",
    "        dataset['words'],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "    labels = dataset['labels']\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(labels):\n",
    "        # 取得對應的 word_ids\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        # 對齊標籤\n",
    "        new_labels.append(\n",
    "            align_labels_with_tokens(word_ids, labels)\n",
    "        )\n",
    "    # 更新 labels 欄位，請留意這邊的 labels 欄位是整數標籤，而非 BIO 標籤\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokenized_ids = preprocess_function(dataset['train'][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(input_tokenized_ids, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "經過 `preprocess_function` 後將得到一組先前我們使用分詞器的 `attention_mask` 及 `input_ids`，也會增加一個對齊後的 `labels`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顯示對齊後的 tokens 與 labels\n",
    "display_tokens_labels(\n",
    "    input_tokenized_ids.tokens()[:max_display],\n",
    "    input_tokenized_ids['labels'][0][:max_display])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請留意經過資料預處理後，`labels` 欄位是整數標籤，而不是 BIO 字串標籤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 批次處理資料\n",
    "\n",
    "使用 `Dataset.map()` 方法，選項設置為 `batched=True`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function, # 對資料集進行分詞與標籤對齊\n",
    "    batched=True, # 是否以批次進行處理\n",
    "    remove_columns=dataset['train'].column_names, # 移除不必要的欄位\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 顯示前 first_n_data 筆資料\n",
    "pd.DataFrame(tokenized_dataset['train'].select(range(first_n_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料校對器 (Data Collator)\n",
    "\n",
    "在微調語言模型時，使用 data collator 是為了有效地準備和處理批次數據。以下是使用 data collator 的幾個主要原因：\n",
    "\n",
    "* 動態填充 (Dynamic Padding): 不同長度的序列需要填充到相同的長度，以便能夠在同一批次中進行處理。Data collator 可以自動計算每個批次的最大長度，並對序列進行適當的填充。\n",
    "\n",
    "* 批次處理 (Batch Processing): Data collator 可以將多個樣本組合成一個批次，這樣可以更高效地利用計算資源，特別是在使用 GPU 或 TPU 時。\n",
    "\n",
    "* 生成注意力掩碼 (Attention Masks): 在填充序列時，data collator 會生成相應的注意力掩碼 (attention masks)，以確保模型只關注實際的數據部分，而忽略填充部分。\n",
    "\n",
    "* 簡化代碼 (Code Simplification): 使用 data collator 可以簡化數據處理的代碼，減少手動處理數據的繁瑣步驟，讓開發者專注於模型設計和訓練。\n",
    "\n",
    "總之，data collator 在微調語言模型時提供了便利和效率，確保數據能夠以一致且高效的方式進行處理。\n",
    "\n",
    "這個課程中，我們不能僅使用 `DataCollatorWithPadding`，因為它只填充輸入（Input IDs, Attention Mask）。在這裡，我們的標籤 (Label) 應該以與輸入完全相同的方式進行填充，以保持相同的大小，使用 -100 作為填充值，以便在損失計算中忽略相應的預測。\n",
    "\n",
    "於是我們可以利用 `DataCollatorForTokenClassification` 完成。與 `DataCollatorWithPadding` 一樣，它需要使用預處理輸入的分詞器：\n",
    "\n",
    ">\n",
    "> * [DataCollatorWithPadding](https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorWithPadding) that will dynamically pad the inputs received.\n",
    "> * [DataCollatorForTokenClassification](https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorForTokenClassification) that will dynamically pad the inputs received, as well as the `labels`.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(\n",
    "  tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 展示 DataCollatorForTokenClassification 的輸出, 標籤以 -100 表示 padding\n",
    "features = [tokenized_dataset['train'][i] for i in range(first_n_data)]\n",
    "batch = data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這裡要注意的主要是第一個例子與第二個例子長度不一，所以長度不足的例子的 `input_ids` 和 `attention_mask` 已經在右側填充了一個 [PAD] 標記（其 ID 是 0）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(batch[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "類似地，我們可以看到 `labels` 已用 -100 填充，以確保填充標記被損失函數忽略。\n",
    "\n",
    "我們終於擁有了訓練所需的所有的前期準備！我們現在只需要使用標準參數實例化訓練器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型評估函數\n",
    "\n",
    "在訓練過程中包含度量標準通常有助於評估模型的性能。您可以使用 Evaluate 庫快速加載評估方法。對於這個任務，請加載 [seqeval](https://huggingface.co/docs/evaluate/a_quick_tour) 框架。Seqeval 實際上會生成多個分數：precision, recall, F1, 和 accuracy。\n",
    "\n",
    "* Precision: 精確率，是指所有被標記為正的樣本中實際為正的比例。\n",
    "\n",
    "$\\ Precision = \\frac{\\text{correctly classified actual positives}}{\\text{everything classified as positives}} = \\frac{TP}{TP + FP} $\n",
    "\n",
    "* Recall: 召回率，是指所有實際為正的樣本中被標記為正的比例。\n",
    "\n",
    "$\\ Recall = \\frac{\\text{correctly classified actual positives}}{\\text{all actual positives}} = \\frac{TP}{TP + FN} $\n",
    "\n",
    "* F1: F1 值是精確率和召回率的調和平均值，用於綜合考慮精確率和召回率。\n",
    "\n",
    "$\\ F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall} $\n",
    "\n",
    "* Accuracy: 準確率，是指所有被正確分類的樣本數量與總樣本數量之比。\n",
    "\n",
    "$\\ Accuracy = \\frac{\\text{correctly classifications}}{\\text{total classifications}} = \\frac{TP + TN}{TP + TN + FP + FN} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    # Unpack logits and labels from the input\n",
    "    logits, labels = eval_preds\n",
    "\n",
    "    # Convert logits to the index of the maximum logit value\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Map predictions and labels to their corresponding label names, ignoring padding (-100)\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    # Compute evaluation metrics using seqeval\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    # Return the computed metrics\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入預訓練模型\n",
    "\n",
    "您現在可以開始訓練您的模型了！使用 `AutoModelForTokenClassification` 加載預訓練的模型，並指定預期標籤的數量和標籤映射：\n",
    "\n",
    "需要留意，因為預訓練模型的分類數為 9，所以我們需要重新設定模型的分類數，且忽略預訓練模型的分類層。\n",
    "\n",
    "* `num_labels`: 指定了任務中標籤的數量。這告訴模型有多少個不同的標籤需要進行分類。\n",
    "\n",
    "* `ignore_mismatched_sizes`: 表示在加載模型時忽略預訓練模型的標籤數量與我們的標籤數量不匹配的情況。這在我們的任務標籤數量與預訓練模型的標籤數量不同時非常有用。\n",
    "\n",
    "* `id2label`: 是一個字典，用於將整數標籤映射到 BIO 標籤。這有助於在模型輸出時將整數標籤轉換為可讀的 BIO 標籤。\n",
    "\n",
    "* `label2id`: 是 `id2label` 的反向映射，用於將 BIO 標籤映射回整數標籤。\n",
    "\n",
    "以下程式碼從預訓練模型中加載一個標記分類模型，並根據特定任務的需求進行配置。它指定了模型名稱、任務標籤數量，並設置了忽略不匹配大小的選項。此外，它還提供了整數標籤與 BIO 標籤之間的映射，便於模型在輸入和輸出時進行轉換。這樣可以確保模型能夠正確處理特定任務中的標籤。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "  config.model_name,\n",
    "  num_labels=config.num_tags, # 任務標籤數量\n",
    "  ignore_mismatched_sizes=True, # 忽略不匹配的大小，預訓練模型的標籤數量與我們的標籤數量不同\n",
    "  id2label=config.id2tag, # 整數標籤到 BIO 標籤的映射\n",
    "  label2id=config.tag2id, # BIO 標籤到整數標籤的映射\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 查看可訓練的參數量約 65M\n",
    "print('Parameters: {:,}, Trainable Parameters: {:,}'.format(\n",
    "  model.num_parameters(),\n",
    "  model.num_parameters(only_trainable=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練參數設定\n",
    "\n",
    "用於設定訓練過程中的各種參數，如學習率、批次大小、梯度累積步數、訓練 epoch 數、權重衰減等。\n",
    "\n",
    "* `output_dir` 指定了訓練輸出的目錄。\n",
    "* `eval_strategy` 和 `save_strategy` 設定為 'epoch'，表示每個 epoch 都會進行評估和儲存。\n",
    "* `load_best_model_at_end` 設定為 `True`，表示訓練結束後會載入最佳模型。\n",
    "* `report_to` 設定為 'none'，禁用了 wandb 報告。\n",
    "* `save_total_limit` 設定了最多儲存 5 個 checkpoints。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir='sample_data/train_output_pii_detection',\n",
    "  learning_rate=config.lr,\n",
    "  per_device_train_batch_size=config.train_batch_size,\n",
    "  per_device_eval_batch_size=config.eval_batch_size,\n",
    "  num_train_epochs=config.epochs,\n",
    "  weight_decay=config.weight_decay,\n",
    "  eval_strategy='epoch', # 每個 epoch 評估一次\n",
    "  save_strategy='epoch', # 每個 epoch 儲存一次\n",
    "  load_best_model_at_end=True,\n",
    "  report_to='none', # Disable wandb on colab\n",
    "  save_total_limit=5, # 最多儲存 5 個 checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練器初始化\n",
    "\n",
    "用於初始化訓練器，並開始訓練模型。\n",
    "\n",
    "* `model` 是要訓練的模型。\n",
    "* `tokenizer` 是用於處理文本的分詞器。\n",
    "* `train_dataset` 和 `eval_dataset` 是訓練和評估數據集。\n",
    "* `data_collator` 是用於整理數據的數據整理器。\n",
    "* `compute_metrics` 是用於計算度量標準的函數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  tokenizer=tokenizer,\n",
    "  args=training_args,\n",
    "  train_dataset=tokenized_dataset['train'],\n",
    "  eval_dataset=tokenized_dataset['validation'],\n",
    "  data_collator=data_collator,\n",
    "  compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 開始訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 開始訓練，這可能需要一些時間\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練完成後，您可以通過運行 `Trainer.evaluate()` 方法在驗證集上評估模型的性能。它會計算模型的損失和其他評估指標，並返回這些結果。這對於了解模型在未見數據上的表現非常有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存微調模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 儲存模型\n",
    "trainer.save_model(config.saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 釋放資源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import garbage collector\n",
    "import gc\n",
    "\n",
    "# 釋放 GPU 記憶體\n",
    "del trainer\n",
    "del tokenizer\n",
    "\n",
    "model.to('cpu')\n",
    "del model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評估微調模型\n",
    "\n",
    "### 載入微調分詞器 (Tokenizer)\n",
    "\n",
    "從已經完成訓練的模型取得 Tokenizer。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  config.saved_model_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入微調後模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "ft_model = AutoModelForTokenClassification.from_pretrained(\n",
    "  config.saved_model_path,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請留意分類數已經從預訓練模型的 9 類，變成 10 類。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(ft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning 後的表現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入微調模型，宣告分類器\n",
    "classifier = pipeline(\n",
    "  task=\"token-classification\",\n",
    "  model=ft_model,\n",
    "  tokenizer=tokenizer,\n",
    "  device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 顯示微調模型預測結果，僅顯示有被標注的部分\n",
    "for val in dataset['test']: # 逐一取出測試資料\n",
    "  print(f'輸入: {val[\"text\"]}')\n",
    "  display_words_entity(val[\"text\"], classifier)\n",
    "  print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
