{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# transformers not support NumPy 2.0 yet\n",
    "!pip install -q numpy~=1.26.4 transformers~=4.46.2\n",
    "!pip install -q datasets~=3.2.0 pydantic~=2.10.4\n",
    "!pip install -q seqeval~=1.2.2 evaluate~=0.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "  # Attempt to get the notebook filename from the IPython environment\n",
    "  __ipynb_file__ = os.path.splitext(os.path.basename(os.environ['JPY_SESSION_NAME']))[0]\n",
    "except (NameError, KeyError):\n",
    "  # Fallback to a default value if the variable is not found\n",
    "  __ipynb_file__ = 'default_notebook_name'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練 PII 偵測模型\n",
    "\n",
    "在這個筆記本中，我們將展示如何使用 `transformers` 套件訓練 PII (個人識別資訊) 偵測模型。我們將使用 `transformers` 套件中的 [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer) 類別來微調一個 Encoder-Only 架構的 BERT 模型。我們將利用到標記分類 ([Token classification](https://huggingface.co/docs/transformers/tasks/token_classification)) 進行下游任務 (downstream task) 的訓練。\n",
    "\n",
    "標記分類為句子中的單詞分配標籤 (Label)。最常見的標記分類任務之一是命名實體識別 (NER)。NER 試圖為句子中的每個實體找到一個標籤，例如人名、地點或組織。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "from transformers import (\n",
    "  AutoTokenizer,\n",
    "  AutoModelForTokenClassification,\n",
    "  DataCollatorForTokenClassification,\n",
    "  pipeline,\n",
    "  TrainingArguments,\n",
    "  Trainer,\n",
    ")\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from pprint import pprint\n",
    "\n",
    "import ast\n",
    "import torch\n",
    "\n",
    "# 檢查是否有 GPU 可以使用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下載資料\n",
    "\n",
    "從 Kaggle 下載 PII External Dataset，並解壓縮到 `sample_data` 資料夾。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 7737k  100 7737k    0     0  3551k      0  0:00:02  0:00:02 --:--:-- 8200k\n"
     ]
    }
   ],
   "source": [
    "# 從 Kaggle 下載 PII External Dataset\n",
    "!curl -L -o ./sample_data/pii-external-dataset.zip \\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/alejopaullier/pii-external-dataset\n",
    "\n",
    "# 解壓縮\n",
    "!unzip -o -q ./sample_data/pii-external-dataset.zip -d ./sample_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解壓縮後的資料為 CSV 檔案，`load_dataset()` 函數可以讀取 CSV 檔案。\n",
    "\n",
    "無論數據集存儲在哪裡，[Datasets](https://huggingface.co/docs/datasets/loading) 都可以幫助您加載它。數據可以存儲在 Hugging Face Hub、本地機器上、在 Github 中，以及在內存數據結構中，也可以在 Pandas DataFrame 之間轉換。在接續的實戰中，我們也都會用到 `load_dataset()` 函數加載數據集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# The full `train` split\n",
    "immutable_dataset = load_dataset('csv', data_files='sample_data/pii_dataset.csv', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料包含什麼？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'text', 'tokens', 'trailing_whitespace', 'labels', 'prompt', 'prompt_id', 'name', 'email', 'phone', 'job', 'address', 'username', 'url', 'hobby', 'len'],\n",
       "    num_rows: 4434\n",
       "})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 顯示原始資料中包含的 features 以及筆數\n",
    "immutable_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tokens",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "labels",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "document",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "trailing_whitespace",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "prompt",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "prompt_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "email",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "phone",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "job",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "address",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "username",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "url",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "hobby",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "len",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "95a29af4-45de-4bb0-86d7-49c5be2c29f8",
       "rows": [
        [
         "0",
         "My name is Aaliyah Popova, and I am a jeweler with 13 years of experience. I remember a very unique and challenging project I had to work on last year. A customer approached me with a precious family heirloom - a diamond necklace that had been passed down through generations. Unfortunately, the necklace was in poor condition, with several loose diamonds and a broken clasp. The customer wanted me to restore it to its former glory, but it was clear that this would be no ordinary repair. Using my specialized tools and techniques, I began the delicate task of dismantling the necklace. Each diamond was carefully removed from its setting, and the damaged clasp was removed. Once the necklace was completely disassembled, I meticulously cleaned each diamond and inspected it for any damage. Fortunately, the diamonds were all in good condition, with no cracks or chips. The next step was to repair the broken clasp. I carefully soldered the broken pieces back together, ensuring that the clasp was sturdy and secure. Once the clasp was repaired, I began the process of reassembling the necklace. Each diamond was carefully placed back into its setting, and the necklace was polished until it sparkled like new. When I presented the restored necklace to the customer, they were overjoyed. They couldn't believe that I had been able to bring their family heirloom back to life. The necklace looked as beautiful as it had when it was first created, and the customer was thrilled to have it back in their possession. If you have a project that you would like to discuss, please feel free to contact me by phone at (95) 94215-7906 or by email at aaliyah.popova4783@aol.edu. I look forward to hearing from you! P.S.: When I'm not creating beautiful jewelry, I enjoy spending time podcasting. I love sharing my knowledge about jewelry and connecting with other people who are passionate about this art form. I also enjoy spending time with my family and exploring new places. If you would like to learn more about me, please feel free to visit my website at [website address] or visit me at my studio located at 97 Lincoln Street.",
         "['My', 'name', 'is', 'Aaliyah', 'Popova,', 'and', 'I', 'am', 'a', 'jeweler', 'with', '13', 'years', 'of', 'experience.', 'I', 'remember', 'a', 'very', 'unique', 'and', 'challenging', 'project', 'I', 'had', 'to', 'work', 'on', 'last', 'year.', 'A', 'customer', 'approached', 'me', 'with', 'a', 'precious', 'family', 'heirloom', '-', 'a', 'diamond', 'necklace', 'that', 'had', 'been', 'passed', 'down', 'through', 'generations.', 'Unfortunately,', 'the', 'necklace', 'was', 'in', 'poor', 'condition,', 'with', 'several', 'loose', 'diamonds', 'and', 'a', 'broken', 'clasp.', 'The', 'customer', 'wanted', 'me', 'to', 'restore', 'it', 'to', 'its', 'former', 'glory,', 'but', 'it', 'was', 'clear', 'that', 'this', 'would', 'be', 'no', 'ordinary', 'repair.', 'Using', 'my', 'specialized', 'tools', 'and', 'techniques,', 'I', 'began', 'the', 'delicate', 'task', 'of', 'dismantling', 'the', 'necklace.', 'Each', 'diamond', 'was', 'carefully', 'removed', 'from', 'its', 'setting,', 'and', 'the', 'damaged', 'clasp', 'was', 'removed.', 'Once', 'the', 'necklace', 'was', 'completely', 'disassembled,', 'I', 'meticulously', 'cleaned', 'each', 'diamond', 'and', 'inspected', 'it', 'for', 'any', 'damage.', 'Fortunately,', 'the', 'diamonds', 'were', 'all', 'in', 'good', 'condition,', 'with', 'no', 'cracks', 'or', 'chips.', 'The', 'next', 'step', 'was', 'to', 'repair', 'the', 'broken', 'clasp.', 'I', 'carefully', 'soldered', 'the', 'broken', 'pieces', 'back', 'together,', 'ensuring', 'that', 'the', 'clasp', 'was', 'sturdy', 'and', 'secure.', 'Once', 'the', 'clasp', 'was', 'repaired,', 'I', 'began', 'the', 'process', 'of', 'reassembling', 'the', 'necklace.', 'Each', 'diamond', 'was', 'carefully', 'placed', 'back', 'into', 'its', 'setting,', 'and', 'the', 'necklace', 'was', 'polished', 'until', 'it', 'sparkled', 'like', 'new.', 'When', 'I', 'presented', 'the', 'restored', 'necklace', 'to', 'the', 'customer,', 'they', 'were', 'overjoyed.', 'They', \"couldn't\", 'believe', 'that', 'I', 'had', 'been', 'able', 'to', 'bring', 'their', 'family', 'heirloom', 'back', 'to', 'life.', 'The', 'necklace', 'looked', 'as', 'beautiful', 'as', 'it', 'had', 'when', 'it', 'was', 'first', 'created,', 'and', 'the', 'customer', 'was', 'thrilled', 'to', 'have', 'it', 'back', 'in', 'their', 'possession.', 'If', 'you', 'have', 'a', 'project', 'that', 'you', 'would', 'like', 'to', 'discuss,', 'please', 'feel', 'free', 'to', 'contact', 'me', 'by', 'phone', 'at', '(95)', '94215-7906', 'or', 'by', 'email', 'at', 'aaliyah.popova4783@aol.edu.', 'I', 'look', 'forward', 'to', 'hearing', 'from', 'you!', 'P.S.:', 'When', \"I'm\", 'not', 'creating', 'beautiful', 'jewelry,', 'I', 'enjoy', 'spending', 'time', 'podcasting.', 'I', 'love', 'sharing', 'my', 'knowledge', 'about', 'jewelry', 'and', 'connecting', 'with', 'other', 'people', 'who', 'are', 'passionate', 'about', 'this', 'art', 'form.', 'I', 'also', 'enjoy', 'spending', 'time', 'with', 'my', 'family', 'and', 'exploring', 'new', 'places.', 'If', 'you', 'would', 'like', 'to', 'learn', 'more', 'about', 'me,', 'please', 'feel', 'free', 'to', 'visit', 'my', 'website', 'at', '[website', 'address]', 'or', 'visit', 'me', 'at', 'my', 'studio', 'located', 'at', '97', 'Lincoln', 'Street.']",
         "['O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUDENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PHONE_NUM', 'I-PHONE_NUM', 'O', 'O', 'O', 'O', 'B-EMAIL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STREET_ADDRESS', 'I-STREET_ADDRESS', 'I-STREET_ADDRESS']",
         "1073d46f-2241-459b-ab01-851be8d26436",
         "[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False]",
         "\n    Aaliyah Popova is a jeweler with 13 years of experience. Write a detailed example in first person of a job-related project he/her did in the past. Add the following information about him/her randomly inside the text: name is Aaliyah Popova, phone number is (95) 94215-7906, email is aaliyah.popova4783@aol.edu, hobby is Podcasting, address is 97 Lincoln Street.\n    ",
         "1",
         "Aaliyah Popova",
         "aaliyah.popova4783@aol.edu",
         "(95) 94215-7906",
         "jeweler",
         "97 Lincoln Street",
         null,
         null,
         "Podcasting",
         "363"
        ]
       ],
       "shape": {
        "columns": 16,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "      <th>document</th>\n",
       "      <th>trailing_whitespace</th>\n",
       "      <th>prompt</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>name</th>\n",
       "      <th>email</th>\n",
       "      <th>phone</th>\n",
       "      <th>job</th>\n",
       "      <th>address</th>\n",
       "      <th>username</th>\n",
       "      <th>url</th>\n",
       "      <th>hobby</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My name is Aaliyah Popova, and I am a jeweler with 13 years of experience. I remember a very unique and challenging project I had to work on last year. A customer approached me with a precious family heirloom - a diamond necklace that had been passed down through generations. Unfortunately, the necklace was in poor condition, with several loose diamonds and a broken clasp. The customer wanted me to restore it to its former glory, but it was clear that this would be no ordinary repair. Using my specialized tools and techniques, I began the delicate task of dismantling the necklace. Each diamond was carefully removed from its setting, and the damaged clasp was removed. Once the necklace was completely disassembled, I meticulously cleaned each diamond and inspected it for any damage. Fortunately, the diamonds were all in good condition, with no cracks or chips. The next step was to repair the broken clasp. I carefully soldered the broken pieces back together, ensuring that the clasp was sturdy and secure. Once the clasp was repaired, I began the process of reassembling the necklace. Each diamond was carefully placed back into its setting, and the necklace was polished until it sparkled like new. When I presented the restored necklace to the customer, they were overjoyed. They couldn't believe that I had been able to bring their family heirloom back to life. The necklace looked as beautiful as it had when it was first created, and the customer was thrilled to have it back in their possession. If you have a project that you would like to discuss, please feel free to contact me by phone at (95) 94215-7906 or by email at aaliyah.popova4783@aol.edu. I look forward to hearing from you! P.S.: When I'm not creating beautiful jewelry, I enjoy spending time podcasting. I love sharing my knowledge about jewelry and connecting with other people who are passionate about this art form. I also enjoy spending time with my family and exploring new places. If you would like to learn more about me, please feel free to visit my website at [website address] or visit me at my studio located at 97 Lincoln Street.</td>\n",
       "      <td>['My', 'name', 'is', 'Aaliyah', 'Popova,', 'and', 'I', 'am', 'a', 'jeweler', 'with', '13', 'years', 'of', 'experience.', 'I', 'remember', 'a', 'very', 'unique', 'and', 'challenging', 'project', 'I', 'had', 'to', 'work', 'on', 'last', 'year.', 'A', 'customer', 'approached', 'me', 'with', 'a', 'precious', 'family', 'heirloom', '-', 'a', 'diamond', 'necklace', 'that', 'had', 'been', 'passed', 'down', 'through', 'generations.', 'Unfortunately,', 'the', 'necklace', 'was', 'in', 'poor', 'condition,', 'with', 'several', 'loose', 'diamonds', 'and', 'a', 'broken', 'clasp.', 'The', 'customer', 'wanted', 'me', 'to', 'restore', 'it', 'to', 'its', 'former', 'glory,', 'but', 'it', 'was', 'clear', 'that', 'this', 'would', 'be', 'no', 'ordinary', 'repair.', 'Using', 'my', 'specialized', 'tools', 'and', 'techniques,', 'I', 'began', 'the', 'delicate', 'task', 'of', 'dismantling', 'the', 'necklace.', 'Each', 'diamond', 'was', 'carefully', 'removed', 'from', 'its', 'setting,', 'and', 'the', 'damaged', 'clasp', 'was', 'removed.', 'Once', 'the', 'necklace', 'was', 'completely', 'disassembled,', 'I', 'meticulously', 'cleaned', 'each', 'diamond', 'and', 'inspected', 'it', 'for', 'any', 'damage.', 'Fortunately,', 'the', 'diamonds', 'were', 'all', 'in', 'good', 'condition,', 'with', 'no', 'cracks', 'or', 'chips.', 'The', 'next', 'step', 'was', 'to', 'repair', 'the', 'broken', 'clasp.', 'I', 'carefully', 'soldered', 'the', 'broken', 'pieces', 'back', 'together,', 'ensuring', 'that', 'the', 'clasp', 'was', 'sturdy', 'and', 'secure.', 'Once', 'the', 'clasp', 'was', 'repaired,', 'I', 'began', 'the', 'process', 'of', 'reassembling', 'the', 'necklace.', 'Each', 'diamond', 'was', 'carefully', 'placed', 'back', 'into', 'its', 'setting,', 'and', 'the', 'necklace', 'was', 'polished', 'until', 'it', 'sparkled', 'like', 'new.', 'When', 'I', 'presented', 'the', 'restored', 'necklace', 'to', 'the', 'customer,', 'they', 'were', 'overjoyed.', 'They', \"couldn't\", 'believe', 'that', 'I', 'had', 'been', 'able', 'to', 'bring', 'their', 'family', 'heirloom', 'back', 'to', 'life.', 'The', 'necklace', 'looked', 'as', 'beautiful', 'as', 'it', 'had', 'when', 'it', 'was', 'first', 'created,', 'and', 'the', 'customer', 'was', 'thrilled', 'to', 'have', 'it', 'back', 'in', 'their', 'possession.', 'If', 'you', 'have', 'a', 'project', 'that', 'you', 'would', 'like', 'to', 'discuss,', 'please', 'feel', 'free', 'to', 'contact', 'me', 'by', 'phone', 'at', '(95)', '94215-7906', 'or', 'by', 'email', 'at', 'aaliyah.popova4783@aol.edu.', 'I', 'look', 'forward', 'to', 'hearing', 'from', 'you!', 'P.S.:', 'When', \"I'm\", 'not', 'creating', 'beautiful', 'jewelry,', 'I', 'enjoy', 'spending', 'time', 'podcasting.', 'I', 'love', 'sharing', 'my', 'knowledge', 'about', 'jewelry', 'and', 'connecting', 'with', 'other', 'people', 'who', 'are', 'passionate', 'about', 'this', 'art', 'form.', 'I', 'also', 'enjoy', 'spending', 'time', 'with', 'my', 'family', 'and', 'exploring', 'new', 'places.', 'If', 'you', 'would', 'like', 'to', 'learn', 'more', 'about', 'me,', 'please', 'feel', 'free', 'to', 'visit', 'my', 'website', 'at', '[website', 'address]', 'or', 'visit', 'me', 'at', 'my', 'studio', 'located', 'at', '97', 'Lincoln', 'Street.']</td>\n",
       "      <td>['O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUDENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PHONE_NUM', 'I-PHONE_NUM', 'O', 'O', 'O', 'O', 'B-EMAIL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STREET_ADDRESS', 'I-STREET_ADDRESS', 'I-STREET_ADDRESS']</td>\n",
       "      <td>1073d46f-2241-459b-ab01-851be8d26436</td>\n",
       "      <td>[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False]</td>\n",
       "      <td>\\n    Aaliyah Popova is a jeweler with 13 years of experience. Write a detailed example in first person of a job-related project he/her did in the past. Add the following information about him/her randomly inside the text: name is Aaliyah Popova, phone number is (95) 94215-7906, email is aaliyah.popova4783@aol.edu, hobby is Podcasting, address is 97 Lincoln Street.\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>Aaliyah Popova</td>\n",
       "      <td>aaliyah.popova4783@aol.edu</td>\n",
       "      <td>(95) 94215-7906</td>\n",
       "      <td>jeweler</td>\n",
       "      <td>97 Lincoln Street</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Podcasting</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           text  \\\n",
       "0  My name is Aaliyah Popova, and I am a jeweler with 13 years of experience. I remember a very unique and challenging project I had to work on last year. A customer approached me with a precious family heirloom - a diamond necklace that had been passed down through generations. Unfortunately, the necklace was in poor condition, with several loose diamonds and a broken clasp. The customer wanted me to restore it to its former glory, but it was clear that this would be no ordinary repair. Using my specialized tools and techniques, I began the delicate task of dismantling the necklace. Each diamond was carefully removed from its setting, and the damaged clasp was removed. Once the necklace was completely disassembled, I meticulously cleaned each diamond and inspected it for any damage. Fortunately, the diamonds were all in good condition, with no cracks or chips. The next step was to repair the broken clasp. I carefully soldered the broken pieces back together, ensuring that the clasp was sturdy and secure. Once the clasp was repaired, I began the process of reassembling the necklace. Each diamond was carefully placed back into its setting, and the necklace was polished until it sparkled like new. When I presented the restored necklace to the customer, they were overjoyed. They couldn't believe that I had been able to bring their family heirloom back to life. The necklace looked as beautiful as it had when it was first created, and the customer was thrilled to have it back in their possession. If you have a project that you would like to discuss, please feel free to contact me by phone at (95) 94215-7906 or by email at aaliyah.popova4783@aol.edu. I look forward to hearing from you! P.S.: When I'm not creating beautiful jewelry, I enjoy spending time podcasting. I love sharing my knowledge about jewelry and connecting with other people who are passionate about this art form. I also enjoy spending time with my family and exploring new places. If you would like to learn more about me, please feel free to visit my website at [website address] or visit me at my studio located at 97 Lincoln Street.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           tokens  \\\n",
       "0  ['My', 'name', 'is', 'Aaliyah', 'Popova,', 'and', 'I', 'am', 'a', 'jeweler', 'with', '13', 'years', 'of', 'experience.', 'I', 'remember', 'a', 'very', 'unique', 'and', 'challenging', 'project', 'I', 'had', 'to', 'work', 'on', 'last', 'year.', 'A', 'customer', 'approached', 'me', 'with', 'a', 'precious', 'family', 'heirloom', '-', 'a', 'diamond', 'necklace', 'that', 'had', 'been', 'passed', 'down', 'through', 'generations.', 'Unfortunately,', 'the', 'necklace', 'was', 'in', 'poor', 'condition,', 'with', 'several', 'loose', 'diamonds', 'and', 'a', 'broken', 'clasp.', 'The', 'customer', 'wanted', 'me', 'to', 'restore', 'it', 'to', 'its', 'former', 'glory,', 'but', 'it', 'was', 'clear', 'that', 'this', 'would', 'be', 'no', 'ordinary', 'repair.', 'Using', 'my', 'specialized', 'tools', 'and', 'techniques,', 'I', 'began', 'the', 'delicate', 'task', 'of', 'dismantling', 'the', 'necklace.', 'Each', 'diamond', 'was', 'carefully', 'removed', 'from', 'its', 'setting,', 'and', 'the', 'damaged', 'clasp', 'was', 'removed.', 'Once', 'the', 'necklace', 'was', 'completely', 'disassembled,', 'I', 'meticulously', 'cleaned', 'each', 'diamond', 'and', 'inspected', 'it', 'for', 'any', 'damage.', 'Fortunately,', 'the', 'diamonds', 'were', 'all', 'in', 'good', 'condition,', 'with', 'no', 'cracks', 'or', 'chips.', 'The', 'next', 'step', 'was', 'to', 'repair', 'the', 'broken', 'clasp.', 'I', 'carefully', 'soldered', 'the', 'broken', 'pieces', 'back', 'together,', 'ensuring', 'that', 'the', 'clasp', 'was', 'sturdy', 'and', 'secure.', 'Once', 'the', 'clasp', 'was', 'repaired,', 'I', 'began', 'the', 'process', 'of', 'reassembling', 'the', 'necklace.', 'Each', 'diamond', 'was', 'carefully', 'placed', 'back', 'into', 'its', 'setting,', 'and', 'the', 'necklace', 'was', 'polished', 'until', 'it', 'sparkled', 'like', 'new.', 'When', 'I', 'presented', 'the', 'restored', 'necklace', 'to', 'the', 'customer,', 'they', 'were', 'overjoyed.', 'They', \"couldn't\", 'believe', 'that', 'I', 'had', 'been', 'able', 'to', 'bring', 'their', 'family', 'heirloom', 'back', 'to', 'life.', 'The', 'necklace', 'looked', 'as', 'beautiful', 'as', 'it', 'had', 'when', 'it', 'was', 'first', 'created,', 'and', 'the', 'customer', 'was', 'thrilled', 'to', 'have', 'it', 'back', 'in', 'their', 'possession.', 'If', 'you', 'have', 'a', 'project', 'that', 'you', 'would', 'like', 'to', 'discuss,', 'please', 'feel', 'free', 'to', 'contact', 'me', 'by', 'phone', 'at', '(95)', '94215-7906', 'or', 'by', 'email', 'at', 'aaliyah.popova4783@aol.edu.', 'I', 'look', 'forward', 'to', 'hearing', 'from', 'you!', 'P.S.:', 'When', \"I'm\", 'not', 'creating', 'beautiful', 'jewelry,', 'I', 'enjoy', 'spending', 'time', 'podcasting.', 'I', 'love', 'sharing', 'my', 'knowledge', 'about', 'jewelry', 'and', 'connecting', 'with', 'other', 'people', 'who', 'are', 'passionate', 'about', 'this', 'art', 'form.', 'I', 'also', 'enjoy', 'spending', 'time', 'with', 'my', 'family', 'and', 'exploring', 'new', 'places.', 'If', 'you', 'would', 'like', 'to', 'learn', 'more', 'about', 'me,', 'please', 'feel', 'free', 'to', 'visit', 'my', 'website', 'at', '[website', 'address]', 'or', 'visit', 'me', 'at', 'my', 'studio', 'located', 'at', '97', 'Lincoln', 'Street.']   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     labels  \\\n",
       "0  ['O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUDENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PHONE_NUM', 'I-PHONE_NUM', 'O', 'O', 'O', 'O', 'B-EMAIL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STREET_ADDRESS', 'I-STREET_ADDRESS', 'I-STREET_ADDRESS']   \n",
       "\n",
       "                               document  \\\n",
       "0  1073d46f-2241-459b-ab01-851be8d26436   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   trailing_whitespace  \\\n",
       "0  [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                  prompt  \\\n",
       "0  \\n    Aaliyah Popova is a jeweler with 13 years of experience. Write a detailed example in first person of a job-related project he/her did in the past. Add the following information about him/her randomly inside the text: name is Aaliyah Popova, phone number is (95) 94215-7906, email is aaliyah.popova4783@aol.edu, hobby is Podcasting, address is 97 Lincoln Street.\\n       \n",
       "\n",
       "   prompt_id            name                       email            phone  \\\n",
       "0          1  Aaliyah Popova  aaliyah.popova4783@aol.edu  (95) 94215-7906   \n",
       "\n",
       "       job            address username   url       hobby  len  \n",
       "0  jeweler  97 Lincoln Street     None  None  Podcasting  363  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 檢視資料集中的第一筆資料\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df = pd.DataFrame(immutable_dataset[:1])\n",
    "# 將 'text', 'tokens', labels' 三個欄位的順序移至前面\n",
    "df.insert(0, 'labels', df.pop('labels'))\n",
    "df.insert(0, 'tokens', df.pop('tokens'))\n",
    "df.insert(0, 'text', df.pop('text'))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這個表格結構，包含多項資訊，在這個筆記本中，我們將使用：\n",
    "\n",
    "* `text`: 一個完整字符串，包含 PII 資訊，將用作輸入。\n",
    "\n",
    "* `token`: 依據 `text` 切割後的字詞及字符，將用作訓練。\n",
    "\n",
    "* `label`: 對應每個 `token` 的標記，其中包含 PII 資訊的類別，將用作訓練。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料前處理\n",
    "\n",
    "保留必要 features: `text`, `tokens` 及 `labels`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 保留必要 features: 'text', 'tokens', 'labels'\n",
    "dataset = immutable_dataset.remove_columns([\n",
    "  'document', 'trailing_whitespace', 'prompt', 'prompt_id', 'name',\n",
    "  'email', 'phone', 'job', 'address', 'username', 'url', 'hobby', 'len'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "從 CSV 檔案讀取後的 `tokens` 與 `labels` 是字符串型別，我們需要將其轉換為 list 型別。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset[0]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset[0]['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下程式碼使用 `dataset.map` 方法來轉換數據集中的每個元素。具體來說，它將每個元素中的 `tokens` 和 `labels` 字段從字符串格式轉換為其對應的 Python 數據結構：\n",
    "\n",
    "* `dataset.map`: map 方法用於對數據集中的每個元素應用一個函數，並返回一個新的數據集。這個方法通常用於數據預處理和轉換。\n",
    "\n",
    "* `lambda x: { ... }`: 這是一個匿名函數（lambda 函數），它接受一個輸入 x，並返回一個字典。x 代表數據集中的一個元素。\n",
    "\n",
    "* `ast.literal_eval`: `ast.literal_eval` 是 Python 的 ast 模塊中的一個函數，它可以將字符串轉換為對應的 Python 數據結構，如列表、字典等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將 'tokens' 與 'labels' 從 string 轉換為 list (CSV 檔案讀取後會變成 string)\n",
    "dataset = dataset.map(\n",
    "  lambda x: {\n",
    "    'tokens': ast.literal_eval(x['tokens']),\n",
    "    'labels': ast.literal_eval(x['labels'])\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上程式碼的作用是對數據集中的每個元素進行轉換，將 `tokens` 和 `labels` 字段從字符串格式轉換為對應的 Python 數據結構。這樣做的目的是便於後續的數據處理和模型訓練。轉換後的數據集將包含已解析的 `tokens` 和 `labels`，而不再是原始的字符串格式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "透過過濾數據集中的元素，確保每個元素中的 `tokens` 和 `labels` 字段的長度相等，確保 `tokens` 與 `labels` 長度相等，避免有缺失的情況。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確認 tokens 長度與 labels 長度相等，避免有缺失的情況\n",
    "dataset = dataset.filter(lambda x: len(x['tokens']) == len(x['labels']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將 `tokens` 欄位重新命名為 `words` 避免與後面的 tokens 概念混淆。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'words', 'labels'],\n",
       "    num_rows: 4434\n",
       "})"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 將 tokens 欄位重新命名為 words 避免與後面的 tokens 概念混淆\n",
    "dataset = dataset.rename_column('tokens', 'words')\n",
    "\n",
    "# 顯示處理後的資料\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將數據集分割為訓練集、驗證集和測試集。\n",
    "\n",
    "* 保留 0.1% 的數據作為測試集，為了控制課程演示所以採用 `shuffle=False`，確保在不同運行中訓練集、驗證集和測試集保持一致。您可以試試看 `shuffle=True` 來打亂數據。\n",
    "\n",
    "* 將剩餘的數據集分割為 80% 的訓練集和 20% 的驗證集。\n",
    "\n",
    "* 創建包含訓練集、驗證集和測試集的數據集字典。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'words', 'labels'],\n",
       "        num_rows: 3543\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'words', 'labels'],\n",
       "        num_rows: 886\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'words', 'labels'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reserve 0.1% of the training set for testing\n",
    "test_dataset = dataset.train_test_split(\n",
    "  test_size=0.001, # 0.1% of the data is used for testing\n",
    "  shuffle=False, # Ensure that train and validation sets are the same across runs\n",
    "  )\n",
    "# Split into 80% training and 20% validation sets\n",
    "train_dataset = test_dataset['train'].train_test_split(\n",
    "  test_size=0.2, # 20% of the data is used for validation\n",
    "  shuffle=False, # Ensure that train and test sets are the same across runs\n",
    "  )\n",
    "dataset = DatasetDict({\n",
    "  'train': train_dataset['train'],\n",
    "  'validation': train_dataset['test'],\n",
    "  'test': test_dataset['test'],\n",
    "  })\n",
    "# 顯示處理後的資料\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "words",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "labels",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "c2693f47-c1bf-4090-8c7f-422d69db9320",
       "rows": [
        [
         "0",
         "My name is Aaliyah Popova, and I am a jeweler with 13 years of experience. I remember a very unique and challenging project I had to work on last year. A customer approached me with a precious family heirloom - a diamond necklace that had been passed down through generations. Unfortunately, the necklace was in poor condition, with several loose diamonds and a broken clasp. The customer wanted me to restore it to its former glory, but it was clear that this would be no ordinary repair. Using my specialized tools and techniques, I began the delicate task of dismantling the necklace. Each diamond was carefully removed from its setting, and the damaged clasp was removed. Once the necklace was completely disassembled, I meticulously cleaned each diamond and inspected it for any damage. Fortunately, the diamonds were all in good condition, with no cracks or chips. The next step was to repair the broken clasp. I carefully soldered the broken pieces back together, ensuring that the clasp was sturdy and secure. Once the clasp was repaired, I began the process of reassembling the necklace. Each diamond was carefully placed back into its setting, and the necklace was polished until it sparkled like new. When I presented the restored necklace to the customer, they were overjoyed. They couldn't believe that I had been able to bring their family heirloom back to life. The necklace looked as beautiful as it had when it was first created, and the customer was thrilled to have it back in their possession. If you have a project that you would like to discuss, please feel free to contact me by phone at (95) 94215-7906 or by email at aaliyah.popova4783@aol.edu. I look forward to hearing from you! P.S.: When I'm not creating beautiful jewelry, I enjoy spending time podcasting. I love sharing my knowledge about jewelry and connecting with other people who are passionate about this art form. I also enjoy spending time with my family and exploring new places. If you would like to learn more about me, please feel free to visit my website at [website address] or visit me at my studio located at 97 Lincoln Street.",
         "['My', 'name', 'is', 'Aaliyah', 'Popova,', 'and', 'I', 'am', 'a', 'jeweler', 'with', '13', 'years', 'of', 'experience.', 'I', 'remember', 'a', 'very', 'unique', 'and', 'challenging', 'project', 'I', 'had', 'to', 'work', 'on', 'last', 'year.', 'A', 'customer', 'approached', 'me', 'with', 'a', 'precious', 'family', 'heirloom', '-', 'a', 'diamond', 'necklace', 'that', 'had', 'been', 'passed', 'down', 'through', 'generations.', 'Unfortunately,', 'the', 'necklace', 'was', 'in', 'poor', 'condition,', 'with', 'several', 'loose', 'diamonds', 'and', 'a', 'broken', 'clasp.', 'The', 'customer', 'wanted', 'me', 'to', 'restore', 'it', 'to', 'its', 'former', 'glory,', 'but', 'it', 'was', 'clear', 'that', 'this', 'would', 'be', 'no', 'ordinary', 'repair.', 'Using', 'my', 'specialized', 'tools', 'and', 'techniques,', 'I', 'began', 'the', 'delicate', 'task', 'of', 'dismantling', 'the', 'necklace.', 'Each', 'diamond', 'was', 'carefully', 'removed', 'from', 'its', 'setting,', 'and', 'the', 'damaged', 'clasp', 'was', 'removed.', 'Once', 'the', 'necklace', 'was', 'completely', 'disassembled,', 'I', 'meticulously', 'cleaned', 'each', 'diamond', 'and', 'inspected', 'it', 'for', 'any', 'damage.', 'Fortunately,', 'the', 'diamonds', 'were', 'all', 'in', 'good', 'condition,', 'with', 'no', 'cracks', 'or', 'chips.', 'The', 'next', 'step', 'was', 'to', 'repair', 'the', 'broken', 'clasp.', 'I', 'carefully', 'soldered', 'the', 'broken', 'pieces', 'back', 'together,', 'ensuring', 'that', 'the', 'clasp', 'was', 'sturdy', 'and', 'secure.', 'Once', 'the', 'clasp', 'was', 'repaired,', 'I', 'began', 'the', 'process', 'of', 'reassembling', 'the', 'necklace.', 'Each', 'diamond', 'was', 'carefully', 'placed', 'back', 'into', 'its', 'setting,', 'and', 'the', 'necklace', 'was', 'polished', 'until', 'it', 'sparkled', 'like', 'new.', 'When', 'I', 'presented', 'the', 'restored', 'necklace', 'to', 'the', 'customer,', 'they', 'were', 'overjoyed.', 'They', \"couldn't\", 'believe', 'that', 'I', 'had', 'been', 'able', 'to', 'bring', 'their', 'family', 'heirloom', 'back', 'to', 'life.', 'The', 'necklace', 'looked', 'as', 'beautiful', 'as', 'it', 'had', 'when', 'it', 'was', 'first', 'created,', 'and', 'the', 'customer', 'was', 'thrilled', 'to', 'have', 'it', 'back', 'in', 'their', 'possession.', 'If', 'you', 'have', 'a', 'project', 'that', 'you', 'would', 'like', 'to', 'discuss,', 'please', 'feel', 'free', 'to', 'contact', 'me', 'by', 'phone', 'at', '(95)', '94215-7906', 'or', 'by', 'email', 'at', 'aaliyah.popova4783@aol.edu.', 'I', 'look', 'forward', 'to', 'hearing', 'from', 'you!', 'P.S.:', 'When', \"I'm\", 'not', 'creating', 'beautiful', 'jewelry,', 'I', 'enjoy', 'spending', 'time', 'podcasting.', 'I', 'love', 'sharing', 'my', 'knowledge', 'about', 'jewelry', 'and', 'connecting', 'with', 'other', 'people', 'who', 'are', 'passionate', 'about', 'this', 'art', 'form.', 'I', 'also', 'enjoy', 'spending', 'time', 'with', 'my', 'family', 'and', 'exploring', 'new', 'places.', 'If', 'you', 'would', 'like', 'to', 'learn', 'more', 'about', 'me,', 'please', 'feel', 'free', 'to', 'visit', 'my', 'website', 'at', '[website', 'address]', 'or', 'visit', 'me', 'at', 'my', 'studio', 'located', 'at', '97', 'Lincoln', 'Street.']",
         "['O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUDENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PHONE_NUM', 'I-PHONE_NUM', 'O', 'O', 'O', 'O', 'B-EMAIL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STREET_ADDRESS', 'I-STREET_ADDRESS', 'I-STREET_ADDRESS']"
        ],
        [
         "1",
         "My name is Konstantin Becker, and I'm a developer with two years of experience. I recently worked on a project where we built a new customer portal for our company. The goal was to create a more user-friendly and intuitive interface that would make it easier for customers to manage their accounts and access information. We started by gathering requirements from our customers and conducting user research to understand their needs and pain points. We then designed a new user interface that was both visually appealing and easy to use. We also implemented a number of new features, such as the ability for customers to view their account history, track their orders, and submit support tickets online. The project was a success, and our customers were very happy with the new portal. They found it to be much easier to use than the old one, and they appreciated the new features. We saw a significant increase in customer satisfaction and engagement as a result of the new portal. Throughout the project, I was responsible for developing the back-end code for the portal. I also worked closely with the design team to ensure that the portal was visually appealing and user-friendly. I'm proud of the work that I did on this project, and I'm confident that it will continue to benefit our customers for years to come. If you would like to contact me, my email address is konstantin.becker@gmail.com and my phone number is 0475 4429797. I live at 826 Webster Street. Quilting is my hobby.",
         "['My', 'name', 'is', 'Konstantin', 'Becker,', 'and', \"I'm\", 'a', 'developer', 'with', 'two', 'years', 'of', 'experience.', 'I', 'recently', 'worked', 'on', 'a', 'project', 'where', 'we', 'built', 'a', 'new', 'customer', 'portal', 'for', 'our', 'company.', 'The', 'goal', 'was', 'to', 'create', 'a', 'more', 'user-friendly', 'and', 'intuitive', 'interface', 'that', 'would', 'make', 'it', 'easier', 'for', 'customers', 'to', 'manage', 'their', 'accounts', 'and', 'access', 'information.', 'We', 'started', 'by', 'gathering', 'requirements', 'from', 'our', 'customers', 'and', 'conducting', 'user', 'research', 'to', 'understand', 'their', 'needs', 'and', 'pain', 'points.', 'We', 'then', 'designed', 'a', 'new', 'user', 'interface', 'that', 'was', 'both', 'visually', 'appealing', 'and', 'easy', 'to', 'use.', 'We', 'also', 'implemented', 'a', 'number', 'of', 'new', 'features,', 'such', 'as', 'the', 'ability', 'for', 'customers', 'to', 'view', 'their', 'account', 'history,', 'track', 'their', 'orders,', 'and', 'submit', 'support', 'tickets', 'online.', 'The', 'project', 'was', 'a', 'success,', 'and', 'our', 'customers', 'were', 'very', 'happy', 'with', 'the', 'new', 'portal.', 'They', 'found', 'it', 'to', 'be', 'much', 'easier', 'to', 'use', 'than', 'the', 'old', 'one,', 'and', 'they', 'appreciated', 'the', 'new', 'features.', 'We', 'saw', 'a', 'significant', 'increase', 'in', 'customer', 'satisfaction', 'and', 'engagement', 'as', 'a', 'result', 'of', 'the', 'new', 'portal.', 'Throughout', 'the', 'project,', 'I', 'was', 'responsible', 'for', 'developing', 'the', 'back-end', 'code', 'for', 'the', 'portal.', 'I', 'also', 'worked', 'closely', 'with', 'the', 'design', 'team', 'to', 'ensure', 'that', 'the', 'portal', 'was', 'visually', 'appealing', 'and', 'user-friendly.', \"I'm\", 'proud', 'of', 'the', 'work', 'that', 'I', 'did', 'on', 'this', 'project,', 'and', \"I'm\", 'confident', 'that', 'it', 'will', 'continue', 'to', 'benefit', 'our', 'customers', 'for', 'years', 'to', 'come.', 'If', 'you', 'would', 'like', 'to', 'contact', 'me,', 'my', 'email', 'address', 'is', 'konstantin.becker@gmail.com', 'and', 'my', 'phone', 'number', 'is', '0475', '4429797.', 'I', 'live', 'at', '826', 'Webster', 'Street.', 'Quilting', 'is', 'my', 'hobby.']",
         "['O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUDENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-EMAIL', 'O', 'O', 'O', 'O', 'O', 'B-PHONE_NUM', 'I-PHONE_NUM', 'O', 'O', 'O', 'B-STREET_ADDRESS', 'I-STREET_ADDRESS', 'I-STREET_ADDRESS', 'O', 'O', 'O', 'O']"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My name is Aaliyah Popova, and I am a jeweler with 13 years of experience. I remember a very unique and challenging project I had to work on last year. A customer approached me with a precious family heirloom - a diamond necklace that had been passed down through generations. Unfortunately, the necklace was in poor condition, with several loose diamonds and a broken clasp. The customer wanted me to restore it to its former glory, but it was clear that this would be no ordinary repair. Using my specialized tools and techniques, I began the delicate task of dismantling the necklace. Each diamond was carefully removed from its setting, and the damaged clasp was removed. Once the necklace was completely disassembled, I meticulously cleaned each diamond and inspected it for any damage. Fortunately, the diamonds were all in good condition, with no cracks or chips. The next step was to repair the broken clasp. I carefully soldered the broken pieces back together, ensuring that the clasp was sturdy and secure. Once the clasp was repaired, I began the process of reassembling the necklace. Each diamond was carefully placed back into its setting, and the necklace was polished until it sparkled like new. When I presented the restored necklace to the customer, they were overjoyed. They couldn't believe that I had been able to bring their family heirloom back to life. The necklace looked as beautiful as it had when it was first created, and the customer was thrilled to have it back in their possession. If you have a project that you would like to discuss, please feel free to contact me by phone at (95) 94215-7906 or by email at aaliyah.popova4783@aol.edu. I look forward to hearing from you! P.S.: When I'm not creating beautiful jewelry, I enjoy spending time podcasting. I love sharing my knowledge about jewelry and connecting with other people who are passionate about this art form. I also enjoy spending time with my family and exploring new places. If you would like to learn more about me, please feel free to visit my website at [website address] or visit me at my studio located at 97 Lincoln Street.</td>\n",
       "      <td>[My, name, is, Aaliyah, Popova,, and, I, am, a, jeweler, with, 13, years, of, experience., I, remember, a, very, unique, and, challenging, project, I, had, to, work, on, last, year., A, customer, approached, me, with, a, precious, family, heirloom, -, a, diamond, necklace, that, had, been, passed, down, through, generations., Unfortunately,, the, necklace, was, in, poor, condition,, with, several, loose, diamonds, and, a, broken, clasp., The, customer, wanted, me, to, restore, it, to, its, former, glory,, but, it, was, clear, that, this, would, be, no, ordinary, repair., Using, my, specialized, tools, and, techniques,, I, began, the, delicate, task, of, dismantling, ...]</td>\n",
       "      <td>[O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My name is Konstantin Becker, and I'm a developer with two years of experience. I recently worked on a project where we built a new customer portal for our company. The goal was to create a more user-friendly and intuitive interface that would make it easier for customers to manage their accounts and access information. We started by gathering requirements from our customers and conducting user research to understand their needs and pain points. We then designed a new user interface that was both visually appealing and easy to use. We also implemented a number of new features, such as the ability for customers to view their account history, track their orders, and submit support tickets online. The project was a success, and our customers were very happy with the new portal. They found it to be much easier to use than the old one, and they appreciated the new features. We saw a significant increase in customer satisfaction and engagement as a result of the new portal. Throughout the project, I was responsible for developing the back-end code for the portal. I also worked closely with the design team to ensure that the portal was visually appealing and user-friendly. I'm proud of the work that I did on this project, and I'm confident that it will continue to benefit our customers for years to come. If you would like to contact me, my email address is konstantin.becker@gmail.com and my phone number is 0475 4429797. I live at 826 Webster Street. Quilting is my hobby.</td>\n",
       "      <td>[My, name, is, Konstantin, Becker,, and, I'm, a, developer, with, two, years, of, experience., I, recently, worked, on, a, project, where, we, built, a, new, customer, portal, for, our, company., The, goal, was, to, create, a, more, user-friendly, and, intuitive, interface, that, would, make, it, easier, for, customers, to, manage, their, accounts, and, access, information., We, started, by, gathering, requirements, from, our, customers, and, conducting, user, research, to, understand, their, needs, and, pain, points., We, then, designed, a, new, user, interface, that, was, both, visually, appealing, and, easy, to, use., We, also, implemented, a, number, of, new, features,, such, as, ...]</td>\n",
       "      <td>[O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           text  \\\n",
       "0  My name is Aaliyah Popova, and I am a jeweler with 13 years of experience. I remember a very unique and challenging project I had to work on last year. A customer approached me with a precious family heirloom - a diamond necklace that had been passed down through generations. Unfortunately, the necklace was in poor condition, with several loose diamonds and a broken clasp. The customer wanted me to restore it to its former glory, but it was clear that this would be no ordinary repair. Using my specialized tools and techniques, I began the delicate task of dismantling the necklace. Each diamond was carefully removed from its setting, and the damaged clasp was removed. Once the necklace was completely disassembled, I meticulously cleaned each diamond and inspected it for any damage. Fortunately, the diamonds were all in good condition, with no cracks or chips. The next step was to repair the broken clasp. I carefully soldered the broken pieces back together, ensuring that the clasp was sturdy and secure. Once the clasp was repaired, I began the process of reassembling the necklace. Each diamond was carefully placed back into its setting, and the necklace was polished until it sparkled like new. When I presented the restored necklace to the customer, they were overjoyed. They couldn't believe that I had been able to bring their family heirloom back to life. The necklace looked as beautiful as it had when it was first created, and the customer was thrilled to have it back in their possession. If you have a project that you would like to discuss, please feel free to contact me by phone at (95) 94215-7906 or by email at aaliyah.popova4783@aol.edu. I look forward to hearing from you! P.S.: When I'm not creating beautiful jewelry, I enjoy spending time podcasting. I love sharing my knowledge about jewelry and connecting with other people who are passionate about this art form. I also enjoy spending time with my family and exploring new places. If you would like to learn more about me, please feel free to visit my website at [website address] or visit me at my studio located at 97 Lincoln Street.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              My name is Konstantin Becker, and I'm a developer with two years of experience. I recently worked on a project where we built a new customer portal for our company. The goal was to create a more user-friendly and intuitive interface that would make it easier for customers to manage their accounts and access information. We started by gathering requirements from our customers and conducting user research to understand their needs and pain points. We then designed a new user interface that was both visually appealing and easy to use. We also implemented a number of new features, such as the ability for customers to view their account history, track their orders, and submit support tickets online. The project was a success, and our customers were very happy with the new portal. They found it to be much easier to use than the old one, and they appreciated the new features. We saw a significant increase in customer satisfaction and engagement as a result of the new portal. Throughout the project, I was responsible for developing the back-end code for the portal. I also worked closely with the design team to ensure that the portal was visually appealing and user-friendly. I'm proud of the work that I did on this project, and I'm confident that it will continue to benefit our customers for years to come. If you would like to contact me, my email address is konstantin.becker@gmail.com and my phone number is 0475 4429797. I live at 826 Webster Street. Quilting is my hobby.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       words  \\\n",
       "0                    [My, name, is, Aaliyah, Popova,, and, I, am, a, jeweler, with, 13, years, of, experience., I, remember, a, very, unique, and, challenging, project, I, had, to, work, on, last, year., A, customer, approached, me, with, a, precious, family, heirloom, -, a, diamond, necklace, that, had, been, passed, down, through, generations., Unfortunately,, the, necklace, was, in, poor, condition,, with, several, loose, diamonds, and, a, broken, clasp., The, customer, wanted, me, to, restore, it, to, its, former, glory,, but, it, was, clear, that, this, would, be, no, ordinary, repair., Using, my, specialized, tools, and, techniques,, I, began, the, delicate, task, of, dismantling, ...]   \n",
       "1  [My, name, is, Konstantin, Becker,, and, I'm, a, developer, with, two, years, of, experience., I, recently, worked, on, a, project, where, we, built, a, new, customer, portal, for, our, company., The, goal, was, to, create, a, more, user-friendly, and, intuitive, interface, that, would, make, it, easier, for, customers, to, manage, their, accounts, and, access, information., We, started, by, gathering, requirements, from, our, customers, and, conducting, user, research, to, understand, their, needs, and, pain, points., We, then, designed, a, new, user, interface, that, was, both, visually, appealing, and, easy, to, use., We, also, implemented, a, number, of, new, features,, such, as, ...]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                        labels  \n",
       "0  [O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...]  \n",
       "1  [O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...]  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 顯示前 first_n_data 筆資料\n",
    "first_n_data = 2\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.DataFrame(dataset['train'].select(range(first_n_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "為了方便我們說明及檢視資料，所以我們期望將 `words` 與 `labels` 並列對齊顯示如下：\n",
    "\n",
    "| 1      | 2   | 3              | 4              |\n",
    "|--------|-----|----------------|----------------|\n",
    "| Hello, | I'm | Nicholas       | Moore,         |\n",
    "| O      | O   | B-NAME_STUDENT | I-NAME_STUDENT |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "於是我們定義一個函數，輸入為單一筆數據及最大列印長度，輸出為對齊後的數據。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_words_labels(x, max_display):\n",
    "    words = x['words'][:max_display]\n",
    "    labels = x['labels'][:max_display]\n",
    "    line1 = \"\"\n",
    "    line2 = \"\"\n",
    "    for word, label in zip(words, labels): # 逐一取出 word 與 label\n",
    "        # 計算 word 與 label 的最大長度, 並將 word 與 label 用空白補齊至相同長度\n",
    "        max_length = max(len(word), len(label))\n",
    "        line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "        line2 += label + \" \" * (max_length - len(label) + 1)\n",
    "    pprint(line1, width=200)\n",
    "    pprint(line2, width=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('My name is Aaliyah        Popova,        and I am a jeweler with 13 years of experience. I remember a very unique and challenging project I had to work on last year. A customer approached me with '\n",
      " 'a precious family heirloom - a diamond necklace that had been passed down through generations. ')\n",
      "('O  O    O  B-NAME_STUDENT I-NAME_STUDENT O   O O  O O       O    O  O     O  O           O O        O O    O      O   O           O       O O   O  O    O  O    O     O O        O          O  O    '\n",
      " 'O O        O      O        O O O       O        O    O   O    O      O    O       O            ')\n",
      "\n",
      "(\"My name is Konstantin     Becker,        and I'm a developer with two years of experience. I recently worked on a project where we built a new customer portal for our company. The goal was to \"\n",
      " 'create a more user-friendly and intuitive interface that would make it easier for customers to manage ')\n",
      "('O  O    O  B-NAME_STUDENT I-NAME_STUDENT O   O   O O         O    O   O     O  O           O O        O      O  O O       O     O  O     O O   O        O      O   O   O        O   O    O   O  '\n",
      " 'O      O O    O             O   O         O         O    O     O    O  O      O   O         O  O      ')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 並列顯示前 max_display 個 words 與 labels\n",
    "max_display = 50\n",
    "\n",
    "# 顯示前 first_n_data 筆資料中的前 max_display 個 words 與 labels\n",
    "for i in range(first_n_data):\n",
    "    display_words_labels(dataset['train'][i], max_display)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料中的 BIO 標注\n",
    "\n",
    "IOB 格式（Inside, Outside, Beginning 的縮寫），也常被稱為 BIO 格式，是計算語言學中用於標記任務（例如命名實體識別 NER，詞性標記 POS）的常見標記格式。\n",
    "\n",
    "* B - for the first token of a named entity\n",
    "\n",
    "* I - for tokens inside named entity's\n",
    "\n",
    "* O - for tokens outside any named entity\n",
    "\n",
    "我們需要先了解數據集中包含哪些 BIO 標注，所以我們先從訓練集中取出所有的標籤，並且去除重複值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 資料集中的標籤 ===\n",
      "['B-EMAIL', 'B-NAME_STUDENT', 'B-PHONE_NUM', 'B-STREET_ADDRESS',\n",
      " 'B-URL_PERSONAL', 'B-USERNAME', 'I-NAME_STUDENT', 'I-PHONE_NUM',\n",
      " 'I-STREET_ADDRESS', 'O']\n",
      "\n",
      "標籤數量: 10\n"
     ]
    }
   ],
   "source": [
    "# 顯示 BIO 標注\n",
    "label_names = set()\n",
    "for data in dataset['train']:\n",
    "    # 將 labels 欄位轉換為 set 並更新 label_names\n",
    "    label_names.update(data['labels'])\n",
    "# convert set to list and sort label names\n",
    "label_names = list(label_names)\n",
    "# sort label names\n",
    "label_names.sort()\n",
    "print('=== 資料集中的標籤 ===')\n",
    "pprint(label_names, compact=True)\n",
    "print()\n",
    "print(f'標籤數量: {len(label_names)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上，我們可以獲得十個標籤，分別為：\n",
    "\n",
    "```json\n",
    "[\n",
    "  'B-EMAIL', 'B-NAME_STUDENT', 'B-PHONE_NUM', 'B-STREET_ADDRESS', 'B-URL_PERSONAL', \n",
    "  'B-USERNAME', 'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'O'\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方便後續訓練過程，我們將建立兩個對照表，一個是將 BIO 標籤映射到一個整數編碼，另一個是將整數編碼映射回 BIO 標籤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== id2tag ===\n",
      "[(0, 'B-EMAIL'), (1, 'B-NAME_STUDENT'), (2, 'B-PHONE_NUM'),\n",
      " (3, 'B-STREET_ADDRESS'), (4, 'B-URL_PERSONAL'), (5, 'B-USERNAME'),\n",
      " (6, 'I-NAME_STUDENT'), (7, 'I-PHONE_NUM'), (8, 'I-STREET_ADDRESS'), (9, 'O')]\n"
     ]
    }
   ],
   "source": [
    "# 整數標籤到 BIO 標籤的映射\n",
    "id2tag = dict(enumerate(label_names))\n",
    "\n",
    "print('=== id2tag ===')\n",
    "pprint(sorted(id2tag.items(), key=lambda x: x[0]), compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一張對照表是 0 代表 `B-EMAIL`，1 代表 `B-NAME_STUDENT`，2 代表 `B-PHONE_NUM`，以此類推。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== tag2id ===\n",
      "[('B-EMAIL', 0), ('B-NAME_STUDENT', 1), ('B-PHONE_NUM', 2),\n",
      " ('B-STREET_ADDRESS', 3), ('B-URL_PERSONAL', 4), ('B-USERNAME', 5),\n",
      " ('I-NAME_STUDENT', 6), ('I-PHONE_NUM', 7), ('I-STREET_ADDRESS', 8), ('O', 9)]\n"
     ]
    }
   ],
   "source": [
    "# BIO 標籤到整數標籤的映射\n",
    "tag2id = dict((v, k) for k, v in id2tag.items())\n",
    "\n",
    "print('=== tag2id ===')\n",
    "pprint(sorted(tag2id.items(), key=lambda x: x[1]), compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反之，建立另一個對照表，將 `B-EMAIL` 映射回 0，將 `B-NAME_STUDENT` 映射回 1，將 `B-PHONE_NUM` 映射回 2，以此類推。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練參數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "當我們在微調或訓練機器學習模型時，有幾個重要的參數需要設定。這些參數會影響模型的訓練效果和性能。以下是這些參數的簡單解釋：\n",
    "\n",
    "* 批次大小（Batch Size）：每次訓練迭代中使用的樣本數量。假設你有1000個數據點，批次大小為32，這意味著模型每次會使用32個數據點來更新權重。當所有數據點都被使用完一次後，這稱為一個 epoch。較大的批次大小可以加速訓練，但需要更多的內存；較小的批次大小則更穩定，但訓練速度較慢。\n",
    "* 訓練輪數（Epochs）：完整遍歷訓練數據集的次數。如果你設定 epochs 為10，這意味著模型會完整地看10次訓練數據集。更多的 epochs 可以讓模型學習得更充分，但過多的 epochs 可能會導致過擬合（Overfitting / 模型在訓練數據上表現很好，但在新數據上表現不好）。\n",
    "* 學習率（Learning Rate）：每次更新模型權重時的步伐大小。學習率決定了模型在每次更新時應該調整多少權重。較高的學習率會使模型快速學習，但可能會跳過最佳解；較低的學習率會使模型穩定學習，但訓練時間較長。學習率需要仔細調整，過高或過低都會影響模型的性能。\n",
    "* 隨機失活（Dropout）：在訓練過程中隨機忽略一些神經元的比例。Dropout 是一種正則化技術，用於防止過擬合。它通過在每次訓練步驟中隨機忽略一些神經元來強制模型學習更穩健的特徵。適當的 Dropout 可以提高模型的泛化能力，但過高的 Dropout 可能會導致模型欠擬合（Underfitting / 模型在訓練數據和新數據上都表現不好）。\n",
    "* 權重衰減（Weight Decay）：在每次更新權重時，對權重施加的正則化項。Weight Decay 是另一種正則化技術，用於防止過擬合。它通過在每次更新時對權重施加懲罰，使權重保持較小的值。適當的 Weight Decay 可以提高模型的泛化能力，但過高的 Weight Decay 可能會導致模型欠擬合。\n",
    "\n",
    "這些參數在模型訓練中扮演著重要角色，影響著模型的學習速度、穩定性和泛化能力。調整這些參數需要根據具體的數據集和任務進行實驗和調整，以找到最佳的組合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練設定\n",
    "\n",
    "我們將使用具備 NER 的蒸餾模型（Model Distillation）加速微調，蒸餾模型是一種模型壓縮技術，用於將一個大型且複雜的模型（稱為「教師模型」）的知識轉移到一個較小且更高效的模型（稱為「學生模型」）。這種技術的目的是在保持模型性能的同時，減少模型的計算資源需求和存儲空間。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 訓練相關設定\n",
    "class Config(BaseModel):\n",
    "  model_name: str = 'dslim/distilbert-NER' # 使用蒸餾模型，降低參數量，加快訓練速度\n",
    "  saved_model_path: str = os.path.join('saved_model', f'{__ipynb_file__}') # path to save the trained model\n",
    "  train_batch_size: int = 4 # size of the input batch in training\n",
    "  eval_batch_size: int = 4 # size of the input batch in evaluation\n",
    "  epochs: int = 1 # number of times to iterate over the entire training dataset\n",
    "  lr: float = 2e-5 # learning rate, controls how fast or slow the model learns\n",
    "  weight_decay: float = 0.01 # weight decay, helps the model stay simple and avoid overfitting by penalizing large weights.\n",
    "\n",
    "  # BIO 標籤的設定\n",
    "  tags: list # BIO 標註的標籤列表\n",
    "  id2tag: dict # 整數標籤到 BIO 標籤的映射\n",
    "  tag2id: dict # BIO 標籤到整數標籤的映射\n",
    "  num_tags: int # 標籤數量\n",
    "\n",
    "config = Config(\n",
    "  tags=label_names,\n",
    "  id2tag=id2tag,\n",
    "  tag2id=tag2id,\n",
    "  num_tags=len(label_names)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning 前的表現"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入預訓練分詞器 (Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertTokenizerFast(name_or_path='dslim/distilbert-NER', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 透過預訓練模型取得 Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  config.model_name,\n",
    ")\n",
    "pprint(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確認為 fast tokenizer，方便後續做資料預處理。\n",
    "\n",
    "> [Fast tokenizers’ special powers](https://huggingface.co/learn/nlp-course/chapter6/3?fw=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 確認 tokenizer 是否為 fast tokenizer\n",
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入預訓練模型\n",
    "\n",
    "透過 `AutoModelForTokenClassification` 載入預訓練模型，並確認模型的分類數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "  config.model_name,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForTokenClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "pprint(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這是一個典型的 Encoder-Only 架構，其中包含一個 BERT 模型和一個線性分類器。BERT 模型用於提取特徵，線性分類器用於將特徵映射到分類標籤。\n",
    "\n",
    "觀察預訓練模型的分類數為 9。\n",
    "\n",
    "```json\n",
    "(classifier): Linear(in_features=768, out_features=9, bias=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning 前的表現"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 單筆演示分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 載入預訓練模型，宣告分類器\n",
    "classifier = pipeline(\n",
    "  task=\"token-classification\",\n",
    "  model=model,\n",
    "  tokenizer=tokenizer,\n",
    "  device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 透過預訓練模型預測分類\n",
    "predict = classifier(dataset['test'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'end': 19,\n",
      "  'entity': 'B-PER',\n",
      "  'index': 6,\n",
      "  'score': 0.99523276,\n",
      "  'start': 11,\n",
      "  'word': 'Nicholas'},\n",
      " {'end': 25,\n",
      "  'entity': 'I-PER',\n",
      "  'index': 7,\n",
      "  'score': 0.9980672,\n",
      "  'start': 20,\n",
      "  'word': 'Moore'},\n",
      " {'end': 758,\n",
      "  'entity': 'B-LOC',\n",
      "  'index': 152,\n",
      "  'score': 0.9941075,\n",
      "  'start': 749,\n",
      "  'word': 'Southeast'},\n",
      " {'end': 763,\n",
      "  'entity': 'I-LOC',\n",
      "  'index': 153,\n",
      "  'score': 0.9938945,\n",
      "  'start': 759,\n",
      "  'word': 'Asia'},\n",
      " {'end': 819,\n",
      "  'entity': 'B-LOC',\n",
      "  'index': 166,\n",
      "  'score': 0.9978047,\n",
      "  'start': 813,\n",
      "  'word': 'Europe'},\n",
      " {'end': 871,\n",
      "  'entity': 'B-LOC',\n",
      "  'index': 178,\n",
      "  'score': 0.9944898,\n",
      "  'start': 866,\n",
      "  'word': 'South'},\n",
      " {'end': 879,\n",
      "  'entity': 'I-LOC',\n",
      "  'index': 179,\n",
      "  'score': 0.9910623,\n",
      "  'start': 872,\n",
      "  'word': 'America'}]\n"
     ]
    }
   ],
   "source": [
    "pprint(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們將獲得一個句子中被標記的字詞，其中 `word` 為字詞，`entity` 為標籤分類。\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    'entity': 'B-PER', \n",
    "    'word': 'Nicholas'\n",
    "  }, {\n",
    "    'entity': 'I-PER',\n",
    "    'word': 'Moore'\n",
    "  },\n",
    "  ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 批次演示分類\n",
    "\n",
    "初步了解如何利用 `pipeline` 進行分類，我們將定義一個 `display_words_entity` 函數，輸入為一個句子，輸出為對齊後的字詞及標籤。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_words_entity(text, classifier):\n",
    "    result = classifier(text) # 預測結果\n",
    "    # pprint(result)\n",
    "\n",
    "    last_label = None\n",
    "    last_index = -1\n",
    "    new_result = []\n",
    "    merged_word = \"\"\n",
    "    for r in result:\n",
    "        # 取出預測結果中的 word 與 entity\n",
    "        # Remove ## from the word if it is a subword token\n",
    "        word = r['word'].replace('##', '')\n",
    "        label = r['entity']\n",
    "        index = r['index']\n",
    "        if last_label == label and index == last_index + 1:\n",
    "            last_index = index\n",
    "            merged_word += word\n",
    "        else: # last_label != label\n",
    "            if last_label != None:\n",
    "                new_result.append({'word': merged_word, 'entity': last_label})\n",
    "            last_label = label\n",
    "            last_index = index\n",
    "            merged_word = word\n",
    "\n",
    "    line1 = \"\"\n",
    "    line2 = \"\"\n",
    "    for r in new_result:\n",
    "        # 取出預測結果中的 word 與 entity\n",
    "        word = r['word']\n",
    "        label = r['entity']\n",
    "        # 計算 word 與 entity 的最大長度, 並將 word 與 entity 用空白補\n",
    "        max_length = max(len(word), len(label))\n",
    "        line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "        line2 += label + \" \" * (max_length - len(label) + 1)\n",
    "    pprint(line1)\n",
    "    pprint(line2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "輸入: Hello, I'm Nicholas Moore, a man with a rich tapestry of life experiences. Throughout my journey, I've encountered countless individuals and embarked on extraordinary adventures, all of which have shaped me into the person I am today. I grew up in a quaint little town nestled amidst rolling hills and vibrant meadows. My childhood was filled with laughter, exploration, and the warmth of family. As I matured, my thirst for knowledge and adventure propelled me to pursue higher education. I delved into the world of literature, philosophy, and science, expanding my horizons and igniting a passion for lifelong learning. In my early twenties, I embarked on a transformative journey that took me across continents and cultures. I backpacked through Southeast Asia, marveled at the wonders of the ancient world in Europe, and immersed myself in the vibrant energy of South America. These experiences opened my eyes to the beauty and diversity of our planet and instilled in me a profound appreciation for different perspectives. Upon returning home, I channeled my newfound insights into a career in international relations. I joined a non-governmental organization dedicated to promoting peace and understanding among nations. Through my work, I had the privilege of collaborating with passionate individuals from all walks of life, working tirelessly to make a positive impact on the world. Along the way, I met remarkable people who became lifelong friends and confidants. I discovered the power of human connection and the importance of nurturing meaningful relationships. Whether it was sharing laughter over a cup of coffee or engaging in deep conversations that challenged my perspectives, these connections enriched my life beyond measure. As I reflect on my journey thus far, I'm grateful for the experiences, both joyous and challenging, that have shaped me. I've learned the value of resilience, empathy, and the unwavering pursuit of my dreams. Though my path may have taken unexpected turns, I wouldn't trade it for anything. If you'd like to connect, feel free to reach me at nicholas_moore8047@yahoo.gov or +91-63720 22261. You can also find me at 35915 Patrick Mews Suite 978, where I'm always open to engaging in thought-provoking conversations and embarking on new adventures.\n",
      "'Nicholas Moore Southeast Asia  Europe South '\n",
      "'B-PER    I-PER B-LOC     I-LOC B-LOC  B-LOC '\n",
      "\n",
      "輸入: Hello, my name is Alexey Novikov and I'm a psychologist who solved a rather unique case recently. A few weeks ago, a woman named Sarah came to me with a peculiar problem. She was experiencing recurring nightmares that left her feeling anxious and exhausted. Upon further exploration, I discovered that Sarah had a fear of confined spaces, which she had developed after being trapped in an elevator for several hours as a child. To address Sarah's issue, I employed a combination of cognitive-behavioral therapy and exposure therapy. Through cognitive-behavioral therapy, we worked on identifying and challenging the negative thoughts and beliefs that were contributing to her fear. Simultaneously, we engaged in exposure therapy, where Sarah was gradually exposed to confined spaces in a controlled and supportive environment. As we progressed with the therapy, Sarah's fear of confined spaces began to diminish. She reported experiencing fewer nightmares and feeling more confident in everyday situations that involved enclosed areas. The transformation in Sarah's life was remarkable. She could now go to the grocery store without feeling overwhelmed, attend social events in smaller rooms, and even take public transportation during rush hour. I am grateful for the opportunity to have helped Sarah overcome her fear. It is incredibly rewarding to witness the positive impact that psychological interventions can have on individuals' lives. If you'd like to connect with me, you can reach me via email at alexey.novikov@msn.gov or by visiting my office located at 161 Creek Road. Thank you for reading!\n",
      "'Alexey Novikov Sarah Sarah Sarah Sarah Sarah Sarah Sarah Creek '\n",
      "'B-PER  I-PER   B-PER B-PER B-PER B-PER B-PER B-PER B-PER B-LOC '\n",
      "\n",
      "輸入: My name is Ludmila Inoue, and I'm a person with a story worth telling. From my humble beginnings at 706 Seagrove Road, I've embarked on a journey that has taken me to great heights. With a spirit of adventure and a determination to succeed, I've navigated life's twists and turns with resilience and grace. Growing up, I was fascinated by the world around me. Books were my companions, and knowledge became my currency. I spent countless hours exploring libraries and immersing myself in different subjects. My passion for learning fueled my desire to make a meaningful impact on the world. As I embarked on my professional journey, I discovered a knack for problem-solving and a talent for connecting with people. I found myself drawn to the dynamic world of business, where I could apply my skills and make a tangible difference. With unwavering dedication and a strong work ethic, I rose through the ranks, leaving a trail of success in my wake. Throughout my career, I've had the privilege of working with some of the brightest minds and most inspiring leaders. These collaborations have enriched my life and taught me invaluable lessons about teamwork, innovation, and the power of perseverance. I've learned that success is not a solo endeavor; it's a collective effort where everyone's contributions matter. My journey has not been without its challenges. I've faced obstacles and setbacks along the way, but I've always approached them as opportunities for growth. I believe that adversity is a catalyst for resilience, and it's in the toughest times that we discover our true strength and potential. As I reflect on my life, I'm filled with gratitude for the experiences that have shaped me into the person I am today. I'm grateful for the people who have supported me, challenged me, and inspired me to reach new heights. I'm grateful for the lessons I've learned, both the triumphs and the setbacks. If I could offer one piece of advice to those who are just starting their own journeys, it would be this: never give up on your dreams. No matter how big or small, your aspirations are valid. Embrace the challenges that come your way, and never stop learning and growing. The world needs your unique talents and perspectives, so share them with confidence. As I continue on this incredible journey, I remain committed to making a positive impact on the world. I'm excited about the possibilities that lie ahead and the opportunities to connect with new people and make a difference. If you'd like to reach out to me, feel free to contact me at (285) 815-7373 or ludmila_inoue@outlook.net. Let's collaborate and create something extraordinary together!\n",
      "'Ludmila Inoue Seagrove '\n",
      "'B-PER   I-PER B-LOC    '\n",
      "\n",
      "輸入: Dr. Tu Garcia, a renowned dermatologist, embarked on a project last year that aimed to raise awareness about skin cancer prevention and early detection. With a passion for educating the community about the importance of skin health, Dr. Garcia dedicated himself to this project, leveraging his expertise and various communication channels. Dr. Garcia began the project by establishing a dedicated webpage on his website, http://blog.tu-garcia.edu, where he published informative articles, videos, and interactive quizzes on skin cancer. He used his extensive knowledge to provide valuable insights into different types of skin cancer, risk factors, and the importance of regular skin self-examinations. To further engage with the community, Dr. Garcia organized several educational workshops and seminars at local community centers and schools. He delivered interactive presentations, using visual aids and real-life examples to emphasize the crucial role of sun protection and early detection in preventing skin cancer. He also facilitated Q&A sessions, allowing participants to clarify doubts and gain a deeper understanding of the subject. Understanding the significance of accessibility, Dr. Garcia made himself available for consultations and inquiries through various channels. He provided his phone number, +91-14426 83047, and email address, tugarcia@outlook.com, ensuring that individuals could reach him conveniently. He responded promptly to queries, offering personalized advice and guidance. Dr. Garcia's efforts were widely appreciated, and the project witnessed a remarkable response. The webpage garnered significant traffic, with individuals seeking information and resources on skin cancer. The workshops and seminars were attended by a diverse audience, including students, adults, and senior citizens, demonstrating the project's inclusivity and effectiveness. Through this project, Dr. Tu Garcia successfully raised awareness about skin cancer prevention and early detection. His dedication to educating the community and providing accessible resources made a positive impact, empowering individuals to take proactive steps towards maintaining healthy skin. The project serves as an exemplary model for dermatologists and healthcare professionals who strive to make a difference in promoting skin health.\n",
      "('Tu    Garcia Garcia Garcia .     Garcia Q     A     .     Garcia .     '\n",
      " 'Garcia Tu    ')\n",
      "('B-PER I-PER  B-PER  I-PER  B-PER I-PER  B-ORG B-ORG B-PER I-PER  B-PER '\n",
      " 'I-PER  B-PER ')\n",
      "\n",
      "輸入: Hello, I'm Badi Nakamura, and I work as a programmer. I'm based out of 2703 Woolsey Street, and you can reach me via email at badinakamura@gmail.org. One day at work, I encountered a puzzling problem. Our software was experiencing frequent crashes, and the error logs weren't providing any clear indications of the root cause. After a thorough investigation, I discovered that the crashes were being triggered by a specific sequence of user actions. This sequence involved a series of rapid button clicks that resulted in a race condition, causing the software to malfunction. To resolve the issue, I implemented a lock mechanism to ensure that only one user action could be processed at a time. This prevented the race condition from occurring and eliminated the crashes. Additionally, I optimized the code to improve its performance and stability. As a result of my efforts, the software's stability significantly improved, and the user experience was greatly enhanced. I was proud to have played a role in resolving this issue and contributing to the overall success of the project.\n",
      "'Badi  Nakamura Woolsey Street ina   ka    '\n",
      "'B-PER I-PER    B-LOC   I-LOC  B-PER B-ORG '\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 顯示預訓練模型預測結果，僅顯示有被標注的部分\n",
    "for val in dataset['test']: # 逐一取出測試資料\n",
    "  print(f'輸入: {val[\"text\"]}')\n",
    "  display_words_entity(val[\"text\"], classifier)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以留意到，預訓練模型已經具備一定的分類能力，但是由於我們的任務是 PII 偵測，所以我們需要進行 Fine-tuning 來提高模型的性能。同時，我們希望調整分類數，以符合我們的任務需求。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 了解分詞器 (Tokenizer) 行為\n",
    "\n",
    "首先，我們先試試看透過分詞器將一個句子轉換為 tokens。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'Aaliyah', 'Popova,', 'and', 'I', 'am', 'a', 'jeweler',\n",
      " 'with', '13', 'years', 'of', 'experience.', 'I', 'remember', 'a', 'very',\n",
      " 'unique', 'and', 'challenging', 'project', 'I', 'had', 'to', 'work', 'on',\n",
      " 'last', 'year.', 'A', 'customer', 'approached', 'me', 'with', 'a', 'precious',\n",
      " 'family', 'heirloom', '-', 'a', 'diamond', 'necklace', 'that', 'had', 'been',\n",
      " 'passed', 'down', 'through', 'generations.', 'Unfortunately,', 'the',\n",
      " 'necklace', 'was', 'in', 'poor', 'condition,', 'with', 'several', 'loose',\n",
      " 'diamonds', 'and', 'a', 'broken', 'clasp.', 'The', 'customer', 'wanted', 'me',\n",
      " 'to', 'restore', 'it', 'to', 'its', 'former', 'glory,', 'but', 'it', 'was',\n",
      " 'clear', 'that', 'this', 'would', 'be', 'no', 'ordinary', 'repair.', 'Using',\n",
      " 'my', 'specialized', 'tools', 'and', 'techniques,', 'I', 'began', 'the',\n",
      " 'delicate', 'task', 'of', 'dismantling', 'the', 'necklace.', 'Each', 'diamond',\n",
      " 'was', 'carefully', 'removed', 'from', 'its', 'setting,', 'and', 'the',\n",
      " 'damaged', 'clasp', 'was', 'removed.', 'Once', 'the', 'necklace', 'was',\n",
      " 'completely', 'disassembled,', 'I', 'meticulously', 'cleaned', 'each',\n",
      " 'diamond', 'and', 'inspected', 'it', 'for', 'any', 'damage.', 'Fortunately,',\n",
      " 'the', 'diamonds', 'were', 'all', 'in', 'good', 'condition,', 'with', 'no',\n",
      " 'cracks', 'or', 'chips.', 'The', 'next', 'step', 'was', 'to', 'repair', 'the',\n",
      " 'broken', 'clasp.', 'I', 'carefully', 'soldered', 'the', 'broken', 'pieces',\n",
      " 'back', 'together,', 'ensuring', 'that', 'the', 'clasp', 'was', 'sturdy',\n",
      " 'and', 'secure.', 'Once', 'the', 'clasp', 'was', 'repaired,', 'I', 'began',\n",
      " 'the', 'process', 'of', 'reassembling', 'the', 'necklace.', 'Each', 'diamond',\n",
      " 'was', 'carefully', 'placed', 'back', 'into', 'its', 'setting,', 'and', 'the',\n",
      " 'necklace', 'was', 'polished', 'until', 'it', 'sparkled', 'like', 'new.',\n",
      " 'When', 'I', 'presented', 'the', 'restored', 'necklace', 'to', 'the',\n",
      " 'customer,', 'they', 'were', 'overjoyed.', 'They', \"couldn't\", 'believe',\n",
      " 'that', 'I', 'had', 'been', 'able', 'to', 'bring', 'their', 'family',\n",
      " 'heirloom', 'back', 'to', 'life.', 'The', 'necklace', 'looked', 'as',\n",
      " 'beautiful', 'as', 'it', 'had', 'when', 'it', 'was', 'first', 'created,',\n",
      " 'and', 'the', 'customer', 'was', 'thrilled', 'to', 'have', 'it', 'back', 'in',\n",
      " 'their', 'possession.', 'If', 'you', 'have', 'a', 'project', 'that', 'you',\n",
      " 'would', 'like', 'to', 'discuss,', 'please', 'feel', 'free', 'to', 'contact',\n",
      " 'me', 'by', 'phone', 'at', '(95)', '94215-7906', 'or', 'by', 'email', 'at',\n",
      " 'aaliyah.popova4783@aol.edu.', 'I', 'look', 'forward', 'to', 'hearing', 'from',\n",
      " 'you!', 'P.S.:', 'When', \"I'm\", 'not', 'creating', 'beautiful', 'jewelry,',\n",
      " 'I', 'enjoy', 'spending', 'time', 'podcasting.', 'I', 'love', 'sharing', 'my',\n",
      " 'knowledge', 'about', 'jewelry', 'and', 'connecting', 'with', 'other',\n",
      " 'people', 'who', 'are', 'passionate', 'about', 'this', 'art', 'form.', 'I',\n",
      " 'also', 'enjoy', 'spending', 'time', 'with', 'my', 'family', 'and',\n",
      " 'exploring', 'new', 'places.', 'If', 'you', 'would', 'like', 'to', 'learn',\n",
      " 'more', 'about', 'me,', 'please', 'feel', 'free', 'to', 'visit', 'my',\n",
      " 'website', 'at', '[website', 'address]', 'or', 'visit', 'me', 'at', 'my',\n",
      " 'studio', 'located', 'at', '97', 'Lincoln', 'Street.']\n"
     ]
    }
   ],
   "source": [
    "input_labels = dataset['train'][0][\"labels\"]\n",
    "input_words = dataset['train'][0][\"words\"]\n",
    "# 輸入為將 'text' 切割後的字詞及字符\n",
    "pprint(input_words, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由於輸入已經預先分詞 (e.g., split into words)，所以我們將 `is_split_into_words` 設為 `True`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1],\n",
      " 'input_ids': [101, 1422, 1271, 1110, 138, 10584, 18713, 7312, 8625, 117, 1105,\n",
      "               146, 1821, 170, 23160, 1200, 1114, 1492, 1201, 1104, 2541, 119,\n",
      "               146, 2676, 170, 1304, 3527, 1105, 10467, 1933, 146, 1125, 1106,\n",
      "               1250, 1113, 1314, 1214, 119, 138, 8132, 4685, 1143, 1114, 170,\n",
      "               9692, 1266, 8048, 13853, 118, 170, 9883, 13828, 1115, 1125, 1151,\n",
      "               2085, 1205, 1194, 8225, 119, 7595, 117, 1103, 13828, 1108, 1107,\n",
      "               2869, 3879, 117, 1114, 1317, 5768, 16031, 1105, 170, 3088, 172,\n",
      "               25552, 119, 1109, 8132, 1458, 1143, 1106, 9176, 1122, 1106, 1157,\n",
      "               1393, 12887, 117, 1133, 1122, 1108, 2330, 1115, 1142, 1156, 1129,\n",
      "               1185, 6655, 6949, 119, 7993, 1139, 7623, 5537, 1105, 4884, 117,\n",
      "               146, 1310, 1103, 10141, 4579, 1104, 4267, 8878, 13756, 1103,\n",
      "               13828, 119, 2994, 9883, 1108, 4727, 2856, 1121, 1157, 3545, 117,\n",
      "               1105, 1103, 4938, 172, 25552, 1108, 2856, 119, 2857, 1103, 13828,\n",
      "               1108, 2423, 4267, 3202, 11553, 16465, 1181, 117, 146, 1899, 1596,\n",
      "               23268, 12370, 1296, 9883, 1105, 22457, 1122, 1111, 1251, 3290,\n",
      "               119, 18101, 117, 1103, 16031, 1127, 1155, 1107, 1363, 3879, 117,\n",
      "               1114, 1185, 16694, 1137, 13228, 119, 1109, 1397, 2585, 1108,\n",
      "               1106, 6949, 1103, 3088, 172, 25552, 119, 146, 4727, 1962, 5686,\n",
      "               1103, 3088, 3423, 1171, 1487, 117, 13291, 1115, 1103, 172, 25552,\n",
      "               1108, 26887, 1105, 5343, 119, 2857, 1103, 172, 25552, 1108,\n",
      "               14129, 117, 146, 1310, 1103, 1965, 1104, 1231, 11192, 5521, 6647,\n",
      "               1103, 13828, 119, 2994, 9883, 1108, 4727, 1973, 1171, 1154, 1157,\n",
      "               3545, 117, 1105, 1103, 13828, 1108, 13247, 1235, 1122, 14798,\n",
      "               2433, 1176, 1207, 119, 1332, 146, 2756, 1103, 5219, 13828, 1106,\n",
      "               1103, 8132, 117, 1152, 1127, 1166, 18734, 1174, 119, 1220, 1577,\n",
      "               112, 189, 2059, 1115, 146, 1125, 1151, 1682, 1106, 2498, 1147,\n",
      "               1266, 8048, 13853, 1171, 1106, 1297, 119, 1109, 13828, 1350,\n",
      "               1112, 2712, 1112, 1122, 1125, 1165, 1122, 1108, 1148, 1687, 117,\n",
      "               1105, 1103, 8132, 1108, 17278, 1106, 1138, 1122, 1171, 1107,\n",
      "               1147, 6224, 119, 1409, 1128, 1138, 170, 1933, 1115, 1128, 1156,\n",
      "               1176, 1106, 6265, 117, 4268, 1631, 1714, 1106, 3232, 1143, 1118,\n",
      "               2179, 1120, 113, 4573, 114, 5706, 18202, 1571, 118, 5899, 1568,\n",
      "               1545, 1137, 1118, 10632, 1120, 170, 10584, 18713, 119, 3618,\n",
      "               8625, 24766, 1604, 1495, 137, 170, 4063, 119, 5048, 1358, 119,\n",
      "               146, 1440, 1977, 1106, 4510, 1121, 1128, 106, 153, 119, 156, 119,\n",
      "               131, 1332, 146, 112, 182, 1136, 3780, 2712, 12731, 117, 146,\n",
      "               5548, 5369, 1159, 19777, 1158, 119, 146, 1567, 6303, 1139, 3044,\n",
      "               1164, 12731, 1105, 6755, 1114, 1168, 1234, 1150, 1132, 14472,\n",
      "               1164, 1142, 1893, 1532, 119, 146, 1145, 5548, 5369, 1159, 1114,\n",
      "               1139, 1266, 1105, 12138, 1207, 2844, 119, 1409, 1128, 1156, 1176,\n",
      "               1106, 3858, 1167, 1164, 1143, 117, 4268, 1631, 1714, 1106, 3143,\n",
      "               1139, 3265, 1120, 164, 3265, 4134, 166, 1137, 3143, 1143, 1120,\n",
      "               1139, 2362, 1388, 1120, 5311, 4617, 1715, 119, 102]}\n"
     ]
    }
   ],
   "source": [
    "# 對 input_words 進行分詞\n",
    "input_tokenized_ids = tokenizer(\n",
    "  input_words,\n",
    "  truncation=True, # 是否截斷序列\n",
    "  # is_split_into_words: Whether or not the input is already pre-tokenized (e.g., split into words).\n",
    "  # If set to True, the tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace) which it will tokenize.\n",
    "  # This is useful for NER or token classification.\n",
    "  is_split_into_words=True)\n",
    "pprint(input_tokenized_ids, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "經過分詞器後將得到一個包含 `attention_mask` 及 `input_ids` 的字典，其中 `attention_mask` 用於指示哪些 tokens 是模型需要關注，哪些是 padding tokens 可以忽略，`input_ids` 則是 tokens 的索引或對應到辭典 (vocab) 的 ID。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 還原編碼後的文字\n",
    "\n",
    "我們亦可以透過 `tokens()` 方法將 `input_ids` 轉換回 tokens。\n",
    "\n",
    "分詞器添加了模型使用的特殊標記（[CLS] 在開頭和 [SEP] 在結尾），並且大多數單詞保持不變。然而，某些單詞 (word) 會被分為數個子詞 (subword)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'My', 'name', 'is', 'A', '##ali', '##yah', 'Pop', '##ova', ',', 'and',\n",
      " 'I', 'am', 'a', 'jewel', '##er', 'with', '13', 'years', 'of', 'experience',\n",
      " '.', 'I', 'remember', 'a', 'very', 'unique', 'and', 'challenging', 'project',\n",
      " 'I', 'had', 'to', 'work', 'on', 'last', 'year', '.', 'A', 'customer',\n",
      " 'approached', 'me', 'with', 'a', 'precious', 'family', 'heir', '##loom', '-',\n",
      " 'a', 'diamond', 'necklace', 'that', 'had', 'been', 'passed', 'down', 'through',\n",
      " 'generations', '.', 'Unfortunately', ',', 'the', 'necklace', 'was', 'in',\n",
      " 'poor', 'condition', ',', 'with', 'several', 'loose', 'diamonds', 'and', 'a',\n",
      " 'broken', 'c', '##lasp', '.', 'The', 'customer', 'wanted', 'me', 'to',\n",
      " 'restore', 'it', 'to', 'its', 'former', 'glory', ',', 'but', 'it', 'was',\n",
      " 'clear', 'that', 'this', 'would', 'be', 'no', 'ordinary', 'repair', '.',\n",
      " 'Using', 'my', 'specialized', 'tools', 'and', 'techniques', ',', 'I', 'began',\n",
      " 'the', 'delicate', 'task', 'of', 'di', '##sman', '##tling', 'the', 'necklace',\n",
      " '.', 'Each', 'diamond', 'was', 'carefully', 'removed', 'from', 'its',\n",
      " 'setting', ',', 'and', 'the', 'damaged', 'c', '##lasp', 'was', 'removed', '.',\n",
      " 'Once', 'the', 'necklace', 'was', 'completely', 'di', '##sa', '##sse',\n",
      " '##mble', '##d', ',', 'I', 'met', '##ic', '##ulously', 'cleaned', 'each',\n",
      " 'diamond', 'and', 'inspected', 'it', 'for', 'any', 'damage', '.',\n",
      " 'Fortunately', ',', 'the', 'diamonds', 'were', 'all', 'in', 'good',\n",
      " 'condition', ',', 'with', 'no', 'cracks', 'or', 'chips', '.', 'The', 'next',\n",
      " 'step', 'was', 'to', 'repair', 'the', 'broken', 'c', '##lasp', '.', 'I',\n",
      " 'carefully', 'sold', '##ered', 'the', 'broken', 'pieces', 'back', 'together',\n",
      " ',', 'ensuring', 'that', 'the', 'c', '##lasp', 'was', 'sturdy', 'and',\n",
      " 'secure', '.', 'Once', 'the', 'c', '##lasp', 'was', 'repaired', ',', 'I',\n",
      " 'began', 'the', 'process', 'of', 're', '##ass', '##em', '##bling', 'the',\n",
      " 'necklace', '.', 'Each', 'diamond', 'was', 'carefully', 'placed', 'back',\n",
      " 'into', 'its', 'setting', ',', 'and', 'the', 'necklace', 'was', 'polished',\n",
      " 'until', 'it', 'spark', '##led', 'like', 'new', '.', 'When', 'I', 'presented',\n",
      " 'the', 'restored', 'necklace', 'to', 'the', 'customer', ',', 'they', 'were',\n",
      " 'over', '##joy', '##ed', '.', 'They', 'couldn', \"'\", 't', 'believe', 'that',\n",
      " 'I', 'had', 'been', 'able', 'to', 'bring', 'their', 'family', 'heir', '##loom',\n",
      " 'back', 'to', 'life', '.', 'The', 'necklace', 'looked', 'as', 'beautiful',\n",
      " 'as', 'it', 'had', 'when', 'it', 'was', 'first', 'created', ',', 'and', 'the',\n",
      " 'customer', 'was', 'thrilled', 'to', 'have', 'it', 'back', 'in', 'their',\n",
      " 'possession', '.', 'If', 'you', 'have', 'a', 'project', 'that', 'you', 'would',\n",
      " 'like', 'to', 'discuss', ',', 'please', 'feel', 'free', 'to', 'contact', 'me',\n",
      " 'by', 'phone', 'at', '(', '95', ')', '94', '##21', '##5', '-', '79', '##0',\n",
      " '##6', 'or', 'by', 'email', 'at', 'a', '##ali', '##yah', '.', 'pop', '##ova',\n",
      " '##47', '##8', '##3', '@', 'a', '##ol', '.', 'ed', '##u', '.', 'I', 'look',\n",
      " 'forward', 'to', 'hearing', 'from', 'you', '!', 'P', '.', 'S', '.', ':',\n",
      " 'When', 'I', \"'\", 'm', 'not', 'creating', 'beautiful', 'jewelry', ',', 'I',\n",
      " 'enjoy', 'spending', 'time', 'podcast', '##ing', '.', 'I', 'love', 'sharing',\n",
      " 'my', 'knowledge', 'about', 'jewelry', 'and', 'connecting', 'with', 'other',\n",
      " 'people', 'who', 'are', 'passionate', 'about', 'this', 'art', 'form', '.', 'I',\n",
      " 'also', 'enjoy', 'spending', 'time', 'with', 'my', 'family', 'and',\n",
      " 'exploring', 'new', 'places', '.', 'If', 'you', 'would', 'like', 'to', 'learn',\n",
      " 'more', 'about', 'me', ',', 'please', 'feel', 'free', 'to', 'visit', 'my',\n",
      " 'website', 'at', '[', 'website', 'address', ']', 'or', 'visit', 'me', 'at',\n",
      " 'my', 'studio', 'located', 'at', '97', 'Lincoln', 'Street', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# 如我們所見，分詞器添加了模型使用的特殊標記（[CLS] 在開頭和 [SEP] 在結尾），\n",
    "# 並且大多數單詞保持不變。然而，某些單詞 (word) 會被分為數個子詞 (subword)，如: Bar, ##eil 和 ##ly\n",
    "pprint(input_tokenized_ids.tokens(), compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "於是，經過分詞器後，這導致了我們的輸入和標籤之間的不匹配，從兩者長度便可窺知一二。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of input_words: 363\n",
      "length of input_labels: 363\n",
      "length of token id: 463\n"
     ]
    }
   ],
   "source": [
    "# 原始資料\n",
    "print(f'length of input_words: {len(input_words)}')\n",
    "print(f'length of input_labels: {len(input_labels)}')\n",
    "# 這導致了我們的輸入和標籤之間的不匹配\n",
    "print(f'length of token id: {len(input_tokenized_ids.tokens())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 還原編碼後的位置\n",
    "\n",
    "感謝 fast tokenizer 我們可以輕鬆地將每個 Token 藉由 `word_ids()` 映射到其對應的單詞位置或 Word IDs。\n",
    "\n",
    "舉例：\n",
    "\n",
    "| Word    | Token | Word ID |\n",
    "|---------|-------|---------|\n",
    "|         | [CLS] | None    |\n",
    "| My      | My    | 0       |\n",
    "| name    | name  | 1       |\n",
    "| is      | is    | 2       |\n",
    "| Ludmila | Lu    | 3       |\n",
    "|         | ##d   | 3       |\n",
    "|         | ##mi  | 3       |\n",
    "|         | ##la  | 3       |\n",
    "|         | [SEP] | None    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 2, 3, 3, 3, 4, 4, 4, 5, 6, 7, 8, 9, 9, 10, 11, 12, 13, 14, 14, 15,\n",
      " 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 29, 30, 31, 32, 33, 34,\n",
      " 35, 36, 37, 38, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 49, 50, 50, 51,\n",
      " 52, 53, 54, 55, 56, 56, 57, 58, 59, 60, 61, 62, 63, 64, 64, 64, 65, 66, 67, 68,\n",
      " 69, 70, 71, 72, 73, 74, 75, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 86,\n",
      " 87, 88, 89, 90, 91, 92, 92, 93, 94, 95, 96, 97, 98, 99, 99, 99, 100, 101, 101,\n",
      " 102, 103, 104, 105, 106, 107, 108, 109, 109, 110, 111, 112, 113, 113, 114, 115,\n",
      " 115, 116, 117, 118, 119, 120, 121, 121, 121, 121, 121, 121, 122, 123, 123, 123,\n",
      " 124, 125, 126, 127, 128, 129, 130, 131, 132, 132, 133, 133, 134, 135, 136, 137,\n",
      " 138, 139, 140, 140, 141, 142, 143, 144, 145, 145, 146, 147, 148, 149, 150, 151,\n",
      " 152, 153, 154, 154, 154, 155, 156, 157, 157, 158, 159, 160, 161, 162, 162, 163,\n",
      " 164, 165, 166, 166, 167, 168, 169, 170, 170, 171, 172, 173, 173, 174, 175, 175,\n",
      " 176, 177, 178, 179, 180, 181, 181, 181, 181, 182, 183, 183, 184, 185, 186, 187,\n",
      " 188, 189, 190, 191, 192, 192, 193, 194, 195, 196, 197, 198, 199, 200, 200, 201,\n",
      " 202, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 211, 212, 213, 214, 214,\n",
      " 214, 214, 215, 216, 216, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226,\n",
      " 227, 227, 228, 229, 230, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240,\n",
      " 241, 242, 243, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255,\n",
      " 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 266, 267, 268, 269,\n",
      " 270, 271, 272, 273, 274, 275, 276, 276, 276, 277, 277, 277, 277, 277, 277, 277,\n",
      " 278, 279, 280, 281, 282, 282, 282, 282, 282, 282, 282, 282, 282, 282, 282, 282,\n",
      " 282, 282, 282, 282, 283, 284, 285, 286, 287, 288, 289, 289, 290, 290, 290, 290,\n",
      " 290, 291, 292, 292, 292, 293, 294, 295, 296, 296, 297, 298, 299, 300, 301, 301,\n",
      " 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316,\n",
      " 317, 318, 319, 320, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331,\n",
      " 332, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 341, 342, 343, 344, 345,\n",
      " 346, 347, 348, 349, 350, 350, 351, 351, 352, 353, 354, 355, 356, 357, 358, 359,\n",
      " 360, 361, 362, 362, None]\n"
     ]
    }
   ],
   "source": [
    "# `word_ids` return the list of tokens (sub-parts of the input strings after word/subword splitting and before conversion to integer indices)\n",
    "# at a given batch index (only works for the output of a fast tokenizer).\n",
    "pprint(input_tokenized_ids.word_ids(), compact=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CLS] 和 [SEP] 並不對應到任何單詞，所以它們的 Word ID 為 None。其他則是以 0 開始的 Word ID，如遇到子詞 (subword) 則會共用相同的 Word ID。\n",
    "\n",
    "根據上述的了解，我們依序將輸入，編碼後的文字，以及編碼後的位置對比，幫助我們理解分詞器的行為。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tokenizer 前 (Words 及 Labels) ===\n",
      "('My name is Aaliyah        Popova,        and I am a jeweler with 13 years of experience. I remember a very unique and challenging project I had to work on last year. A customer approached me with '\n",
      " 'a precious family heirloom - a diamond necklace that had been passed down through generations. ')\n",
      "('O  O    O  B-NAME_STUDENT I-NAME_STUDENT O   O O  O O       O    O  O     O  O           O O        O O    O      O   O           O       O O   O  O    O  O    O     O O        O          O  O    '\n",
      " 'O O        O      O        O O O       O        O    O   O    O      O    O       O            ')\n",
      "\n",
      "=== Tokenizer 後 (還原編碼後的文字) ===\n",
      "['[CLS]', 'My', 'name', 'is', 'A', '##ali', '##yah', 'Pop', '##ova', ',', 'and',\n",
      " 'I', 'am', 'a', 'jewel', '##er', 'with', '13', 'years', 'of', 'experience',\n",
      " '.', 'I', 'remember', 'a', 'very', 'unique', 'and', 'challenging', 'project',\n",
      " 'I', 'had', 'to', 'work', 'on', 'last', 'year', '.', 'A', 'customer',\n",
      " 'approached', 'me', 'with', 'a', 'precious', 'family', 'heir', '##loom', '-',\n",
      " 'a']\n",
      "\n",
      "=== Tokenizer 後 (還原編碼後的位置) ===\n",
      "[None, 0, 1, 2, 3, 3, 3, 4, 4, 4, 5, 6, 7, 8, 9, 9, 10, 11, 12, 13, 14, 14, 15,\n",
      " 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 29, 30, 31, 32, 33, 34,\n",
      " 35, 36, 37, 38, 38, 39, 40]\n"
     ]
    }
   ],
   "source": [
    "# 資料對比\n",
    "print('=== Tokenizer 前 (Words 及 Labels) ===')\n",
    "display_words_labels(dataset['train'][0], max_display)\n",
    "\n",
    "print()\n",
    "print('=== Tokenizer 後 (還原編碼後的文字) ===')\n",
    "pprint(input_tokenized_ids.tokens()[:max_display], compact=True)\n",
    "\n",
    "print()\n",
    "print('=== Tokenizer 後 (還原編碼後的位置) ===')\n",
    "pprint(input_tokenized_ids.word_ids()[:max_display], compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料預處理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "metadata": {}
   },
   "source": [
    "#### 重新校準標籤\n",
    "\n",
    "我們可以搭配 `word_id` 擴展標籤 (Label) 以匹配單詞 Token ID，將標籤與分詞後的標記對齊。\n",
    "\n",
    "1. 首先，我們將應用的規則是特殊 Token 獲得 -100 標籤。這是因為默認情況下，-100 是我們在損失函數中被忽略的索引。\n",
    "2. 然後，每個相同 Word ID 的 Token 獲得與其所在單詞相同的標籤，因為它們是同一實體的一部分。\n",
    "\n",
    "定義一個函數接受兩個參數：`word_ids` 和 `labels`，並返回重新校準後的標籤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(\n",
    "        word_ids: list,\n",
    "        labels: list,) -> list:\n",
    "    new_labels = [] # 用於存儲對齊後的新標籤\n",
    "    current_word_id = None # 用於追踪當前的單詞 ID\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None: # None 代表特殊 Token\n",
    "            label = -100 # 特殊 Token 獲得 -100 的標籤\n",
    "        elif word_id != current_word_id: # Word ID 改變，代表新的 Token\n",
    "            current_word_id = word_id # 更新 Word ID\n",
    "            label = -100 if word_id is None else config.tag2id.get(labels[word_id]) # 取得對應的標籤\n",
    "        else: # 與前一個 Token 相同的字\n",
    "            label = config.tag2id.get(labels[word_id])\n",
    "        new_labels.append(label) # 附加新的標籤\n",
    "    # 返回新的標籤\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再來檢視一次原先 `input_labels` 中的標籤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 9, 9, 1, 6, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      " 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      " 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      " 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      " 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      " 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      " 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      " 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      " 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      " 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      " 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 2, 7, 9, 9, 9, 9, 0, 9, 9, 9,\n",
      " 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      " 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      " 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 3, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "# 先將其轉換為標籤編碼\n",
    "input_label_ids = [config.tag2id.get(label) for label in input_labels]\n",
    "pprint(input_label_ids, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "經過重新校準後，我們可以看到標籤已經與分詞後的標記對齊，且長度相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 9, 9, 9, 1, 1, 1, 6, 6, 6, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      " 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      " 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      " 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      " 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      " 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      " 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      " 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      " 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      " 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      " 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      " 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      " 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 2,\n",
      " 2, 2, 7, 7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      " 0, 0, 0, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      " 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      " 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      " 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 3, 8, 8, 8, -100]\n"
     ]
    }
   ],
   "source": [
    "input_label_ids_aligned = align_labels_with_tokens(\n",
    "  input_tokenized_ids.word_ids(), # 透過 word_ids 取得對應的 word id\n",
    "  input_labels, # 原始標籤\n",
    ")\n",
    "pprint(input_label_ids_aligned, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of token id: 463\n",
      "length of labels: 463\n"
     ]
    }
   ],
   "source": [
    "# 顯示對齊後的標籤，兩者數量應該相同\n",
    "print(f'length of token id: {len(input_tokenized_ids.tokens())}')\n",
    "print(f'length of labels: {len(input_label_ids_aligned)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "經過重新校準後，我們的標籤與 Token 一一對應。\n",
    "\n",
    "| Word    | Token | Word ID | Label          |\n",
    "|---------|-------|---------|----------------|\n",
    "|         | [CLS] | None    | -              |\n",
    "| My      | My    | 0       | O              |\n",
    "| name    | name  | 1       | O              |\n",
    "| is      | is    | 2       | O              |\n",
    "| Ludmila | Lu    | 3       | B-NAME_STUDENT |\n",
    "|         | ##d   | 3       | B-NAME_STUDENT |\n",
    "|         | ##mi  | 3       | B-NAME_STUDENT |\n",
    "|         | ##la  | 3       | B-NAME_STUDENT |\n",
    "|         | [SEP] | None    | -              |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[CLS] My name is A              ##ali          ##yah          Pop            ##ova          ,              and I am a jewel ##er with 13 years of experience . I remember a very unique and '\n",
      " 'challenging project I had to work on last year . A customer approached me with a precious family heir ##loom - a ')\n",
      "('-     O  O    O  B-NAME_STUDENT B-NAME_STUDENT B-NAME_STUDENT I-NAME_STUDENT I-NAME_STUDENT I-NAME_STUDENT O   O O  O O     O    O    O  O     O  O          O O O        O O    O      O   '\n",
      " 'O           O       O O   O  O    O  O    O    O O O        O          O  O    O O        O      O    O      O O ')\n"
     ]
    }
   ],
   "source": [
    "def display_tokens_labels(tokens, label_ids):\n",
    "    line1 = \"\"\n",
    "    line2 = \"\"\n",
    "    for token, label_id in zip(tokens, label_ids): # 逐一取出 token 與 label_id\n",
    "        # 將整數標籤轉換為 BIO 標籤方便比較\n",
    "        label = config.id2tag.get(label_id, '-')\n",
    "        # 計算 token 與 label 的最大長度, 並將 token 與 label 用空白補齊至相同長度\n",
    "        max_length = max(len(token), len(label))\n",
    "        line1 += token + \" \" * (max_length - len(token) + 1)\n",
    "        line2 += label + \" \" * (max_length - len(label) + 1)\n",
    "    pprint(line1, width=200)\n",
    "    pprint(line2, width=200)\n",
    "\n",
    "# 顯示對齊後的 tokens 與 labels\n",
    "display_tokens_labels(\n",
    "    input_tokenized_ids.tokens()[:max_display],\n",
    "    input_label_ids_aligned[:max_display])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定義預處理函數\n",
    "\n",
    "要預處理整個數據集，我們需要對所有輸入進行分詞，並對所有標籤應用 `align_labels_with_tokens()`。為了利用快速分詞器的速度，最好一次分詞大量文本，因此我們將編寫一個批次處理函數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def preprocess_function(dataset):\n",
    "    # 使用分詞器對資料集進行分詞\n",
    "    tokenized_inputs = tokenizer(\n",
    "        dataset['words'],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "    labels = dataset['labels']\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(labels):\n",
    "        # 取得對應的 word_ids\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        # 對齊標籤\n",
    "        new_labels.append(\n",
    "            align_labels_with_tokens(word_ids, labels)\n",
    "        )\n",
    "    # 更新 labels 欄位，請留意這邊的 labels 欄位是整數標籤，而非 BIO 標籤\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokenized_ids = preprocess_function(dataset['train'][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                     1, 1, 1]],\n",
      " 'input_ids': [[101, 1422, 1271, 1110, 138, 10584, 18713, 7312, 8625, 117, 1105,\n",
      "                146, 1821, 170, 23160, 1200, 1114, 1492, 1201, 1104, 2541, 119,\n",
      "                146, 2676, 170, 1304, 3527, 1105, 10467, 1933, 146, 1125, 1106,\n",
      "                1250, 1113, 1314, 1214, 119, 138, 8132, 4685, 1143, 1114, 170,\n",
      "                9692, 1266, 8048, 13853, 118, 170, 9883, 13828, 1115, 1125,\n",
      "                1151, 2085, 1205, 1194, 8225, 119, 7595, 117, 1103, 13828, 1108,\n",
      "                1107, 2869, 3879, 117, 1114, 1317, 5768, 16031, 1105, 170, 3088,\n",
      "                172, 25552, 119, 1109, 8132, 1458, 1143, 1106, 9176, 1122, 1106,\n",
      "                1157, 1393, 12887, 117, 1133, 1122, 1108, 2330, 1115, 1142,\n",
      "                1156, 1129, 1185, 6655, 6949, 119, 7993, 1139, 7623, 5537, 1105,\n",
      "                4884, 117, 146, 1310, 1103, 10141, 4579, 1104, 4267, 8878,\n",
      "                13756, 1103, 13828, 119, 2994, 9883, 1108, 4727, 2856, 1121,\n",
      "                1157, 3545, 117, 1105, 1103, 4938, 172, 25552, 1108, 2856, 119,\n",
      "                2857, 1103, 13828, 1108, 2423, 4267, 3202, 11553, 16465, 1181,\n",
      "                117, 146, 1899, 1596, 23268, 12370, 1296, 9883, 1105, 22457,\n",
      "                1122, 1111, 1251, 3290, 119, 18101, 117, 1103, 16031, 1127,\n",
      "                1155, 1107, 1363, 3879, 117, 1114, 1185, 16694, 1137, 13228,\n",
      "                119, 1109, 1397, 2585, 1108, 1106, 6949, 1103, 3088, 172, 25552,\n",
      "                119, 146, 4727, 1962, 5686, 1103, 3088, 3423, 1171, 1487, 117,\n",
      "                13291, 1115, 1103, 172, 25552, 1108, 26887, 1105, 5343, 119,\n",
      "                2857, 1103, 172, 25552, 1108, 14129, 117, 146, 1310, 1103, 1965,\n",
      "                1104, 1231, 11192, 5521, 6647, 1103, 13828, 119, 2994, 9883,\n",
      "                1108, 4727, 1973, 1171, 1154, 1157, 3545, 117, 1105, 1103,\n",
      "                13828, 1108, 13247, 1235, 1122, 14798, 2433, 1176, 1207, 119,\n",
      "                1332, 146, 2756, 1103, 5219, 13828, 1106, 1103, 8132, 117, 1152,\n",
      "                1127, 1166, 18734, 1174, 119, 1220, 1577, 112, 189, 2059, 1115,\n",
      "                146, 1125, 1151, 1682, 1106, 2498, 1147, 1266, 8048, 13853,\n",
      "                1171, 1106, 1297, 119, 1109, 13828, 1350, 1112, 2712, 1112,\n",
      "                1122, 1125, 1165, 1122, 1108, 1148, 1687, 117, 1105, 1103, 8132,\n",
      "                1108, 17278, 1106, 1138, 1122, 1171, 1107, 1147, 6224, 119,\n",
      "                1409, 1128, 1138, 170, 1933, 1115, 1128, 1156, 1176, 1106, 6265,\n",
      "                117, 4268, 1631, 1714, 1106, 3232, 1143, 1118, 2179, 1120, 113,\n",
      "                4573, 114, 5706, 18202, 1571, 118, 5899, 1568, 1545, 1137, 1118,\n",
      "                10632, 1120, 170, 10584, 18713, 119, 3618, 8625, 24766, 1604,\n",
      "                1495, 137, 170, 4063, 119, 5048, 1358, 119, 146, 1440, 1977,\n",
      "                1106, 4510, 1121, 1128, 106, 153, 119, 156, 119, 131, 1332, 146,\n",
      "                112, 182, 1136, 3780, 2712, 12731, 117, 146, 5548, 5369, 1159,\n",
      "                19777, 1158, 119, 146, 1567, 6303, 1139, 3044, 1164, 12731,\n",
      "                1105, 6755, 1114, 1168, 1234, 1150, 1132, 14472, 1164, 1142,\n",
      "                1893, 1532, 119, 146, 1145, 5548, 5369, 1159, 1114, 1139, 1266,\n",
      "                1105, 12138, 1207, 2844, 119, 1409, 1128, 1156, 1176, 1106,\n",
      "                3858, 1167, 1164, 1143, 117, 4268, 1631, 1714, 1106, 3143, 1139,\n",
      "                3265, 1120, 164, 3265, 4134, 166, 1137, 3143, 1143, 1120, 1139,\n",
      "                2362, 1388, 1120, 5311, 4617, 1715, 119, 102]],\n",
      " 'labels': [[-100, 9, 9, 9, 1, 1, 1, 6, 6, 6, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "             9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "             9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "             9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "             9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "             9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "             9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "             9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "             9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "             9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "             9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "             9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "             9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "             9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "             9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "             9, 9, 9, 9, 9, 9, 9, 2, 2, 2, 7, 7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 0,\n",
      "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 9, 9, 9, 9, 9, 9,\n",
      "             9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "             9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "             9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "             9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 3, 8, 8,\n",
      "             8, -100]]}\n"
     ]
    }
   ],
   "source": [
    "pprint(input_tokenized_ids, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "經過 `preprocess_function` 後將得到一組先前我們使用分詞器的 `attention_mask` 及 `input_ids`，也會增加一個對齊後的 `labels`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[CLS] My name is A              ##ali          ##yah          Pop            ##ova          ,              and I am a jewel ##er with 13 years of experience . I remember a very unique and '\n",
      " 'challenging project I had to work on last year . A customer approached me with a precious family heir ##loom - a ')\n",
      "('-     O  O    O  B-NAME_STUDENT B-NAME_STUDENT B-NAME_STUDENT I-NAME_STUDENT I-NAME_STUDENT I-NAME_STUDENT O   O O  O O     O    O    O  O     O  O          O O O        O O    O      O   '\n",
      " 'O           O       O O   O  O    O  O    O    O O O        O          O  O    O O        O      O    O      O O ')\n"
     ]
    }
   ],
   "source": [
    "# 顯示對齊後的 tokens 與 labels\n",
    "display_tokens_labels(\n",
    "    input_tokenized_ids.tokens()[:max_display],\n",
    "    input_tokenized_ids['labels'][0][:max_display])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請留意經過資料預處理後，`labels` 欄位是整數標籤，而不是 BIO 字串標籤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 批次處理資料\n",
    "\n",
    "使用 `Dataset.map()` 方法，選項設置為 `batched=True`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonas/.pyenv/versions/learning-transformer-with-huggingface-3.11.11/lib/python3.11/site-packages/dill/_dill.py:414: PicklingWarning: Cannot locate reference to <class '__main__.Config'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "/Users/jonas/.pyenv/versions/learning-transformer-with-huggingface-3.11.11/lib/python3.11/site-packages/dill/_dill.py:414: PicklingWarning: Cannot pickle <class '__main__.Config'>: __main__.Config has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3543/3543 [00:04<00:00, 743.67 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 886/886 [00:01<00:00, 750.29 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 568.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function, # 對資料集進行分詞與標籤對齊\n",
    "    batched=True, # 是否以批次進行處理\n",
    "    remove_columns=dataset['train'].column_names, # 移除不必要的欄位\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "labels",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "input_ids",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "attention_mask",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "ddd98bb9-a7df-4723-bbf9-d688628da8b6",
       "rows": [
        [
         "0",
         "[-100, 9, 9, 9, 1, 1, 1, 6, 6, 6, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 2, 2, 2, 7, 7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 3, 8, 8, 8, -100]",
         "[101, 1422, 1271, 1110, 138, 10584, 18713, 7312, 8625, 117, 1105, 146, 1821, 170, 23160, 1200, 1114, 1492, 1201, 1104, 2541, 119, 146, 2676, 170, 1304, 3527, 1105, 10467, 1933, 146, 1125, 1106, 1250, 1113, 1314, 1214, 119, 138, 8132, 4685, 1143, 1114, 170, 9692, 1266, 8048, 13853, 118, 170, 9883, 13828, 1115, 1125, 1151, 2085, 1205, 1194, 8225, 119, 7595, 117, 1103, 13828, 1108, 1107, 2869, 3879, 117, 1114, 1317, 5768, 16031, 1105, 170, 3088, 172, 25552, 119, 1109, 8132, 1458, 1143, 1106, 9176, 1122, 1106, 1157, 1393, 12887, 117, 1133, 1122, 1108, 2330, 1115, 1142, 1156, 1129, 1185, 6655, 6949, 119, 7993, 1139, 7623, 5537, 1105, 4884, 117, 146, 1310, 1103, 10141, 4579, 1104, 4267, 8878, 13756, 1103, 13828, 119, 2994, 9883, 1108, 4727, 2856, 1121, 1157, 3545, 117, 1105, 1103, 4938, 172, 25552, 1108, 2856, 119, 2857, 1103, 13828, 1108, 2423, 4267, 3202, 11553, 16465, 1181, 117, 146, 1899, 1596, 23268, 12370, 1296, 9883, 1105, 22457, 1122, 1111, 1251, 3290, 119, 18101, 117, 1103, 16031, 1127, 1155, 1107, 1363, 3879, 117, 1114, 1185, 16694, 1137, 13228, 119, 1109, 1397, 2585, 1108, 1106, 6949, 1103, 3088, 172, 25552, 119, 146, 4727, 1962, 5686, 1103, 3088, 3423, 1171, 1487, 117, 13291, 1115, 1103, 172, 25552, 1108, 26887, 1105, 5343, 119, 2857, 1103, 172, 25552, 1108, 14129, 117, 146, 1310, 1103, 1965, 1104, 1231, 11192, 5521, 6647, 1103, 13828, 119, 2994, 9883, 1108, 4727, 1973, 1171, 1154, 1157, 3545, 117, 1105, 1103, 13828, 1108, 13247, 1235, 1122, 14798, 2433, 1176, 1207, 119, 1332, 146, 2756, 1103, 5219, 13828, 1106, 1103, 8132, 117, 1152, 1127, 1166, 18734, 1174, 119, 1220, 1577, 112, 189, 2059, 1115, 146, 1125, 1151, 1682, 1106, 2498, 1147, 1266, 8048, 13853, 1171, 1106, 1297, 119, 1109, 13828, 1350, 1112, 2712, 1112, 1122, 1125, 1165, 1122, 1108, 1148, 1687, 117, 1105, 1103, 8132, 1108, 17278, 1106, 1138, 1122, 1171, 1107, 1147, 6224, 119, 1409, 1128, 1138, 170, 1933, 1115, 1128, 1156, 1176, 1106, 6265, 117, 4268, 1631, 1714, 1106, 3232, 1143, 1118, 2179, 1120, 113, 4573, 114, 5706, 18202, 1571, 118, 5899, 1568, 1545, 1137, 1118, 10632, 1120, 170, 10584, 18713, 119, 3618, 8625, 24766, 1604, 1495, 137, 170, 4063, 119, 5048, 1358, 119, 146, 1440, 1977, 1106, 4510, 1121, 1128, 106, 153, 119, 156, 119, 131, 1332, 146, 112, 182, 1136, 3780, 2712, 12731, 117, 146, 5548, 5369, 1159, 19777, 1158, 119, 146, 1567, 6303, 1139, 3044, 1164, 12731, 1105, 6755, 1114, 1168, 1234, 1150, 1132, 14472, 1164, 1142, 1893, 1532, 119, 146, 1145, 5548, 5369, 1159, 1114, 1139, 1266, 1105, 12138, 1207, 2844, 119, 1409, 1128, 1156, 1176, 1106, 3858, 1167, 1164, 1143, 117, 4268, 1631, 1714, 1106, 3143, 1139, 3265, 1120, 164, 3265, 4134, 166, 1137, 3143, 1143, 1120, 1139, 2362, 1388, 1120, 5311, 4617, 1715, 119, 102]",
         "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
        ],
        [
         "1",
         "[-100, 9, 9, 9, 1, 6, 6, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 9, 9, 9, 9, 2, 2, 7, 7, 7, 7, 7, 7, 9, 9, 9, 3, 3, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, -100]",
         "[101, 1422, 1271, 1110, 26956, 16139, 117, 1105, 146, 112, 182, 170, 9991, 1114, 1160, 1201, 1104, 2541, 119, 146, 3055, 1589, 1113, 170, 1933, 1187, 1195, 1434, 170, 1207, 8132, 10823, 1111, 1412, 1419, 119, 1109, 2273, 1108, 1106, 2561, 170, 1167, 4795, 118, 4931, 1105, 1107, 7926, 8588, 8551, 1115, 1156, 1294, 1122, 5477, 1111, 5793, 1106, 5494, 1147, 5756, 1105, 2469, 1869, 119, 1284, 1408, 1118, 7410, 5420, 1121, 1412, 5793, 1105, 9239, 4795, 1844, 1106, 2437, 1147, 2993, 1105, 2489, 1827, 119, 1284, 1173, 2011, 170, 1207, 4795, 8551, 1115, 1108, 1241, 19924, 17117, 1105, 3123, 1106, 1329, 119, 1284, 1145, 7042, 170, 1295, 1104, 1207, 1956, 117, 1216, 1112, 1103, 2912, 1111, 5793, 1106, 2458, 1147, 3300, 1607, 117, 1854, 1147, 3791, 117, 1105, 12295, 1619, 10068, 3294, 119, 1109, 1933, 1108, 170, 2244, 117, 1105, 1412, 5793, 1127, 1304, 2816, 1114, 1103, 1207, 10823, 119, 1220, 1276, 1122, 1106, 1129, 1277, 5477, 1106, 1329, 1190, 1103, 1385, 1141, 117, 1105, 1152, 12503, 1103, 1207, 1956, 119, 1284, 1486, 170, 2418, 2773, 1107, 8132, 10241, 1105, 8164, 1112, 170, 1871, 1104, 1103, 1207, 10823, 119, 7092, 1103, 1933, 117, 146, 1108, 2784, 1111, 4297, 1103, 1171, 118, 1322, 3463, 1111, 1103, 10823, 119, 146, 1145, 1589, 4099, 1114, 1103, 1902, 1264, 1106, 4989, 1115, 1103, 10823, 1108, 19924, 17117, 1105, 4795, 118, 4931, 119, 146, 112, 182, 6884, 1104, 1103, 1250, 1115, 146, 1225, 1113, 1142, 1933, 117, 1105, 146, 112, 182, 9588, 1115, 1122, 1209, 2760, 1106, 5257, 1412, 5793, 1111, 1201, 1106, 1435, 119, 1409, 1128, 1156, 1176, 1106, 3232, 1143, 117, 1139, 10632, 4134, 1110, 180, 4199, 17071, 1394, 119, 1129, 8638, 137, 176, 14746, 119, 3254, 1105, 1139, 2179, 1295, 1110, 5129, 26253, 3140, 26752, 1559, 1580, 1559, 119, 146, 1686, 1120, 5787, 1545, 11674, 1715, 119, 154, 20833, 1158, 1110, 1139, 22510, 119, 102]",
         "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-100, 9, 9, 9, 1, 1, 1, 6, 6, 6, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, ...]</td>\n",
       "      <td>[101, 1422, 1271, 1110, 138, 10584, 18713, 7312, 8625, 117, 1105, 146, 1821, 170, 23160, 1200, 1114, 1492, 1201, 1104, 2541, 119, 146, 2676, 170, 1304, 3527, 1105, 10467, 1933, 146, 1125, 1106, 1250, 1113, 1314, 1214, 119, 138, 8132, 4685, 1143, 1114, 170, 9692, 1266, 8048, 13853, 118, 170, 9883, 13828, 1115, 1125, 1151, 2085, 1205, 1194, 8225, 119, 7595, 117, 1103, 13828, 1108, 1107, 2869, 3879, 117, 1114, 1317, 5768, 16031, 1105, 170, 3088, 172, 25552, 119, 1109, 8132, 1458, 1143, 1106, 9176, 1122, 1106, 1157, 1393, 12887, 117, 1133, 1122, 1108, 2330, 1115, 1142, 1156, 1129, 1185, ...]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-100, 9, 9, 9, 1, 6, 6, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, ...]</td>\n",
       "      <td>[101, 1422, 1271, 1110, 26956, 16139, 117, 1105, 146, 112, 182, 170, 9991, 1114, 1160, 1201, 1104, 2541, 119, 146, 3055, 1589, 1113, 170, 1933, 1187, 1195, 1434, 170, 1207, 8132, 10823, 1111, 1412, 1419, 119, 1109, 2273, 1108, 1106, 2561, 170, 1167, 4795, 118, 4931, 1105, 1107, 7926, 8588, 8551, 1115, 1156, 1294, 1122, 5477, 1111, 5793, 1106, 5494, 1147, 5756, 1105, 2469, 1869, 119, 1284, 1408, 1118, 7410, 5420, 1121, 1412, 5793, 1105, 9239, 4795, 1844, 1106, 2437, 1147, 2993, 1105, 2489, 1827, 119, 1284, 1173, 2011, 170, 1207, 4795, 8551, 1115, 1108, 1241, 19924, 17117, 1105, 3123, ...]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                 labels  \\\n",
       "0  [-100, 9, 9, 9, 1, 1, 1, 6, 6, 6, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, ...]   \n",
       "1  [-100, 9, 9, 9, 1, 6, 6, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, ...]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            input_ids  \\\n",
       "0  [101, 1422, 1271, 1110, 138, 10584, 18713, 7312, 8625, 117, 1105, 146, 1821, 170, 23160, 1200, 1114, 1492, 1201, 1104, 2541, 119, 146, 2676, 170, 1304, 3527, 1105, 10467, 1933, 146, 1125, 1106, 1250, 1113, 1314, 1214, 119, 138, 8132, 4685, 1143, 1114, 170, 9692, 1266, 8048, 13853, 118, 170, 9883, 13828, 1115, 1125, 1151, 2085, 1205, 1194, 8225, 119, 7595, 117, 1103, 13828, 1108, 1107, 2869, 3879, 117, 1114, 1317, 5768, 16031, 1105, 170, 3088, 172, 25552, 119, 1109, 8132, 1458, 1143, 1106, 9176, 1122, 1106, 1157, 1393, 12887, 117, 1133, 1122, 1108, 2330, 1115, 1142, 1156, 1129, 1185, ...]   \n",
       "1  [101, 1422, 1271, 1110, 26956, 16139, 117, 1105, 146, 112, 182, 170, 9991, 1114, 1160, 1201, 1104, 2541, 119, 146, 3055, 1589, 1113, 170, 1933, 1187, 1195, 1434, 170, 1207, 8132, 10823, 1111, 1412, 1419, 119, 1109, 2273, 1108, 1106, 2561, 170, 1167, 4795, 118, 4931, 1105, 1107, 7926, 8588, 8551, 1115, 1156, 1294, 1122, 5477, 1111, 5793, 1106, 5494, 1147, 5756, 1105, 2469, 1869, 119, 1284, 1408, 1118, 7410, 5420, 1121, 1412, 5793, 1105, 9239, 4795, 1844, 1106, 2437, 1147, 2993, 1105, 2489, 1827, 119, 1284, 1173, 2011, 170, 1207, 4795, 8551, 1115, 1108, 1241, 19924, 17117, 1105, 3123, ...]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                      attention_mask  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 顯示前 first_n_data 筆資料\n",
    "pd.DataFrame(tokenized_dataset['train'].select(range(first_n_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料校對器 (Data Collator)\n",
    "\n",
    "在微調語言模型時，使用 data collator 是為了有效地準備和處理批次數據。以下是使用 data collator 的幾個主要原因：\n",
    "\n",
    "* 動態填充 (Dynamic Padding): 不同長度的序列需要填充到相同的長度，以便能夠在同一批次中進行處理。Data collator 可以自動計算每個批次的最大長度，並對序列進行適當的填充。\n",
    "\n",
    "* 批次處理 (Batch Processing): Data collator 可以將多個樣本組合成一個批次，這樣可以更高效地利用計算資源，特別是在使用 GPU 或 TPU 時。\n",
    "\n",
    "* 生成注意力掩碼 (Attention Masks): 在填充序列時，data collator 會生成相應的注意力掩碼 (attention masks)，以確保模型只關注實際的數據部分，而忽略填充部分。\n",
    "\n",
    "* 簡化代碼 (Code Simplification): 使用 data collator 可以簡化數據處理的代碼，減少手動處理數據的繁瑣步驟，讓開發者專注於模型設計和訓練。\n",
    "\n",
    "總之，data collator 在微調語言模型時提供了便利和效率，確保數據能夠以一致且高效的方式進行處理。\n",
    "\n",
    "這個課程中，我們不能僅使用 `DataCollatorWithPadding`，因為它只填充輸入（Input IDs, Attention Mask）。在這裡，我們的標籤 (Label) 應該以與輸入完全相同的方式進行填充，以保持相同的大小，使用 -100 作為填充值，以便在損失計算中忽略相應的預測。\n",
    "\n",
    "於是我們可以利用 `DataCollatorForTokenClassification` 完成。與 `DataCollatorWithPadding` 一樣，它需要使用預處理輸入的分詞器：\n",
    "\n",
    ">\n",
    "> * [DataCollatorWithPadding](https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorWithPadding) that will dynamically pad the inputs received.\n",
    "> * [DataCollatorForTokenClassification](https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorForTokenClassification) that will dynamically pad the inputs received, as well as the `labels`.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(\n",
    "  tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 展示 DataCollatorForTokenClassification 的輸出, 標籤以 -100 表示 padding\n",
    "features = [tokenized_dataset['train'][i] for i in range(first_n_data)]\n",
    "batch = data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0]]),\n",
      " 'input_ids': tensor([[  101,  1422,  1271,  1110,   138, 10584, 18713,  7312,  8625,   117,\n",
      "          1105,   146,  1821,   170, 23160,  1200,  1114,  1492,  1201,  1104,\n",
      "          2541,   119,   146,  2676,   170,  1304,  3527,  1105, 10467,  1933,\n",
      "           146,  1125,  1106,  1250,  1113,  1314,  1214,   119,   138,  8132,\n",
      "          4685,  1143,  1114,   170,  9692,  1266,  8048, 13853,   118,   170,\n",
      "          9883, 13828,  1115,  1125,  1151,  2085,  1205,  1194,  8225,   119,\n",
      "          7595,   117,  1103, 13828,  1108,  1107,  2869,  3879,   117,  1114,\n",
      "          1317,  5768, 16031,  1105,   170,  3088,   172, 25552,   119,  1109,\n",
      "          8132,  1458,  1143,  1106,  9176,  1122,  1106,  1157,  1393, 12887,\n",
      "           117,  1133,  1122,  1108,  2330,  1115,  1142,  1156,  1129,  1185,\n",
      "          6655,  6949,   119,  7993,  1139,  7623,  5537,  1105,  4884,   117,\n",
      "           146,  1310,  1103, 10141,  4579,  1104,  4267,  8878, 13756,  1103,\n",
      "         13828,   119,  2994,  9883,  1108,  4727,  2856,  1121,  1157,  3545,\n",
      "           117,  1105,  1103,  4938,   172, 25552,  1108,  2856,   119,  2857,\n",
      "          1103, 13828,  1108,  2423,  4267,  3202, 11553, 16465,  1181,   117,\n",
      "           146,  1899,  1596, 23268, 12370,  1296,  9883,  1105, 22457,  1122,\n",
      "          1111,  1251,  3290,   119, 18101,   117,  1103, 16031,  1127,  1155,\n",
      "          1107,  1363,  3879,   117,  1114,  1185, 16694,  1137, 13228,   119,\n",
      "          1109,  1397,  2585,  1108,  1106,  6949,  1103,  3088,   172, 25552,\n",
      "           119,   146,  4727,  1962,  5686,  1103,  3088,  3423,  1171,  1487,\n",
      "           117, 13291,  1115,  1103,   172, 25552,  1108, 26887,  1105,  5343,\n",
      "           119,  2857,  1103,   172, 25552,  1108, 14129,   117,   146,  1310,\n",
      "          1103,  1965,  1104,  1231, 11192,  5521,  6647,  1103, 13828,   119,\n",
      "          2994,  9883,  1108,  4727,  1973,  1171,  1154,  1157,  3545,   117,\n",
      "          1105,  1103, 13828,  1108, 13247,  1235,  1122, 14798,  2433,  1176,\n",
      "          1207,   119,  1332,   146,  2756,  1103,  5219, 13828,  1106,  1103,\n",
      "          8132,   117,  1152,  1127,  1166, 18734,  1174,   119,  1220,  1577,\n",
      "           112,   189,  2059,  1115,   146,  1125,  1151,  1682,  1106,  2498,\n",
      "          1147,  1266,  8048, 13853,  1171,  1106,  1297,   119,  1109, 13828,\n",
      "          1350,  1112,  2712,  1112,  1122,  1125,  1165,  1122,  1108,  1148,\n",
      "          1687,   117,  1105,  1103,  8132,  1108, 17278,  1106,  1138,  1122,\n",
      "          1171,  1107,  1147,  6224,   119,  1409,  1128,  1138,   170,  1933,\n",
      "          1115,  1128,  1156,  1176,  1106,  6265,   117,  4268,  1631,  1714,\n",
      "          1106,  3232,  1143,  1118,  2179,  1120,   113,  4573,   114,  5706,\n",
      "         18202,  1571,   118,  5899,  1568,  1545,  1137,  1118, 10632,  1120,\n",
      "           170, 10584, 18713,   119,  3618,  8625, 24766,  1604,  1495,   137,\n",
      "           170,  4063,   119,  5048,  1358,   119,   146,  1440,  1977,  1106,\n",
      "          4510,  1121,  1128,   106,   153,   119,   156,   119,   131,  1332,\n",
      "           146,   112,   182,  1136,  3780,  2712, 12731,   117,   146,  5548,\n",
      "          5369,  1159, 19777,  1158,   119,   146,  1567,  6303,  1139,  3044,\n",
      "          1164, 12731,  1105,  6755,  1114,  1168,  1234,  1150,  1132, 14472,\n",
      "          1164,  1142,  1893,  1532,   119,   146,  1145,  5548,  5369,  1159,\n",
      "          1114,  1139,  1266,  1105, 12138,  1207,  2844,   119,  1409,  1128,\n",
      "          1156,  1176,  1106,  3858,  1167,  1164,  1143,   117,  4268,  1631,\n",
      "          1714,  1106,  3143,  1139,  3265,  1120,   164,  3265,  4134,   166,\n",
      "          1137,  3143,  1143,  1120,  1139,  2362,  1388,  1120,  5311,  4617,\n",
      "          1715,   119,   102],\n",
      "        [  101,  1422,  1271,  1110, 26956, 16139,   117,  1105,   146,   112,\n",
      "           182,   170,  9991,  1114,  1160,  1201,  1104,  2541,   119,   146,\n",
      "          3055,  1589,  1113,   170,  1933,  1187,  1195,  1434,   170,  1207,\n",
      "          8132, 10823,  1111,  1412,  1419,   119,  1109,  2273,  1108,  1106,\n",
      "          2561,   170,  1167,  4795,   118,  4931,  1105,  1107,  7926,  8588,\n",
      "          8551,  1115,  1156,  1294,  1122,  5477,  1111,  5793,  1106,  5494,\n",
      "          1147,  5756,  1105,  2469,  1869,   119,  1284,  1408,  1118,  7410,\n",
      "          5420,  1121,  1412,  5793,  1105,  9239,  4795,  1844,  1106,  2437,\n",
      "          1147,  2993,  1105,  2489,  1827,   119,  1284,  1173,  2011,   170,\n",
      "          1207,  4795,  8551,  1115,  1108,  1241, 19924, 17117,  1105,  3123,\n",
      "          1106,  1329,   119,  1284,  1145,  7042,   170,  1295,  1104,  1207,\n",
      "          1956,   117,  1216,  1112,  1103,  2912,  1111,  5793,  1106,  2458,\n",
      "          1147,  3300,  1607,   117,  1854,  1147,  3791,   117,  1105, 12295,\n",
      "          1619, 10068,  3294,   119,  1109,  1933,  1108,   170,  2244,   117,\n",
      "          1105,  1412,  5793,  1127,  1304,  2816,  1114,  1103,  1207, 10823,\n",
      "           119,  1220,  1276,  1122,  1106,  1129,  1277,  5477,  1106,  1329,\n",
      "          1190,  1103,  1385,  1141,   117,  1105,  1152, 12503,  1103,  1207,\n",
      "          1956,   119,  1284,  1486,   170,  2418,  2773,  1107,  8132, 10241,\n",
      "          1105,  8164,  1112,   170,  1871,  1104,  1103,  1207, 10823,   119,\n",
      "          7092,  1103,  1933,   117,   146,  1108,  2784,  1111,  4297,  1103,\n",
      "          1171,   118,  1322,  3463,  1111,  1103, 10823,   119,   146,  1145,\n",
      "          1589,  4099,  1114,  1103,  1902,  1264,  1106,  4989,  1115,  1103,\n",
      "         10823,  1108, 19924, 17117,  1105,  4795,   118,  4931,   119,   146,\n",
      "           112,   182,  6884,  1104,  1103,  1250,  1115,   146,  1225,  1113,\n",
      "          1142,  1933,   117,  1105,   146,   112,   182,  9588,  1115,  1122,\n",
      "          1209,  2760,  1106,  5257,  1412,  5793,  1111,  1201,  1106,  1435,\n",
      "           119,  1409,  1128,  1156,  1176,  1106,  3232,  1143,   117,  1139,\n",
      "         10632,  4134,  1110,   180,  4199, 17071,  1394,   119,  1129,  8638,\n",
      "           137,   176, 14746,   119,  3254,  1105,  1139,  2179,  1295,  1110,\n",
      "          5129, 26253,  3140, 26752,  1559,  1580,  1559,   119,   146,  1686,\n",
      "          1120,  5787,  1545, 11674,  1715,   119,   154, 20833,  1158,  1110,\n",
      "          1139, 22510,   119,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0]]),\n",
      " 'labels': tensor([[-100,    9,    9,    9,    1,    1,    1,    6,    6,    6,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            2,    2,    2,    7,    7,    7,    7,    7,    7,    7,    9,    9,\n",
      "            9,    9,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    3,    8,    8,    8, -100],\n",
      "        [-100,    9,    9,    9,    1,    6,    6,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    9,    9,    9,\n",
      "            9,    9,    2,    2,    7,    7,    7,    7,    7,    7,    9,    9,\n",
      "            9,    3,    3,    8,    8,    8,    9,    9,    9,    9,    9,    9,\n",
      "            9, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100]])}\n"
     ]
    }
   ],
   "source": [
    "pprint(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這裡要注意的主要是第一個例子與第二個例子長度不一，所以長度不足的例子的 `input_ids` 和 `attention_mask` 已經在右側填充了一個 [PAD] 標記（其 ID 是 0）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-100,    9,    9,    9,    1,    1,    1,    6,    6,    6,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            2,    2,    2,    7,    7,    7,    7,    7,    7,    7,    9,    9,\n",
      "            9,    9,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    3,    8,    8,    8, -100],\n",
      "        [-100,    9,    9,    9,    1,    6,    6,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
      "            9,    9,    9,    9,    9,    9,    9,    9,    9,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    9,    9,    9,\n",
      "            9,    9,    2,    2,    7,    7,    7,    7,    7,    7,    9,    9,\n",
      "            9,    3,    3,    8,    8,    8,    9,    9,    9,    9,    9,    9,\n",
      "            9, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100]])\n"
     ]
    }
   ],
   "source": [
    "pprint(batch[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "類似地，我們可以看到 `labels` 已用 -100 填充，以確保填充標記被損失函數忽略。\n",
    "\n",
    "我們終於擁有了訓練所需的所有的前期準備！我們現在只需要使用標準參數實例化訓練器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型評估函數\n",
    "\n",
    "在訓練過程中包含度量標準通常有助於評估模型的性能。您可以使用 Evaluate 庫快速加載評估方法。對於這個任務，請加載 [seqeval](https://huggingface.co/docs/evaluate/a_quick_tour) 框架。Seqeval 實際上會生成多個分數：precision, recall, F1, 和 accuracy。\n",
    "\n",
    "* Precision: 精確率，是指所有被標記為正的樣本中實際為正的比例。\n",
    "\n",
    "$\\ Precision = \\frac{\\text{correctly classified actual positives}}{\\text{everything classified as positives}} = \\frac{TP}{TP + FP} $\n",
    "\n",
    "* Recall: 召回率，是指所有實際為正的樣本中被標記為正的比例。\n",
    "\n",
    "$\\ Recall = \\frac{\\text{correctly classified actual positives}}{\\text{all actual positives}} = \\frac{TP}{TP + FN} $\n",
    "\n",
    "* F1: F1 值是精確率和召回率的調和平均值，用於綜合考慮精確率和召回率。\n",
    "\n",
    "$\\ F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall} $\n",
    "\n",
    "* Accuracy: 準確率，是指所有被正確分類的樣本數量與總樣本數量之比。\n",
    "\n",
    "$\\ Accuracy = \\frac{\\text{correctly classifications}}{\\text{total classifications}} = \\frac{TP + TN}{TP + TN + FP + FN} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    # Unpack logits and labels from the input\n",
    "    logits, labels = eval_preds\n",
    "\n",
    "    # Convert logits to the index of the maximum logit value\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Map predictions and labels to their corresponding label names, ignoring padding (-100)\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    # Compute evaluation metrics using seqeval\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    # Return the computed metrics\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入預訓練模型\n",
    "\n",
    "您現在可以開始訓練您的模型了！使用 `AutoModelForTokenClassification` 加載預訓練的模型，並指定預期標籤的數量和標籤映射：\n",
    "\n",
    "需要留意，因為預訓練模型的分類數為 9，所以我們需要重新設定模型的分類數，且忽略預訓練模型的分類層。\n",
    "\n",
    "* `num_labels`: 指定了任務中標籤的數量。這告訴模型有多少個不同的標籤需要進行分類。\n",
    "\n",
    "* `ignore_mismatched_sizes`: 表示在加載模型時忽略預訓練模型的標籤數量與我們的標籤數量不匹配的情況。這在我們的任務標籤數量與預訓練模型的標籤數量不同時非常有用。\n",
    "\n",
    "* `id2label`: 是一個字典，用於將整數標籤映射到 BIO 標籤。這有助於在模型輸出時將整數標籤轉換為可讀的 BIO 標籤。\n",
    "\n",
    "* `label2id`: 是 `id2label` 的反向映射，用於將 BIO 標籤映射回整數標籤。\n",
    "\n",
    "以下程式碼從預訓練模型中加載一個標記分類模型，並根據特定任務的需求進行配置。它指定了模型名稱、任務標籤數量，並設置了忽略不匹配大小的選項。此外，它還提供了整數標籤與 BIO 標籤之間的映射，便於模型在輸入和輸出時進行轉換。這樣可以確保模型能夠正確處理特定任務中的標籤。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at dslim/distilbert-NER and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([9, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "  config.model_name,\n",
    "  num_labels=config.num_tags, # 任務標籤數量\n",
    "  ignore_mismatched_sizes=True, # 忽略不匹配的大小，預訓練模型的標籤數量與我們的標籤數量不同\n",
    "  id2label=config.id2tag, # 整數標籤到 BIO 標籤的映射\n",
    "  label2id=config.tag2id, # BIO 標籤到整數標籤的映射\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 65,198,602, Trainable Parameters: 65,198,602\n"
     ]
    }
   ],
   "source": [
    "# 查看可訓練的參數量約 65M\n",
    "print('Parameters: {:,}, Trainable Parameters: {:,}'.format(\n",
    "  model.num_parameters(),\n",
    "  model.num_parameters(only_trainable=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練參數設定\n",
    "\n",
    "用於設定訓練過程中的各種參數，如學習率、批次大小、梯度累積步數、訓練 epoch 數、權重衰減等。\n",
    "\n",
    "* `output_dir` 指定了訓練輸出的目錄。\n",
    "* `logging_steps` 訓練時的日誌步數，決定每隔多少步輸出一次訓練日誌。\n",
    "* `eval_strategy` 和 `save_strategy` 設定為 'steps'，表示每 `logging_steps` 個 steps 都會進行評估和儲存。\n",
    "* `load_best_model_at_end` 設定為 `True`，表示訓練結束後會載入最佳模型。\n",
    "* `report_to` 禁用 wandb 報告，適用於 Colab 環境，避免需要配置 wandb。\n",
    "* `save_total_limit` 最多儲存 5 個 checkpoints，控制儲存的模型檔案數量以節省磁碟空間。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir=os.path.join('saved_model', f'{__ipynb_file__}_train_log'), # 訓練輸出目錄\n",
    "  learning_rate=config.lr, # 學習率\n",
    "  per_device_train_batch_size=config.train_batch_size, # 每個設備的訓練批次大小\n",
    "  per_device_eval_batch_size=config.eval_batch_size,\n",
    "  logging_steps=100, # 訓練時的日誌步數, 預設每 500 步輸出一次日誌\n",
    "  num_train_epochs=config.epochs, # 訓練的總 epoch 數\n",
    "  weight_decay=config.weight_decay, # 權重衰減\n",
    "  eval_strategy='steps', # 評估策略\n",
    "  save_strategy='steps', # 儲存策略\n",
    "  load_best_model_at_end=True,\n",
    "  report_to='none', # Disable wandb on colab\n",
    "  save_total_limit=5, # 最多儲存 5 個 checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練器初始化\n",
    "\n",
    "用於初始化訓練器，並開始訓練模型。\n",
    "\n",
    "* `model` 是要訓練的模型。\n",
    "* `tokenizer` 是用於處理文本的分詞器。\n",
    "* `train_dataset` 和 `eval_dataset` 是訓練和評估數據集。\n",
    "* `data_collator` 是用於整理數據的數據整理器。\n",
    "* `compute_metrics` 是用於計算度量標準的函數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j8/hkxkfjqd58j9t_718vfr4tjw0000gn/T/ipykernel_22107/2800174042.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  tokenizer=tokenizer,\n",
    "  args=training_args,\n",
    "  train_dataset=tokenized_dataset['train'],\n",
    "  eval_dataset=tokenized_dataset['validation'],\n",
    "  data_collator=data_collator,\n",
    "  compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 開始訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='886' max='886' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [886/886 08:59, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.187000</td>\n",
       "      <td>0.059100</td>\n",
       "      <td>0.770696</td>\n",
       "      <td>0.786544</td>\n",
       "      <td>0.778539</td>\n",
       "      <td>0.982758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.041753</td>\n",
       "      <td>0.833223</td>\n",
       "      <td>0.868155</td>\n",
       "      <td>0.850330</td>\n",
       "      <td>0.988434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.038400</td>\n",
       "      <td>0.039231</td>\n",
       "      <td>0.858878</td>\n",
       "      <td>0.874068</td>\n",
       "      <td>0.866406</td>\n",
       "      <td>0.989297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.039900</td>\n",
       "      <td>0.034184</td>\n",
       "      <td>0.878506</td>\n",
       "      <td>0.875932</td>\n",
       "      <td>0.877217</td>\n",
       "      <td>0.990531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.034500</td>\n",
       "      <td>0.031786</td>\n",
       "      <td>0.879389</td>\n",
       "      <td>0.895270</td>\n",
       "      <td>0.887258</td>\n",
       "      <td>0.991388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.029400</td>\n",
       "      <td>0.029933</td>\n",
       "      <td>0.884565</td>\n",
       "      <td>0.903367</td>\n",
       "      <td>0.893867</td>\n",
       "      <td>0.991982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.028532</td>\n",
       "      <td>0.882058</td>\n",
       "      <td>0.917111</td>\n",
       "      <td>0.899243</td>\n",
       "      <td>0.992282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.029900</td>\n",
       "      <td>0.027806</td>\n",
       "      <td>0.887585</td>\n",
       "      <td>0.918602</td>\n",
       "      <td>0.902827</td>\n",
       "      <td>0.992505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=886, training_loss=0.052497613779847296, metrics={'train_runtime': 539.8688, 'train_samples_per_second': 6.563, 'train_steps_per_second': 1.641, 'total_flos': 436537886641608.0, 'train_loss': 0.052497613779847296, 'epoch': 1.0})"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 開始訓練，這可能需要一些時間\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練完成後，您可以通過運行 `Trainer.evaluate()` 方法在驗證集上評估模型的性能。它會計算模型的損失和其他評估指標，並返回這些結果。這對於了解模型在未見數據上的表現非常有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='222' max='222' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [222/222 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.03178605064749718,\n",
       " 'eval_precision': 0.8793888336560096,\n",
       " 'eval_recall': 0.8952695503942041,\n",
       " 'eval_f1': 0.8872581368951773,\n",
       " 'eval_accuracy': 0.9913876090117317,\n",
       " 'eval_runtime': 26.4477,\n",
       " 'eval_samples_per_second': 33.5,\n",
       " 'eval_steps_per_second': 8.394,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存微調模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 儲存模型\n",
    "trainer.save_model(config.saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 釋放資源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import garbage collector\n",
    "import gc\n",
    "\n",
    "# 釋放 GPU 記憶體\n",
    "del trainer\n",
    "del tokenizer\n",
    "\n",
    "model.to('cpu')\n",
    "del model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評估微調模型\n",
    "\n",
    "### 載入微調分詞器 (Tokenizer)\n",
    "\n",
    "從已經完成訓練的模型取得 Tokenizer。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  config.saved_model_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入微調後模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "ft_model = AutoModelForTokenClassification.from_pretrained(\n",
    "  config.saved_model_path,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請留意分類數已經從預訓練模型的 9 類，變成 10 類。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForTokenClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "pprint(ft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning 後的表現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入微調模型，宣告分類器\n",
    "classifier = pipeline(\n",
    "  task=\"token-classification\",\n",
    "  model=ft_model,\n",
    "  tokenizer=tokenizer,\n",
    "  device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "輸入: Hello, I'm Nicholas Moore, a man with a rich tapestry of life experiences. Throughout my journey, I've encountered countless individuals and embarked on extraordinary adventures, all of which have shaped me into the person I am today. I grew up in a quaint little town nestled amidst rolling hills and vibrant meadows. My childhood was filled with laughter, exploration, and the warmth of family. As I matured, my thirst for knowledge and adventure propelled me to pursue higher education. I delved into the world of literature, philosophy, and science, expanding my horizons and igniting a passion for lifelong learning. In my early twenties, I embarked on a transformative journey that took me across continents and cultures. I backpacked through Southeast Asia, marveled at the wonders of the ancient world in Europe, and immersed myself in the vibrant energy of South America. These experiences opened my eyes to the beauty and diversity of our planet and instilled in me a profound appreciation for different perspectives. Upon returning home, I channeled my newfound insights into a career in international relations. I joined a non-governmental organization dedicated to promoting peace and understanding among nations. Through my work, I had the privilege of collaborating with passionate individuals from all walks of life, working tirelessly to make a positive impact on the world. Along the way, I met remarkable people who became lifelong friends and confidants. I discovered the power of human connection and the importance of nurturing meaningful relationships. Whether it was sharing laughter over a cup of coffee or engaging in deep conversations that challenged my perspectives, these connections enriched my life beyond measure. As I reflect on my journey thus far, I'm grateful for the experiences, both joyous and challenging, that have shaped me. I've learned the value of resilience, empathy, and the unwavering pursuit of my dreams. Though my path may have taken unexpected turns, I wouldn't trade it for anything. If you'd like to connect, feel free to reach me at nicholas_moore8047@yahoo.gov or +91-63720 22261. You can also find me at 35915 Patrick Mews Suite 978, where I'm always open to engaging in thought-provoking conversations and embarking on new adventures.\n",
      "('Nicholas       Moore,         nicholas_moore 8047        @yahoo.gov '\n",
      " '+91-        637         20          222         61          35915       '\n",
      " 'Patrick          ws      Suite            978         ')\n",
      "('B-NAME_STUDENT I-NAME_STUDENT B-EMAIL        I-PHONE_NUM B-EMAIL    '\n",
      " 'B-PHONE_NUM I-PHONE_NUM B-PHONE_NUM I-PHONE_NUM B-PHONE_NUM B-PHONE_NUM '\n",
      " 'I-STREET_ADDRESS B-EMAIL I-STREET_ADDRESS B-PHONE_NUM ')\n",
      "\n",
      "輸入: Hello, my name is Alexey Novikov and I'm a psychologist who solved a rather unique case recently. A few weeks ago, a woman named Sarah came to me with a peculiar problem. She was experiencing recurring nightmares that left her feeling anxious and exhausted. Upon further exploration, I discovered that Sarah had a fear of confined spaces, which she had developed after being trapped in an elevator for several hours as a child. To address Sarah's issue, I employed a combination of cognitive-behavioral therapy and exposure therapy. Through cognitive-behavioral therapy, we worked on identifying and challenging the negative thoughts and beliefs that were contributing to her fear. Simultaneously, we engaged in exposure therapy, where Sarah was gradually exposed to confined spaces in a controlled and supportive environment. As we progressed with the therapy, Sarah's fear of confined spaces began to diminish. She reported experiencing fewer nightmares and feeling more confident in everyday situations that involved enclosed areas. The transformation in Sarah's life was remarkable. She could now go to the grocery store without feeling overwhelmed, attend social events in smaller rooms, and even take public transportation during rush hour. I am grateful for the opportunity to have helped Sarah overcome her fear. It is incredibly rewarding to witness the positive impact that psychological interventions can have on individuals' lives. If you'd like to connect with me, you can reach me via email at alexey.novikov@msn.gov or by visiting my office located at 161 Creek Road. Thank you for reading!\n",
      "'Alexey         Novikov        alexey.novikov@msn.gov 161              '\n",
      "'B-NAME_STUDENT I-NAME_STUDENT B-EMAIL                B-STREET_ADDRESS '\n",
      "\n",
      "輸入: My name is Ludmila Inoue, and I'm a person with a story worth telling. From my humble beginnings at 706 Seagrove Road, I've embarked on a journey that has taken me to great heights. With a spirit of adventure and a determination to succeed, I've navigated life's twists and turns with resilience and grace. Growing up, I was fascinated by the world around me. Books were my companions, and knowledge became my currency. I spent countless hours exploring libraries and immersing myself in different subjects. My passion for learning fueled my desire to make a meaningful impact on the world. As I embarked on my professional journey, I discovered a knack for problem-solving and a talent for connecting with people. I found myself drawn to the dynamic world of business, where I could apply my skills and make a tangible difference. With unwavering dedication and a strong work ethic, I rose through the ranks, leaving a trail of success in my wake. Throughout my career, I've had the privilege of working with some of the brightest minds and most inspiring leaders. These collaborations have enriched my life and taught me invaluable lessons about teamwork, innovation, and the power of perseverance. I've learned that success is not a solo endeavor; it's a collective effort where everyone's contributions matter. My journey has not been without its challenges. I've faced obstacles and setbacks along the way, but I've always approached them as opportunities for growth. I believe that adversity is a catalyst for resilience, and it's in the toughest times that we discover our true strength and potential. As I reflect on my life, I'm filled with gratitude for the experiences that have shaped me into the person I am today. I'm grateful for the people who have supported me, challenged me, and inspired me to reach new heights. I'm grateful for the lessons I've learned, both the triumphs and the setbacks. If I could offer one piece of advice to those who are just starting their own journeys, it would be this: never give up on your dreams. No matter how big or small, your aspirations are valid. Embrace the challenges that come your way, and never stop learning and growing. The world needs your unique talents and perspectives, so share them with confidence. As I continue on this incredible journey, I remain committed to making a positive impact on the world. I'm excited about the possibilities that lie ahead and the opportunities to connect with new people and make a difference. If you'd like to reach out to me, feel free to contact me at (285) 815-7373 or ludmila_inoue@outlook.net. Let's collaborate and create something extraordinary together!\n",
      "'Ludmila        Inoue,         706              '\n",
      "'B-NAME_STUDENT I-NAME_STUDENT B-STREET_ADDRESS '\n",
      "\n",
      "輸入: Dr. Tu Garcia, a renowned dermatologist, embarked on a project last year that aimed to raise awareness about skin cancer prevention and early detection. With a passion for educating the community about the importance of skin health, Dr. Garcia dedicated himself to this project, leveraging his expertise and various communication channels. Dr. Garcia began the project by establishing a dedicated webpage on his website, http://blog.tu-garcia.edu, where he published informative articles, videos, and interactive quizzes on skin cancer. He used his extensive knowledge to provide valuable insights into different types of skin cancer, risk factors, and the importance of regular skin self-examinations. To further engage with the community, Dr. Garcia organized several educational workshops and seminars at local community centers and schools. He delivered interactive presentations, using visual aids and real-life examples to emphasize the crucial role of sun protection and early detection in preventing skin cancer. He also facilitated Q&A sessions, allowing participants to clarify doubts and gain a deeper understanding of the subject. Understanding the significance of accessibility, Dr. Garcia made himself available for consultations and inquiries through various channels. He provided his phone number, +91-14426 83047, and email address, tugarcia@outlook.com, ensuring that individuals could reach him conveniently. He responded promptly to queries, offering personalized advice and guidance. Dr. Garcia's efforts were widely appreciated, and the project witnessed a remarkable response. The webpage garnered significant traffic, with individuals seeking information and resources on skin cancer. The workshops and seminars were attended by a diverse audience, including students, adults, and senior citizens, demonstrating the project's inclusivity and effectiveness. Through this project, Dr. Tu Garcia successfully raised awareness about skin cancer prevention and early detection. His dedication to educating the community and providing accessible resources made a positive impact, empowering individuals to take proactive steps towards maintaining healthy skin. The project serves as an exemplary model for dermatologists and healthcare professionals who strive to make a difference in promoting skin health.\n",
      "('Tu             Garcia,        Garcia         Garcia         '\n",
      " 'http://blog.tu-garcia.edu, Garcia         Garcia         +91-14426   '\n",
      " \"83047,      tugarcia@outlook.com, Garcia's       Tu             \")\n",
      "('B-NAME_STUDENT I-NAME_STUDENT B-NAME_STUDENT B-NAME_STUDENT '\n",
      " 'B-URL_PERSONAL             B-NAME_STUDENT B-NAME_STUDENT B-PHONE_NUM '\n",
      " 'I-PHONE_NUM B-EMAIL               B-NAME_STUDENT B-NAME_STUDENT ')\n",
      "\n",
      "輸入: Hello, I'm Badi Nakamura, and I work as a programmer. I'm based out of 2703 Woolsey Street, and you can reach me via email at badinakamura@gmail.org. One day at work, I encountered a puzzling problem. Our software was experiencing frequent crashes, and the error logs weren't providing any clear indications of the root cause. After a thorough investigation, I discovered that the crashes were being triggered by a specific sequence of user actions. This sequence involved a series of rapid button clicks that resulted in a race condition, causing the software to malfunction. To resolve the issue, I implemented a lock mechanism to ensure that only one user action could be processed at a time. This prevented the race condition from occurring and eliminated the crashes. Additionally, I optimized the code to improve its performance and stability. As a result of my efforts, the software's stability significantly improved, and the user experience was greatly enhanced. I was proud to have played a role in resolving this issue and contributing to the overall success of the project.\n",
      "'Badi           Nakamura,      2703             WoolseyStreet,   '\n",
      "'B-NAME_STUDENT I-NAME_STUDENT B-STREET_ADDRESS I-STREET_ADDRESS '\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 顯示微調模型預測結果，僅顯示有被標注的部分\n",
    "for val in dataset['test']: # 逐一取出測試資料\n",
    "  print(f'輸入: {val[\"text\"]}')\n",
    "  display_words_entity(val[\"text\"], classifier)\n",
    "  print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
