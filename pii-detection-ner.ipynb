{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "metadata": {},
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# transformers not support NumPy 2.0 yet\n",
    "!pip install -q numpy~=1.26.4 transformers~=4.46.2\n",
    "!pip install -q datasets seqeval matplotlib pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import pipeline\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "\n",
    "# 檢查是否有 GPU 可以使用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下載資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# 從 Kaggle 下載 PII External Dataset\n",
    "!curl -L -o ./sample_data/pii-detect-gpt3-5-synthetic-data-8k.zip \\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/dileepjayamal/pii-detect-gpt3-5-synthetic-data-8k\n",
    "\n",
    "# 解壓縮\n",
    "!unzip -o -q ./sample_data/pii-detect-gpt3-5-synthetic-data-8k.zip -d ./sample_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 將 json 檔案轉換成 jsonl 檔案\n",
    "def json_to_jsonl(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for d in data:\n",
    "            json.dump(d, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "\n",
    "input_file = 'sample_data/PII_Detect_GPT3.5_Generated_data_v1.json'\n",
    "output_file = 'sample_data/PII_Detect_GPT3.5_Generated_data_v1.jsonl'\n",
    "json_to_jsonl(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 資料包含什麼？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "immutable_dataset = load_dataset('json', data_files=output_file)\n",
    "\n",
    "immutable_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Keep required features: 'tokens', 'labels'\n",
    "dataset = immutable_dataset.remove_columns(['trailing_whitespace'])\n",
    "# Keep length of tokens is equal to length of labels\n",
    "dataset = dataset.filter(lambda x: len(x['tokens']) == len(x['labels']))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 顯示前 5 筆資料\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.DataFrame(dataset['train'].select(range(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 顯示資 BIO 詞性標注\n",
    "label_names = set()\n",
    "for data in dataset['train']:\n",
    "    label_names.update(data['labels'])\n",
    "# convert set to list and sort label names\n",
    "label_names = list(label_names)\n",
    "pprint(label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 訓練相關設定\n",
    "class Config(BaseModel):\n",
    "  seed: int = 42\n",
    "  model_name: str = \"bert-base-cased\" # name of pretrained backbone\n",
    "  train_seq_len: int = 1024 # max size of input sequence for training\n",
    "  train_batch_size: int = 4 # size of the input batch in training\n",
    "  eval_batch_size: int = 4 # size of the input batch in evaluation\n",
    "  epochs: int = 3 # number of epochs to train\n",
    "  lr: float = 2e-5 # learning rate\n",
    "  tags: list # BIO (Beginning, Inner, Outer) format tags\n",
    "  id2tag: dict # integer label to BIO format tags mapping\n",
    "  tag2id: dict # BIO format tags to integer tags mapping\n",
    "  num_tags: int # number of PII (NER) tags\n",
    "\n",
    "tags_name = [\n",
    "  'O',\n",
    "  'B-URL_PERSONAL', 'I-URL_PERSONAL',\n",
    "  'B-ID_NUM', 'I-ID_NUM',\n",
    "  'B-NAME_STUDENT', 'I-NAME_STUDENT',\n",
    "  'B-PHONE_NUM', 'I-PHONE_NUM',\n",
    "  'B-STREET_ADDRESS', 'I-STREET_ADDRESS',\n",
    "  'B-USERNAME', 'I-USERNAME',\n",
    "  'B-EMAIL', 'I-EMAIL',\n",
    "]\n",
    "id2tag = dict(enumerate(tags_name))\n",
    "config = Config(\n",
    "  tags=tags_name,\n",
    "  id2tag=id2tag,\n",
    "  tag2id=dict((v, k) for k, v in id2tag.items()),\n",
    "  num_tags=len(tags_name)\n",
    "  )\n",
    "\n",
    "pprint(f'id2tag: {sorted(config.id2tag.items(), key=lambda x: x[0])}')\n",
    "pprint(f'tag2id: {sorted(config.tag2id.items(), key=lambda x: x[1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 並列顯示前 max_display 個 tokens 與 labels\n",
    "max_display = 20\n",
    "\n",
    "for i in range(5):\n",
    "    tokens = dataset['train'][i]['tokens'][:max_display]\n",
    "    labels = dataset['train'][i]['labels'][:max_display]\n",
    "    line1 = \"\"\n",
    "    line2 = \"\"\n",
    "    for token, label in zip(tokens, labels):\n",
    "        max_length = max(len(token), len(label))\n",
    "        line1 += token + \" \" * (max_length - len(token) + 1)\n",
    "        line2 += label + \" \" * (max_length - len(label) + 1)\n",
    "    pprint(line1, width=200)\n",
    "    pprint(line2, width=200)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 先觀察 Fine-tuning 前的表現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model = \"vblagoje/bert-english-uncased-finetuned-pos\"\n",
    "classifier = pipeline(\n",
    "  task=\"token-classification\",\n",
    "  model=model,\n",
    "  device=device,)\n",
    "classifier(\"My name is Frank, my email is frank@gmail.com, and my phone number is 123-456-7890.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 了解 Tokenizer 行為"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "\n",
    "# 以第一筆資料為例\n",
    "input_tokens = dataset[\"train\"][0][\"tokens\"]\n",
    "input_labels = dataset[\"train\"][0][\"labels\"]\n",
    "input_token_ids = tokenizer(input_tokens, is_split_into_words=True)\n",
    "\n",
    "# 如我們所見，分詞器添加了模型使用的特殊標記（[CLS] 在開頭和 [SEP] 在結尾），\n",
    "# 並且大多數單詞保持不變。然而，單詞 Bareilly 被分詞為三個子詞，Bar, ##eil 和 ##ly\n",
    "pprint(input_token_ids.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 原始資料\n",
    "pprint(f'length of token: {len(input_tokens)}')\n",
    "pprint(f'length of tag: {len(input_labels)}')\n",
    "# 這導致了我們的輸入和標籤之間的不匹配\n",
    "pprint(f'length of token id: {len(input_token_ids.tokens())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 我們可以輕鬆地將每個標記映射到其對應的單詞。\n",
    "input_token_ids.word_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "metadata": {}
   },
   "source": [
    "# 重新校準 Tokenizer 與標籤\n",
    "\n",
    "稍作處理後，我們可以擴展標籤列表以匹配標記。首先，我們將應用的規則是特殊標記獲得 -100 標籤。這是因為默認情況下，-100 是我們將使用的損失函數中被忽略的索引。然後，每個標記獲得與其所在單詞開始標記相同的標籤，因為它們是同一實體的一部分。對於單詞內但不在開頭的標記，我們將 B- 替換為 I- ，因為該標記不是開始實體："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(word_ids, labels):\n",
    "    new_labels = []\n",
    "    current_word_id = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None:\n",
    "            # Special token\n",
    "            label = -100\n",
    "        elif word_id != current_word_id:\n",
    "            # Start of a new token!\n",
    "            current_word_id = word_id\n",
    "            label = -100 if word_id is None else config.tag2id.get(labels[word_id])\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = config.tag2id.get(labels[word_id])\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "        new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "labels = align_labels_with_tokens(input_token_ids.word_ids(), input_labels)\n",
    "\n",
    "pprint(f'length of token id: {len(input_token_ids.tokens())}')\n",
    "pprint(f'length of labels: {len(labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# B-NAME_STUDENT(5) I-NAME_STUDENT(6)\n",
    "pprint(labels[:max_display])\n",
    "# Russell Contreras\n",
    "pprint(input_token_ids.tokens()[:max_display])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 批次重新 Tokenizer 與標籤\n",
    "\n",
    "要預處理整個數據集，我們需要對所有輸入進行分詞，並對所有標籤應用 `align_labels_with_tokens()`。為了利用快速分詞器的速度，最好一次分詞大量文本，因此我們將編寫一個批次處理函數，並使用 Dataset.map() 方法，選項設置為 batched=True。\n",
    "\n",
    "與之前的示例不同的是，當分詞器的輸入是文本列表（或在我們的情況下，是單詞列表的列表）時，word_ids() 函數需要獲取我們想要單詞 ID 的示例索引，因此我們也添加了這一點："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(dataset):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        dataset['tokens'], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = dataset['labels']\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(word_ids, labels))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 顯示前 3 筆資料\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.DataFrame(tokenized_datasets['train'].select(range(3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
