{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "metadata": {},
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# transformers not support NumPy 2.0 yet\n",
    "!pip install -q numpy~=1.26.4 transformers~=4.46.2\n",
    "!pip install -q datasets evaluate matplotlib pydantic seqeval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練 PII 偵測模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import evaluate\n",
    "\n",
    "from transformers import (\n",
    "  AutoTokenizer,\n",
    "  AutoModelForTokenClassification,\n",
    "  TrainingArguments,\n",
    "  Trainer,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from pprint import pprint\n",
    "\n",
    "import ast\n",
    "import torch\n",
    "\n",
    "# 檢查是否有 GPU 可以使用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下載資料\n",
    "\n",
    "從 Kaggle 下載 PII External Dataset，並解壓縮到 `sample_data` 資料夾。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# 從 Kaggle 下載 PII External Dataset\n",
    "!curl -L -o ./sample_data/pii-external-dataset.zip \\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/alejopaullier/pii-external-dataset\n",
    "\n",
    "# 解壓縮\n",
    "!unzip -o -q ./sample_data/pii-external-dataset.zip -d ./sample_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料包含什麼？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# The full `train` split\n",
    "immutable_dataset = load_dataset('csv', data_files='sample_data/pii_dataset.csv', split='train')\n",
    "# Split into 80% training and 20% testing sets\n",
    "immutable_dataset = immutable_dataset.train_test_split(\n",
    "  test_size=0.2, # 20% of the data is used for testing\n",
    "  shuffle=False, # Ensure that train and test sets are the same across runs\n",
    "  )\n",
    "immutable_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 保留必要 features: 'tokens', 'labels'\n",
    "dataset = immutable_dataset.remove_columns([\n",
    "  'document', 'text', 'trailing_whitespace', 'prompt', 'prompt_id', 'name',\n",
    "  'email', 'phone', 'job', 'address', 'username', 'url', 'hobby', 'len'])\n",
    "\n",
    "# convert 'tokens' and 'labels' from string to list\n",
    "dataset = dataset.map(lambda x: {'tokens': ast.literal_eval(x['tokens']), 'labels': ast.literal_eval(x['labels'])})\n",
    "\n",
    "# 確認 tokens 長度與 labels 長度相等，避免有缺失的情況, 以 json string 的方式將 tokens 與 labels 轉換後比較\n",
    "dataset = dataset.filter(lambda x: len(x['tokens']) == len(x['labels']))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 將 tokens 欄位重新命名為 words 避免與後面的 tokens 概念混淆\n",
    "dataset = dataset.rename_column('tokens', 'words')\n",
    "# 顯示前 first_n_data 筆資料\n",
    "first_n_data = 3\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.DataFrame(dataset['train'].select(range(first_n_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料中的 BIO 詞性標注\n",
    "\n",
    "IOB 格式（inside, outside, beginning 的縮寫），也常被稱為 BIO 格式，是計算語言學中用於標記任務（例如命名實體識別 NER，詞性標記 POS）的常見標記格式。\n",
    "\n",
    "* B - for the first token of a named entity\n",
    "* I - for tokens inside named entity's\n",
    "* O - for tokens outside any named entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 顯示 BIO 詞性標注\n",
    "label_names = set()\n",
    "for data in dataset['train']:\n",
    "    label_names.update(data['labels'])\n",
    "# convert set to list and sort label names\n",
    "label_names = list(label_names)\n",
    "pprint(label_names, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 訓練相關設定\n",
    "class Config(BaseModel):\n",
    "  seed: int = 42\n",
    "  model_name: str = 'dslim/distilbert-NER' # 使用蒸餾模型，降低參數量，加快訓練速度\n",
    "  saved_model_path: str = 'sample_data/saved_model' # path to save the trained model\n",
    "  train_seq_len: int = 1024 # max size of input sequence for training\n",
    "  train_batch_size: int = 4 # size of the input batch in training\n",
    "  eval_batch_size: int = 4 # size of the input batch in evaluation\n",
    "  epochs: int = 1 # 為加速訓練，只訓練一個 epoch\n",
    "  lr: float = 2e-5 # learning rate, controls how fast or slow the model learns\n",
    "  weight_decay: float = 0.01 # weight decay, helps the model stay simple and avoid overfitting by penalizing large weights.\n",
    "  tags: list # BIO (Beginning, Inner, Outer) format tags\n",
    "  id2tag: dict # integer label to BIO format tags mapping\n",
    "  tag2id: dict # BIO format tags to integer tags mapping\n",
    "  num_tags: int # number of PII (NER) tags\n",
    "\n",
    "id2tag = dict(enumerate(label_names))\n",
    "config = Config(\n",
    "  tags=label_names,\n",
    "  id2tag=id2tag,\n",
    "  tag2id=dict((v, k) for k, v in id2tag.items()),\n",
    "  num_tags=len(label_names)\n",
    "  )\n",
    "\n",
    "pprint(f'id2tag: {sorted(config.id2tag.items(), key=lambda x: x[0])}')\n",
    "pprint(f'tag2id: {sorted(config.tag2id.items(), key=lambda x: x[1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 並列顯示前 max_display 個 words 與 labels\n",
    "max_display = 50\n",
    "\n",
    "def show_nth_data(dataset, nth, max_display):\n",
    "    words = dataset[nth]['words'][:max_display]\n",
    "    labels = dataset[nth]['labels'][:max_display]\n",
    "    line1 = \"\"\n",
    "    line2 = \"\"\n",
    "    for word, label in zip(words, labels):\n",
    "        max_length = max(len(word), len(label))\n",
    "        line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "        line2 += label + \" \" * (max_length - len(label) + 1)\n",
    "    pprint(line1, width=200)\n",
    "    pprint(line2, width=200)\n",
    "    print()\n",
    "\n",
    "def show_data(dataset, first_n_data, max_display):\n",
    "    for i in range(first_n_data):\n",
    "        show_nth_data(dataset, i, max_display)\n",
    "\n",
    "# show_data(dataset['train'], first_n_data, max_display)\n",
    "show_data(dataset['test'], first_n_data, max_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 先觀察 Fine-tuning 前的表現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "test_text = '''\n",
    "Hello, I'm Badi Nakamura, and I work as a programmer.\n",
    "I'm based out of 2703 Woolsey Street, and you can reach me via email at badinakamura@gmail.org.\n",
    "'''\n",
    "\n",
    "classifier = pipeline(\n",
    "  task=\"token-classification\",\n",
    "  model=config.model_name,\n",
    "  device=device,)\n",
    "\n",
    "# 合併顯示預測結果\n",
    "def show_prediction(text, classifier):\n",
    "    result = classifier(text)\n",
    "    line1 = \"\"\n",
    "    line2 = \"\"\n",
    "    for r in result:\n",
    "        word = r['word']\n",
    "        label = r['entity']\n",
    "        max_length = max(len(word), len(label))\n",
    "        line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "        line2 += label + \" \" * (max_length - len(label) + 1)\n",
    "    pprint(line1, width=200)\n",
    "    pprint(line2, width=200)\n",
    "\n",
    "print(f'輸入: {test_text}')\n",
    "show_prediction(test_text, classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 了解 Tokenizer 行為"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "# 確認 tokenizer 是否為 fast tokenizer\n",
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "# 以第 n 筆資料為例\n",
    "data_cat = 'test'\n",
    "data_nth = 2\n",
    "input_words = dataset[data_cat][data_nth][\"words\"]\n",
    "input_labels = dataset[data_cat][data_nth][\"labels\"]\n",
    "input_token_ids = tokenizer(\n",
    "  input_words,\n",
    "  # is_split_into_words: Whether or not the input is already pre-tokenized (e.g., split into words).\n",
    "  # If set to True, the tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace) which it will tokenize.\n",
    "  # This is useful for NER or token classification.\n",
    "  is_split_into_words=True)\n",
    "\n",
    "# 如我們所見，分詞器添加了模型使用的特殊標記（[CLS] 在開頭和 [SEP] 在結尾），\n",
    "# 並且大多數單詞保持不變。然而，某些單詞 (word) 會被分為數個子詞 (subword)，如: Bar, ##eil 和 ##ly\n",
    "pprint(input_token_ids.tokens(), compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 原始資料\n",
    "print(f'length of input_words: {len(input_words)}')\n",
    "print(f'length of input_labels: {len(input_labels)}')\n",
    "# 這導致了我們的輸入和標籤之間的不匹配\n",
    "print(f'length of token id: {len(input_token_ids.tokens())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 資料對比\n",
    "print('=== Tokenizer 前 ===')\n",
    "show_nth_data(dataset[data_cat], data_nth, max_display)\n",
    "print('=== Tokenizer 後 ===')\n",
    "pprint(input_token_ids.tokens()[:max_display], compact=True)\n",
    "# 感謝 fast tokenizer 我們可以輕鬆地將每個標記映射到其對應的單詞。\n",
    "# `word_ids` return the list of tokens (sub-parts of the input strings after word/subword splitting and before conversion to integer indices)\n",
    "# at a given batch index (only works for the output of a fast tokenizer).\n",
    "print()\n",
    "print('=== 對應的 word_ids ===')\n",
    "pprint(input_token_ids.word_ids()[:max_display], compact=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "metadata": {}
   },
   "source": [
    "## 重新校準 Tokenizer 與標籤\n",
    "\n",
    "稍作處理後，我們可以擴展標籤列表以匹配標記。首先，我們將應用的規則是特殊標記獲得 -100 標籤。這是因為默認情況下，-100 是我們在損失函數中被忽略的索引。然後，每個標記獲得與其所在單詞開始標記相同的標籤，因為它們是同一實體的一部分。對於單詞內但不在開頭的標記，我們將 B- 替換為 I- ，因為該標記不是開始實體："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(word_ids, labels):\n",
    "    new_labels = []\n",
    "    current_word_id = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None:\n",
    "            # Special token\n",
    "            label = -100\n",
    "        elif word_id != current_word_id:\n",
    "            # Start of a new token!\n",
    "            current_word_id = word_id\n",
    "            label = -100 if word_id is None else config.tag2id.get(labels[word_id])\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = config.tag2id.get(labels[word_id])\n",
    "            # # If the label is B-XXX we change it to I-XXX\n",
    "            # if label % 2 == 1:\n",
    "            #     label += 1\n",
    "        new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "labels = align_labels_with_tokens(input_token_ids.word_ids(), input_labels)\n",
    "\n",
    "print(f'length of token id: {len(input_token_ids.tokens())}')\n",
    "print(f'length of labels: {len(labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print('=== 對應的標籤 ===')\n",
    "pprint(labels[:max_display], compact=True)\n",
    "# Da-Fu Anderson, 814 Keswick Boulevard\n",
    "pprint(input_token_ids.tokens()[:max_display], compact=True)\n",
    "# 與原始資料比較\n",
    "print()\n",
    "print('=== 原始資料 ===')\n",
    "show_nth_data(dataset[data_cat], data_nth, max_display)\n",
    "print('=== 標籤定義 ===')\n",
    "pprint(sorted(config.id2tag.items(), key=lambda x: x[0]), compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 批次重新 Tokenizer 與標籤\n",
    "\n",
    "要預處理整個數據集，我們需要對所有輸入進行分詞，並對所有標籤應用 `align_labels_with_tokens()`。為了利用快速分詞器的速度，最好一次分詞大量文本，因此我們將編寫一個批次處理函數，並使用 Dataset.map() 方法，選項設置為 batched=True。\n",
    "\n",
    "與之前的示例不同的是，當分詞器的輸入是文本列表（或在我們的情況下，是單詞列表的列表）時，word_ids() 函數需要獲取我們想要單詞 ID 的示例索引，因此我們也添加了這一點："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(dataset):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        dataset['words'], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = dataset['labels']\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(word_ids, labels))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 顯示前 first_n_data 筆資料\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.DataFrame(tokenized_dataset[data_cat].select(range(first_n_data)))\n",
    "\n",
    "# token_type_ids: 同常用於多句子，因為我們只有單一句子，所以都會是 0\n",
    "# attention_mask: 1 表示該 token 是真實的，0 表示是 padding token，BERT 只會關注 1 的 token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 數據整理\n",
    "\n",
    "我們不能僅使用 `DataCollatorWithPadding`，因為它只填充輸入（input IDs, attention mask, and token type IDs）。在這裡，我們的標籤應該以與輸入完全相同的方式進行填充，以保持相同的大小，使用 -100 作為值，以便在損失計算中忽略相應的預測。\n",
    "\n",
    "這一切都由 `DataCollatorForTokenClassification` 完成。與 `DataCollatorWithPadding` 一樣，它需要使用預處理輸入的分詞器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 展示 DataCollatorForTokenClassification 的輸出, 標籤以 -100 表示 padding\n",
    "batch = data_collator([tokenized_dataset[data_cat][i] for i in range(first_n_data)])\n",
    "pprint(batch[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型評估函數\n",
    "\n",
    "在訓練過程中包含度量標準通常有助於評估模型的性能。您可以使用 Evaluate 庫快速加載評估方法。對於這個任務，請加載 [seqeval](https://huggingface.co/docs/evaluate/a_quick_tour) 框架。Seqeval 實際上會生成多個分數：precision, recall, F1, 和 accuracy。\n",
    "\n",
    "* Precision: 精確率，是指所有被標記為正的樣本中實際為正的比例。\n",
    "\n",
    "$\\ Precision = \\frac{\\text{correctly classified actual positives}}{\\text{everything classified as positives}} = \\frac{TP}{TP + FP} $\n",
    "\n",
    "* Recall: 召回率，是指所有實際為正的樣本中被標記為正的比例。\n",
    "\n",
    "$\\ Recall = \\frac{\\text{correctly classified actual positives}}{\\text{all actual positives}} = \\frac{TP}{TP + FN} $\n",
    "\n",
    "* F1: F1 值是精確率和召回率的調和平均值，用於綜合考慮精確率和召回率。\n",
    "\n",
    "$\\ F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall} $\n",
    "\n",
    "* Accuracy: 準確率，是指所有被正確分類的樣本數量與總樣本數量之比。\n",
    "\n",
    "$\\ Accuracy = \\frac{\\text{correctly classifications}}{\\text{total classifications}} = \\frac{TP + TN}{TP + TN + FP + FN} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    # Unpack logits and labels from the input\n",
    "    logits, labels = eval_preds\n",
    "\n",
    "    # Convert logits to the index of the maximum logit value\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Map predictions and labels to their corresponding label names, ignoring padding (-100)\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    # Compute evaluation metrics using seqeval\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    # Return the computed metrics\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練模型\n",
    "\n",
    "您現在可以開始訓練您的模型了！使用 AutoModelForTokenClassification 加載預訓練的模型，並指定預期標籤的數量和標籤映射："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "  config.model_name,\n",
    "  num_labels=config.num_tags,\n",
    "  ignore_mismatched_sizes=True, # 忽略不匹配的大小\n",
    "  id2label=config.id2tag,\n",
    "  label2id=config.tag2id,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir='sample_data/train_output',\n",
    "  learning_rate=config.lr,\n",
    "  per_device_train_batch_size=config.train_batch_size,\n",
    "  per_device_eval_batch_size=config.eval_batch_size,\n",
    "  num_train_epochs=config.epochs,\n",
    "  weight_decay=config.weight_decay,\n",
    "  eval_strategy='epoch', # 每個 epoch 評估一次\n",
    "  save_strategy='epoch', # 每個 epoch 儲存一次\n",
    "  load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=tokenized_dataset['train'],\n",
    "  eval_dataset=tokenized_dataset['test'],\n",
    "  data_collator=data_collator,\n",
    "  tokenizer=tokenizer,\n",
    "  compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 開始訓練，這可能需要一些時間\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 儲存模型\n",
    "trainer.save_model(config.saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 載入模型\n",
    "classifier = pipeline(\n",
    "  task=\"token-classification\",\n",
    "  model=config.saved_model_path,\n",
    "  device=device,)\n",
    "\n",
    "# 顯示預測結果\n",
    "print(f'輸入: {test_text}')\n",
    "show_prediction(test_text, classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
