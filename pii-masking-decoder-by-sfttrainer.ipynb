{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# transformers not support NumPy 2.0 yet\n",
    "!pip install -q numpy~=1.26.4 transformers~=4.46.2\n",
    "!pip install -q datasets~=3.2.0 pydantic~=2.10.4\n",
    "!pip install -q peft~=0.14.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練 PII 遮掩模型\n",
    "\n",
    "在這個筆記本中，我們將展示如何使用 `transformers` 套件訓練 PII (個人識別資訊) 遮掩模型。我們將使用 `transformers` 套件中的 `SFTTrainer` ([Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/sft_trainer)) 類別來訓練模型，。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonas_cheng/.pyenv/versions/learning-transformer-with-huggingface-3.11.2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import garbage collector\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "  AutoTokenizer,\n",
    "  AutoModelForCausalLM,\n",
    "  TrainingArguments,\n",
    "  Trainer,\n",
    ")\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "  DataCollatorForSeq2Seq,\n",
    "  pipeline,\n",
    ")\n",
    "\n",
    "from typing import Any\n",
    "from pydantic import BaseModel\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "\n",
    "# 載入 PEFT 相關套件\n",
    "from peft import LoraConfig, TaskType, PeftModel, get_peft_model\n",
    "# 載入 SFTTrainer 相關套件\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "# 檢查是否有 GPU 可以使用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下載資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# The full `train` split, only 50% of dataset\n",
    "split = \"train[:50%]\" if device.type != 'mps' else \"train[:1%]\"\n",
    "immutable_dataset = load_dataset(\"ai4privacy/pii-masking-65k\", split=split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料包含什麼？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['masked_text', 'unmasked_text', 'token_entity_labels', 'tokenised_unmasked_text'],\n",
       "        num_rows: 172\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['masked_text', 'unmasked_text', 'token_entity_labels', 'tokenised_unmasked_text'],\n",
       "        num_rows: 43\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['masked_text', 'unmasked_text', 'token_entity_labels', 'tokenised_unmasked_text'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reserve 0.05% of the training set for testing\n",
    "test_dataset = immutable_dataset.train_test_split(\n",
    "  test_size=0.0005, # 0.05% of the data is used for testing\n",
    "  shuffle=False, # Ensure that train and validation sets are the same across runs\n",
    "  )\n",
    "# Split into 80% training and 20% validation sets\n",
    "train_dataset = test_dataset['train'].train_test_split(\n",
    "  test_size=0.2, # 20% of the data is used for validation\n",
    "  shuffle=False, # Ensure that train and test sets are the same across runs\n",
    "  )\n",
    "immutable_dataset = DatasetDict({\n",
    "  'train': train_dataset['train'],\n",
    "  'validation': train_dataset['test'],\n",
    "  'test': test_dataset['test'],\n",
    "  })\n",
    "# 顯示原始資料\n",
    "immutable_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['masked_text', 'unmasked_text'],\n",
       "        num_rows: 172\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['masked_text', 'unmasked_text'],\n",
       "        num_rows: 43\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['masked_text', 'unmasked_text'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保留必要 features: 'masked_text', 'unmasked_text'\n",
    "dataset = immutable_dataset.remove_columns(['token_entity_labels', 'tokenised_unmasked_text'])\n",
    "# 顯示處理後的資料\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>masked_text</th>\n",
       "      <th>unmasked_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[PREFIX_1] [FIRSTNAME_1] [MIDDLENAME_1] [LASTNAME_1], as a [JOBDESCRIPTOR_1] [JOBTITLE_1] at [COMPANY_NAME_1], your knowledge of change management is vital for our company's transformation. We request you to create a change management strategy.</td>\n",
       "      <td>Mr. Adolphus Reagan Ziemann, as a Central Principal Applications Executive at McLaughlin, Nader and Purdy, your knowledge of change management is vital for our company's transformation. We request you to create a change management strategy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hello [FIRSTNAME_1], would you please investigate the potential fallouts associated with the revisions in the [JOBAREA_1] department? Please incorporate your findings in your management strategy required previously.</td>\n",
       "      <td>Hello Hannah, would you please investigate the potential fallouts associated with the revisions in the Security department? Please incorporate your findings in your management strategy required previously.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We also request a review of our policies with respect to the upcoming changes and to bring in your expertise in case a policy change is advised. You can communicate the updates via email at [EMAIL_1].</td>\n",
       "      <td>We also request a review of our policies with respect to the upcoming changes and to bring in your expertise in case a policy change is advised. You can communicate the updates via email at Bartholome_Goldner85@yahoo.com.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                            masked_text  \\\n",
       "0  [PREFIX_1] [FIRSTNAME_1] [MIDDLENAME_1] [LASTNAME_1], as a [JOBDESCRIPTOR_1] [JOBTITLE_1] at [COMPANY_NAME_1], your knowledge of change management is vital for our company's transformation. We request you to create a change management strategy.   \n",
       "1                               Hello [FIRSTNAME_1], would you please investigate the potential fallouts associated with the revisions in the [JOBAREA_1] department? Please incorporate your findings in your management strategy required previously.   \n",
       "2                                              We also request a review of our policies with respect to the upcoming changes and to bring in your expertise in case a policy change is advised. You can communicate the updates via email at [EMAIL_1].   \n",
       "\n",
       "                                                                                                                                                                                                                                      unmasked_text  \n",
       "0  Mr. Adolphus Reagan Ziemann, as a Central Principal Applications Executive at McLaughlin, Nader and Purdy, your knowledge of change management is vital for our company's transformation. We request you to create a change management strategy.  \n",
       "1                                     Hello Hannah, would you please investigate the potential fallouts associated with the revisions in the Security department? Please incorporate your findings in your management strategy required previously.  \n",
       "2                     We also request a review of our policies with respect to the upcoming changes and to bring in your expertise in case a policy change is advised. You can communicate the updates via email at Bartholome_Goldner85@yahoo.com.  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 顯示前 first_n_data 筆資料\n",
    "first_n_data = 3\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.DataFrame(dataset['train'].select(range(first_n_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 訓練相關設定, 利用降低 batch size 提高 gradient accumulation steps 來節省記憶體\n",
    "class Config(BaseModel):\n",
    "  model_name: str = 'microsoft/Phi-3.5-mini-instruct'\n",
    "  torch_dtype: Any = torch.bfloat16 # 半精度浮點數\n",
    "  adam_epsilon: float = 1e-4 # 當使用半精度浮點數時，需要設定較大的 adam epsilon\n",
    "  saved_model_path: str = 'sample_data/saved_encoder_model' # path to save the trained model\n",
    "  saved_lora_path: str = 'sample_data/saved_lora_model' # path to save the trained LORA model\n",
    "  train_batch_size: int = 2 # size of the input batch in training\n",
    "  eval_batch_size: int = 2 # size of the input batch in evaluation\n",
    "  gradient_accumulation_steps: int = 2 # number of updates steps to accumulate before performing a backward/update pass\n",
    "  epochs: int = 3 # number of times to iterate over the entire training dataset\n",
    "  lr: float = 2e-5 # learning rate, controls how fast or slow the model learns\n",
    "  weight_decay: float = 0.01 # weight decay, helps the model stay simple and avoid overfitting by penalizing large weights.\n",
    "\n",
    "  # LORA 相關設定\n",
    "  rank: int = 128 # rank of the PEFT model\n",
    "\n",
    "config = Config(\n",
    "  torch_dtype=torch.bfloat16 if device.type != 'mps' else torch.float16 # MPS 需要使用 torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 先觀察 Fine-tuning 前的表現\n",
    "\n",
    "### 載入 Tokenizer\n",
    "\n",
    "* 如果沒有定義 `pad_token`，請定義一個 `pad_token`，並將其加入 Tokenizer 中。\n",
    "* 如果 `padding_side` 不是 `right`，請將其設定為 `right`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaTokenizerFast(name_or_path='microsoft/Phi-3.5-mini-instruct', vocab_size=32000, model_max_length=131072, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '<|endoftext|>', 'unk_token': '<unk>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t32000: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32001: AddedToken(\"<|assistant|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32002: AddedToken(\"<|placeholder1|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32003: AddedToken(\"<|placeholder2|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32004: AddedToken(\"<|placeholder3|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32005: AddedToken(\"<|placeholder4|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32006: AddedToken(\"<|system|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32007: AddedToken(\"<|end|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32008: AddedToken(\"<|placeholder5|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32009: AddedToken(\"<|placeholder6|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32010: AddedToken(\"<|user|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 透過預訓練模型取得 Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  config.model_name,\n",
    ")\n",
    "# 檢視 Tokenizer，是否存在 padding token 及 padding side 等資訊\n",
    "pprint(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 設定 Padding Side ===\n",
      "LlamaTokenizerFast(name_or_path='microsoft/Phi-3.5-mini-instruct', vocab_size=32000, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '<|endoftext|>', 'unk_token': '<unk>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t32000: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32001: AddedToken(\"<|assistant|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32002: AddedToken(\"<|placeholder1|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32003: AddedToken(\"<|placeholder2|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32004: AddedToken(\"<|placeholder3|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32005: AddedToken(\"<|placeholder4|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32006: AddedToken(\"<|system|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32007: AddedToken(\"<|end|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32008: AddedToken(\"<|placeholder5|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32009: AddedToken(\"<|placeholder6|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32010: AddedToken(\"<|user|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Add pad_token to the tokenizer\n",
    "if tokenizer.pad_token is None:\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  print('=== 設定 Padding Token ===')\n",
    "  pprint(tokenizer)\n",
    "# Make sure padding_side is 'right'\n",
    "if tokenizer.padding_side != 'right':\n",
    "  tokenizer.padding_side = 'right'\n",
    "  print('=== 設定 Padding Side ===')\n",
    "  pprint(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入預訓練模型\n",
    "\n",
    "由於 GPU 記憶體有限，我們將使用半精度進行模型 Fine-tuning。這邊需要留意，使用半精度進行 Fine-tuning 時，`TrainingArguments` 中的 `adam_epsilon` 需要設定為 `1e-4`。預設的 `adam_epsilon` 是 `1e-8`，這個值在半精度訓練時會出現問題。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:14<00:00,  7.48s/it]\n"
     ]
    }
   ],
   "source": [
    "# 半精度浮點數訓練\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  config.model_name,\n",
    "  torch_dtype=config.torch_dtype,\n",
    "  low_cpu_mem_usage=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight: torch.float16\n",
      "model.layers.0.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.0.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.0.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.0.mlp.down_proj.weight: torch.float16\n",
      "model.layers.0.input_layernorm.weight: torch.float16\n",
      "model.layers.0.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.1.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.1.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.1.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.1.mlp.down_proj.weight: torch.float16\n",
      "model.layers.1.input_layernorm.weight: torch.float16\n",
      "model.layers.1.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.2.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.2.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.2.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.2.mlp.down_proj.weight: torch.float16\n",
      "model.layers.2.input_layernorm.weight: torch.float16\n",
      "model.layers.2.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.3.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.3.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.3.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.3.mlp.down_proj.weight: torch.float16\n",
      "model.layers.3.input_layernorm.weight: torch.float16\n",
      "model.layers.3.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.4.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.4.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.4.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.4.mlp.down_proj.weight: torch.float16\n",
      "model.layers.4.input_layernorm.weight: torch.float16\n",
      "model.layers.4.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.5.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.5.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.5.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.5.mlp.down_proj.weight: torch.float16\n",
      "model.layers.5.input_layernorm.weight: torch.float16\n",
      "model.layers.5.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.6.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.6.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.6.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.6.mlp.down_proj.weight: torch.float16\n",
      "model.layers.6.input_layernorm.weight: torch.float16\n",
      "model.layers.6.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.7.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.7.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.7.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.7.mlp.down_proj.weight: torch.float16\n",
      "model.layers.7.input_layernorm.weight: torch.float16\n",
      "model.layers.7.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.8.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.8.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.8.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.8.mlp.down_proj.weight: torch.float16\n",
      "model.layers.8.input_layernorm.weight: torch.float16\n",
      "model.layers.8.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.9.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.9.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.9.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.9.mlp.down_proj.weight: torch.float16\n",
      "model.layers.9.input_layernorm.weight: torch.float16\n",
      "model.layers.9.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.10.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.10.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.10.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.10.mlp.down_proj.weight: torch.float16\n",
      "model.layers.10.input_layernorm.weight: torch.float16\n",
      "model.layers.10.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.11.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.11.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.11.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.11.mlp.down_proj.weight: torch.float16\n",
      "model.layers.11.input_layernorm.weight: torch.float16\n",
      "model.layers.11.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.12.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.12.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.12.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.12.mlp.down_proj.weight: torch.float16\n",
      "model.layers.12.input_layernorm.weight: torch.float16\n",
      "model.layers.12.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.13.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.13.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.13.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.13.mlp.down_proj.weight: torch.float16\n",
      "model.layers.13.input_layernorm.weight: torch.float16\n",
      "model.layers.13.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.14.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.14.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.14.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.14.mlp.down_proj.weight: torch.float16\n",
      "model.layers.14.input_layernorm.weight: torch.float16\n",
      "model.layers.14.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.15.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.15.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.15.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.15.mlp.down_proj.weight: torch.float16\n",
      "model.layers.15.input_layernorm.weight: torch.float16\n",
      "model.layers.15.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.16.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.16.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.16.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.16.mlp.down_proj.weight: torch.float16\n",
      "model.layers.16.input_layernorm.weight: torch.float16\n",
      "model.layers.16.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.17.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.17.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.17.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.17.mlp.down_proj.weight: torch.float16\n",
      "model.layers.17.input_layernorm.weight: torch.float16\n",
      "model.layers.17.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.18.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.18.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.18.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.18.mlp.down_proj.weight: torch.float16\n",
      "model.layers.18.input_layernorm.weight: torch.float16\n",
      "model.layers.18.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.19.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.19.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.19.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.19.mlp.down_proj.weight: torch.float16\n",
      "model.layers.19.input_layernorm.weight: torch.float16\n",
      "model.layers.19.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.20.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.20.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.20.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.20.mlp.down_proj.weight: torch.float16\n",
      "model.layers.20.input_layernorm.weight: torch.float16\n",
      "model.layers.20.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.21.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.21.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.21.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.21.mlp.down_proj.weight: torch.float16\n",
      "model.layers.21.input_layernorm.weight: torch.float16\n",
      "model.layers.21.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.22.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.22.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.22.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.22.mlp.down_proj.weight: torch.float16\n",
      "model.layers.22.input_layernorm.weight: torch.float16\n",
      "model.layers.22.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.23.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.23.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.23.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.23.mlp.down_proj.weight: torch.float16\n",
      "model.layers.23.input_layernorm.weight: torch.float16\n",
      "model.layers.23.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.24.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.24.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.24.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.24.mlp.down_proj.weight: torch.float16\n",
      "model.layers.24.input_layernorm.weight: torch.float16\n",
      "model.layers.24.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.25.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.25.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.25.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.25.mlp.down_proj.weight: torch.float16\n",
      "model.layers.25.input_layernorm.weight: torch.float16\n",
      "model.layers.25.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.26.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.26.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.26.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.26.mlp.down_proj.weight: torch.float16\n",
      "model.layers.26.input_layernorm.weight: torch.float16\n",
      "model.layers.26.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.27.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.27.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.27.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.27.mlp.down_proj.weight: torch.float16\n",
      "model.layers.27.input_layernorm.weight: torch.float16\n",
      "model.layers.27.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.28.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.28.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.28.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.28.mlp.down_proj.weight: torch.float16\n",
      "model.layers.28.input_layernorm.weight: torch.float16\n",
      "model.layers.28.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.29.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.29.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.29.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.29.mlp.down_proj.weight: torch.float16\n",
      "model.layers.29.input_layernorm.weight: torch.float16\n",
      "model.layers.29.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.30.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.30.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.30.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.30.mlp.down_proj.weight: torch.float16\n",
      "model.layers.30.input_layernorm.weight: torch.float16\n",
      "model.layers.30.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.31.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.31.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.31.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.31.mlp.down_proj.weight: torch.float16\n",
      "model.layers.31.input_layernorm.weight: torch.float16\n",
      "model.layers.31.post_attention_layernorm.weight: torch.float16\n",
      "model.norm.weight: torch.float16\n",
      "lm_head.weight: torch.float16\n"
     ]
    }
   ],
   "source": [
    "# 獲取模型參數名稱及型態，確認是否使用半精度浮點數\n",
    "for name, param in model.named_parameters():\n",
    "  print(f'{name}: {param.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 詠唱格式化 (Prompt Formatting)\n",
    "\n",
    "先定義我們的詠唱 (Prompt) 格式。為此，我們將創建一個格式化函數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "system_message = 'Given the information below, mask the personal identifiable information.'\n",
    "\n",
    "def instruction_formatter(x):\n",
    "  text = f'''\n",
    "    <|system|> {system_message}.\n",
    "    <|user|> {x['unmasked_text']}\n",
    "    <|assistant|>\n",
    "  '''\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning 前的表現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 載入預訓練模型\n",
    "generator = pipeline(\n",
    "  task='text-generation',\n",
    "  model=model,\n",
    "  tokenizer=tokenizer,\n",
    "  device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonas_cheng/.pyenv/versions/learning-transformer-with-huggingface-3.11.2/lib/python3.11/site-packages/transformers/pytorch_utils.py:325: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    <|system|> Given the information below, mask the personal identifiable information.\n",
      "    <|user|> In order to finalize your registration for our Animal-assisted Therapy Program, please make the payment of Guarani 428.22. This can be made to our Auto Loan Account with account number 58114957. Once the payment is received, we will reach out to you with more information. Please use the provided contact (364) 467-8496 x139 for any queries related to your registration. Please use your GH5C2WDB7WVB30204 as a reference.\n",
      "    <|assistant|>\n",
      "   To complete participation in an animal assistance theraputic program: Kindly process remittance worth equivalent value [GUARANI_AMOUNT] using designated auto loan facility identified by unique identifier '[AUTOPLACEMENT]. Post transaction confirmation; communication shall ensue providing additional details pertinent thereof. For enquiries regarding said engagement utilise telecommunications channel labelled under code/reference tagged herein - ['REFERENCE'].  \n"
     ]
    }
   ],
   "source": [
    "# 顯示預訓練模型預測結果\n",
    "input = instruction_formatter(dataset['test'][0])\n",
    "response = generator(\n",
    "  input,\n",
    "  max_new_tokens=128, # 限制最大生成字數\n",
    "  repetition_penalty=1.5, # 重複機率, 1~2 之間, 1.0 (no penalty), 2.0 (maximum penalty)\n",
    ")\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練模型\n",
    "\n",
    "隨著 `trl` 的最新版本發布，現在支持流行的指令 (instruction) 和對話 (conversation) 數據集格式。這意味著我們只需要將數據集轉換為支持的格式之一，`trl` 會處理其餘的部分。這些格式包括：\n",
    "\n",
    "* 指令格式 instruction format\n",
    "\n",
    "```json\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "```\n",
    "\n",
    "* 對話格式 conversational format\n",
    "\n",
    "```json\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料預處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def create_conversation(dataset):\n",
    "  rows = []\n",
    "  unmasked_texts = dataset['unmasked_text']\n",
    "  masked_texts = dataset['masked_text']\n",
    "  for unmasked_text, masked_text in zip(unmasked_texts, masked_texts):\n",
    "    rows.append([\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": unmasked_text},\n",
    "        {\"role\": \"assistant\", \"content\": masked_text}\n",
    "      ],)\n",
    "  return {'messages': rows}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "conversation_dataset = dataset.map(\n",
    "  create_conversation,\n",
    "  batched=True,\n",
    "  remove_columns=dataset['train'].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'content': 'Given the information below, mask the personal identifiable information.', 'role': 'system'}, {'content': 'Mr. Adolphus Reagan Ziemann, as a Central Principal Applications Executive at McLaughlin, Nader and Purdy, your knowledge of change management is vital for our company's transformation. We request you to create a change management strategy.', 'role': 'user'}, {'content': '[PREFIX_1] [FIRSTNAME_1] [MIDDLENAME_1] [LASTNAME_1], as a [JOBDESCRIPTOR_1] [JOBTITLE_1] at [COMPANY_NAME_1], your knowledge of change management is vital for our company's transformation. We request you to create a change management strategy.', 'role': 'assistant'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'content': 'Given the information below, mask the personal identifiable information.', 'role': 'system'}, {'content': 'Hello Hannah, would you please investigate the potential fallouts associated with the revisions in the Security department? Please incorporate your findings in your management strategy required previously.', 'role': 'user'}, {'content': 'Hello [FIRSTNAME_1], would you please investigate the potential fallouts associated with the revisions in the [JOBAREA_1] department? Please incorporate your findings in your management strategy required previously.', 'role': 'assistant'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'content': 'Given the information below, mask the personal identifiable information.', 'role': 'system'}, {'content': 'We also request a review of our policies with respect to the upcoming changes and to bring in your expertise in case a policy change is advised. You can communicate the updates via email at Bartholome_Goldner85@yahoo.com.', 'role': 'user'}, {'content': 'We also request a review of our policies with respect to the upcoming changes and to bring in your expertise in case a policy change is advised. You can communicate the updates via email at [EMAIL_1].', 'role': 'assistant'}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 messages\n",
       "0  [{'content': 'Given the information below, mask the personal identifiable information.', 'role': 'system'}, {'content': 'Mr. Adolphus Reagan Ziemann, as a Central Principal Applications Executive at McLaughlin, Nader and Purdy, your knowledge of change management is vital for our company's transformation. We request you to create a change management strategy.', 'role': 'user'}, {'content': '[PREFIX_1] [FIRSTNAME_1] [MIDDLENAME_1] [LASTNAME_1], as a [JOBDESCRIPTOR_1] [JOBTITLE_1] at [COMPANY_NAME_1], your knowledge of change management is vital for our company's transformation. We request you to create a change management strategy.', 'role': 'assistant'}]\n",
       "1                                                                  [{'content': 'Given the information below, mask the personal identifiable information.', 'role': 'system'}, {'content': 'Hello Hannah, would you please investigate the potential fallouts associated with the revisions in the Security department? Please incorporate your findings in your management strategy required previously.', 'role': 'user'}, {'content': 'Hello [FIRSTNAME_1], would you please investigate the potential fallouts associated with the revisions in the [JOBAREA_1] department? Please incorporate your findings in your management strategy required previously.', 'role': 'assistant'}]\n",
       "2                                                                 [{'content': 'Given the information below, mask the personal identifiable information.', 'role': 'system'}, {'content': 'We also request a review of our policies with respect to the upcoming changes and to bring in your expertise in case a policy change is advised. You can communicate the updates via email at Bartholome_Goldner85@yahoo.com.', 'role': 'user'}, {'content': 'We also request a review of our policies with respect to the upcoming changes and to bring in your expertise in case a policy change is advised. You can communicate the updates via email at [EMAIL_1].', 'role': 'assistant'}]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 顯示前 first_n_data 筆資料\n",
    "first_n_data = 3\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.DataFrame(conversation_dataset['train'].select(range(first_n_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'content': 'Given the information below, mask the personal '\n",
      "                          'identifiable information.',\n",
      "               'role': 'system'},\n",
      "              {'content': 'Mr. Adolphus Reagan Ziemann, as a Central Principal '\n",
      "                          'Applications Executive at McLaughlin, Nader and '\n",
      "                          'Purdy, your knowledge of change management is vital '\n",
      "                          \"for our company's transformation. We request you to \"\n",
      "                          'create a change management strategy.',\n",
      "               'role': 'user'},\n",
      "              {'content': '[PREFIX_1] [FIRSTNAME_1] [MIDDLENAME_1] '\n",
      "                          '[LASTNAME_1], as a [JOBDESCRIPTOR_1] [JOBTITLE_1] '\n",
      "                          'at [COMPANY_NAME_1], your knowledge of change '\n",
      "                          \"management is vital for our company's \"\n",
      "                          'transformation. We request you to create a change '\n",
      "                          'management strategy.',\n",
      "               'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "# 顯示單筆方便閱讀\n",
    "pprint(conversation_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA 的訓練策略 - 降維打擊\n",
    "\n",
    "LoRA（Low-Rank Adaptation）是一種用於訓練大型語言模型的技術，旨在提高訓練效率並減少計算資源的需求。以下是為何需要透過LoRA訓練的一些原因：\n",
    "\n",
    "降低計算成本：LoRA 通過將模型的權重矩陣分解為低秩矩陣，顯著減少了參數的數量，從而降低了計算成本和內存需求。\n",
    "\n",
    "加速訓練速度：由於參數數量減少，LoRA 可以加速模型的訓練過程，使得在相同的硬件資源下能夠更快地完成訓練。\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/lora_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 3,821,079,552, Trainable Parameters: 3,821,079,552\n"
     ]
    }
   ],
   "source": [
    "# 查看預訓練模型可訓練的參數量，其數量相當龐大，所以需要透過 Low Rank Approximation (LORA) 來降低參數量\n",
    "print('Parameters: {:,}, Trainable Parameters: {:,}'.format(\n",
    "  model.num_parameters(),\n",
    "  model.num_parameters(only_trainable=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PEFT 配置\n",
    "\n",
    "`target_module`: 要降維的模型層，可以透過 `model.named_parameters` 查看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoraConfig(task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>,\n",
      "           peft_type=<PeftType.LORA: 'LORA'>,\n",
      "           auto_mapping=None,\n",
      "           base_model_name_or_path=None,\n",
      "           revision=None,\n",
      "           inference_mode=False,\n",
      "           r=128,\n",
      "           target_modules={'qkv_proj'},\n",
      "           exclude_modules=None,\n",
      "           lora_alpha=8,\n",
      "           lora_dropout=0.0,\n",
      "           fan_in_fan_out=False,\n",
      "           bias='none',\n",
      "           use_rslora=False,\n",
      "           modules_to_save=None,\n",
      "           init_lora_weights=True,\n",
      "           layers_to_transform=None,\n",
      "           layers_pattern=None,\n",
      "           rank_pattern={},\n",
      "           alpha_pattern={},\n",
      "           megatron_config=None,\n",
      "           megatron_core='megatron.core',\n",
      "           loftq_config={},\n",
      "           eva_config=None,\n",
      "           use_dora=False,\n",
      "           layer_replication=None,\n",
      "           runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False),\n",
      "           lora_bias=False)\n"
     ]
    }
   ],
   "source": [
    "# PEFT 配置\n",
    "lora_config = LoraConfig(\n",
    "  task_type=TaskType.CAUSAL_LM,\n",
    "  r=config.rank,\n",
    "  target_modules=['qkv_proj'],\n",
    ")\n",
    "pprint(lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 取得 PEFT 模型\n",
    "\n",
    "搭配預訓模型及 PEFT 配置，我們可以取得 PEFT 模型。我們可以觀察受到降維影響的模型層。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonas_cheng/.pyenv/versions/learning-transformer-with-huggingface-3.11.2/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "# 取得 PEFT 模型\n",
    "peft_model = get_peft_model(\n",
    "  model, # 預訓練模型\n",
    "  lora_config, # PEFT 配置\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 取得 PEFT 模型, 觀察受 PEFT 影響的模型參數\n",
    "peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 調整 PEFT 模型精度\n",
    "\n",
    "PEFT 模型的精度是 `torch.float32`，我們可以透過 `model.half()` 將其轉換為半精度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 獲取 PERF 模型參數名稱及型態，確認是否使用半精度浮點數\n",
    "for name, param in peft_model.named_parameters():\n",
    "  print(f'{name}: {param.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 同樣採用半精度浮點數訓練\n",
    "peft_model = peft_model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 50,331,648 || all params: 3,871,411,200 || trainable%: 1.3001\n"
     ]
    }
   ],
   "source": [
    "# 查看可訓練的參數量\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定義訓練參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 55 examples [00:00, 189.05 examples/s]\n",
      "Generating train split: 14 examples [00:00, 510.03 examples/s]\n",
      "/Users/jonas_cheng/.pyenv/versions/learning-transformer-with-huggingface-3.11.2/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = SFTConfig(\n",
    "  output_dir='sample_data/train_output_pii_masking',\n",
    "  learning_rate=config.lr,\n",
    "  per_device_train_batch_size=config.train_batch_size,\n",
    "  per_device_eval_batch_size=config.eval_batch_size,\n",
    "  gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "  num_train_epochs=config.epochs,\n",
    "  weight_decay=config.weight_decay,\n",
    "  eval_strategy='epoch', # 每個 epoch 評估一次\n",
    "  save_strategy='epoch', # 每個 epoch 儲存一次\n",
    "  load_best_model_at_end=True,\n",
    "  report_to='none', # Disable wandb on colab\n",
    "  adam_epsilon=config.adam_epsilon, # 當使用半精度浮點數時，需要設定較大的 adam epsilon\n",
    "  packing=True,\n",
    "  max_seq_length=1024, # max sequence length for model and packing of the dataset\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    peft_config=lora_config,\n",
    "    train_dataset=conversation_dataset['train'],\n",
    "    eval_dataset=conversation_dataset['validation'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 開始訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/42 : < :, Epoch 0.07/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 18.09 GB, other allocations: 1.92 MB, max allowed: 18.13 GB). Tried to allocate 125.01 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 開始訓練，這可能需要一些時間\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/learning-transformer-with-huggingface-3.11.2/lib/python3.11/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/learning-transformer-with-huggingface-3.11.2/lib/python3.11/site-packages/transformers/trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2479\u001b[0m )\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.pyenv/versions/learning-transformer-with-huggingface-3.11.2/lib/python3.11/site-packages/transformers/trainer.py:3579\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3578\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3579\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3581\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3584\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3585\u001b[0m ):\n",
      "File \u001b[0;32m~/.pyenv/versions/learning-transformer-with-huggingface-3.11.2/lib/python3.11/site-packages/transformers/trainer.py:3633\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3631\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3632\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3633\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3634\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3635\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/learning-transformer-with-huggingface-3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/learning-transformer-with-huggingface-3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/learning-transformer-with-huggingface-3.11.2/lib/python3.11/site-packages/peft/peft_model.py:1719\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1718\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1719\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1730\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1732\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/learning-transformer-with-huggingface-3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/learning-transformer-with-huggingface-3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/learning-transformer-with-huggingface-3.11.2/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/learning-transformer-with-huggingface-3.11.2/lib/python3.11/site-packages/transformers/models/phi3/modeling_phi3.py:1279\u001b[0m, in \u001b[0;36mPhi3ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1277\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1279\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloss_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1282\u001b[0m     output \u001b[38;5;241m=\u001b[39m (logits,) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/.pyenv/versions/learning-transformer-with-huggingface-3.11.2/lib/python3.11/site-packages/transformers/loss/loss_utils.py:46\u001b[0m, in \u001b[0;36mForCausalLMLoss\u001b[0;34m(logits, labels, vocab_size, num_items_in_batch, ignore_index, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Enable model parallelism\u001b[39;00m\n\u001b[1;32m     45\u001b[0m shift_labels \u001b[38;5;241m=\u001b[39m shift_labels\u001b[38;5;241m.\u001b[39mto(shift_logits\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 46\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mfixed_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshift_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.pyenv/versions/learning-transformer-with-huggingface-3.11.2/lib/python3.11/site-packages/transformers/loss/loss_utils.py:26\u001b[0m, in \u001b[0;36mfixed_cross_entropy\u001b[0;34m(source, target, num_items_in_batch, ignore_index, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfixed_cross_entropy\u001b[39m(source, target, num_items_in_batch: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, ignore_index: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     25\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 26\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reduction \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     28\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m num_items_in_batch\n",
      "File \u001b[0;32m~/.pyenv/versions/learning-transformer-with-huggingface-3.11.2/lib/python3.11/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 18.09 GB, other allocations: 1.92 MB, max allowed: 18.13 GB). Tried to allocate 125.01 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "# 開始訓練，這可能需要一些時間\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存 LoRA 模型參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 保存 Lora 参数\n",
    "peft_model.save_pretrained(config.saved_lora_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 合併 LoRA 模型參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合併原始模型和 Lora 参数\n",
    "new_model = PeftModel.from_pretrained(model, config.saved_lora_path)\n",
    "\n",
    "print(\"=== 合併前的模型結構 ===\")\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 合併並卸載 Lora 参数\n",
    "new_model.merge_and_unload()\n",
    "\n",
    "print(\"=== 合併後的模型結構 ===\")\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 保存合併後的模型\n",
    "new_model.save_pretrained(config.saved_model_path)\n",
    "tokenizer.save_pretrained(config.saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 釋放資源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 釋放 GPU 記憶體\n",
    "del new_model\n",
    "del trainer\n",
    "\n",
    "peft_model.to('cpu')\n",
    "del peft_model\n",
    "\n",
    "model.to('cpu')\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評估模型\n",
    "\n",
    "### 載入微調後 Tokenizer\n",
    "\n",
    "從已經完成訓練的模型取得 Tokenizer，可以留意這個訓練時保存下來的 Tokenizer 仍保有訓練時的設定，包涵 `pad_token` 和 `padding_side`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  config.saved_model_path\n",
    ")\n",
    "# 檢視 Tokenizer\n",
    "pprint(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入微調後模型\n",
    "\n",
    "以半精度浮點數載入已經完成訓練的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 以半精度浮點數載入已經完成訓練的模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  config.saved_model_path,\n",
    "  low_cpu_mem_usage=True,\n",
    "  torch_dtype=config.torch_dtype,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning 後的表現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 載入新模型\n",
    "generator = pipeline(\n",
    "  task='text-generation',\n",
    "  model=model,\n",
    "  tokenizer=tokenizer,\n",
    "  device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 顯示新模型預測結果\n",
    "input = instruction_formatter(dataset['test'][0])\n",
    "response = generator(\n",
    "  input,\n",
    "  max_new_tokens=512, # 限制最大生成字數\n",
    "  repetition_penalty=1.5, # 重複機率, 1~2 之間, 1.0 (no penalty), 2.0 (maximum penalty)\n",
    ")\n",
    "print(response[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
