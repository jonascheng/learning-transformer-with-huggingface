{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# transformers not support NumPy 2.0 yet\n",
    "!pip install -q numpy~=1.26.4 transformers~=4.46.2\n",
    "!pip install -q datasets~=3.2.0 pydantic~=2.10.4\n",
    "!pip install -q peft~=0.14.0 trl~=0.13.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練 PII 遮掩模型\n",
    "\n",
    "在這個筆記本中，我們將展示如何使用 `transformers` 套件訓練 PII (個人識別資訊) 遮掩模型。我們將使用 `transformers` 套件中的 `SFTTrainer` ([Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/sft_trainer)) 類別來訓練模型，。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonas_cheng/.pyenv/versions/learning-transformer-with-huggingface-3.11.2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import garbage collector\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "  AutoTokenizer,\n",
    "  AutoModelForCausalLM,\n",
    ")\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "  pipeline,\n",
    ")\n",
    "\n",
    "from typing import Any\n",
    "from pydantic import BaseModel\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "\n",
    "# 載入 PEFT 相關套件\n",
    "from peft import LoraConfig, TaskType, PeftModel, get_peft_model\n",
    "# 載入 SFTTrainer 相關套件\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "# 檢查是否有 GPU 可以使用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下載資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# The full `train` split, only 50% of dataset\n",
    "split = \"train[:50%]\" if device.type != 'mps' else \"train[:1%]\"\n",
    "immutable_dataset = load_dataset(\"ai4privacy/pii-masking-65k\", split=split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料包含什麼？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['masked_text', 'unmasked_text', 'token_entity_labels', 'tokenised_unmasked_text'],\n",
       "        num_rows: 172\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['masked_text', 'unmasked_text', 'token_entity_labels', 'tokenised_unmasked_text'],\n",
       "        num_rows: 43\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['masked_text', 'unmasked_text', 'token_entity_labels', 'tokenised_unmasked_text'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reserve 0.05% of the training set for testing\n",
    "test_dataset = immutable_dataset.train_test_split(\n",
    "  test_size=0.0005, # 0.05% of the data is used for testing\n",
    "  shuffle=False, # Ensure that train and validation sets are the same across runs\n",
    "  )\n",
    "# Split into 80% training and 20% validation sets\n",
    "train_dataset = test_dataset['train'].train_test_split(\n",
    "  test_size=0.2, # 20% of the data is used for validation\n",
    "  shuffle=False, # Ensure that train and test sets are the same across runs\n",
    "  )\n",
    "immutable_dataset = DatasetDict({\n",
    "  'train': train_dataset['train'],\n",
    "  'validation': train_dataset['test'],\n",
    "  'test': test_dataset['test'],\n",
    "  })\n",
    "# 顯示原始資料\n",
    "immutable_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['masked_text', 'unmasked_text'],\n",
       "        num_rows: 172\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['masked_text', 'unmasked_text'],\n",
       "        num_rows: 43\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['masked_text', 'unmasked_text'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保留必要 features: 'masked_text', 'unmasked_text'\n",
    "dataset = immutable_dataset.remove_columns(['token_entity_labels', 'tokenised_unmasked_text'])\n",
    "# 顯示處理後的資料\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>masked_text</th>\n",
       "      <th>unmasked_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[PREFIX_1] [FIRSTNAME_1] [MIDDLENAME_1] [LASTNAME_1], as a [JOBDESCRIPTOR_1] [JOBTITLE_1] at [COMPANY_NAME_1], your knowledge of change management is vital for our company's transformation. We request you to create a change management strategy.</td>\n",
       "      <td>Mr. Adolphus Reagan Ziemann, as a Central Principal Applications Executive at McLaughlin, Nader and Purdy, your knowledge of change management is vital for our company's transformation. We request you to create a change management strategy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hello [FIRSTNAME_1], would you please investigate the potential fallouts associated with the revisions in the [JOBAREA_1] department? Please incorporate your findings in your management strategy required previously.</td>\n",
       "      <td>Hello Hannah, would you please investigate the potential fallouts associated with the revisions in the Security department? Please incorporate your findings in your management strategy required previously.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We also request a review of our policies with respect to the upcoming changes and to bring in your expertise in case a policy change is advised. You can communicate the updates via email at [EMAIL_1].</td>\n",
       "      <td>We also request a review of our policies with respect to the upcoming changes and to bring in your expertise in case a policy change is advised. You can communicate the updates via email at Bartholome_Goldner85@yahoo.com.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                            masked_text  \\\n",
       "0  [PREFIX_1] [FIRSTNAME_1] [MIDDLENAME_1] [LASTNAME_1], as a [JOBDESCRIPTOR_1] [JOBTITLE_1] at [COMPANY_NAME_1], your knowledge of change management is vital for our company's transformation. We request you to create a change management strategy.   \n",
       "1                               Hello [FIRSTNAME_1], would you please investigate the potential fallouts associated with the revisions in the [JOBAREA_1] department? Please incorporate your findings in your management strategy required previously.   \n",
       "2                                              We also request a review of our policies with respect to the upcoming changes and to bring in your expertise in case a policy change is advised. You can communicate the updates via email at [EMAIL_1].   \n",
       "\n",
       "                                                                                                                                                                                                                                      unmasked_text  \n",
       "0  Mr. Adolphus Reagan Ziemann, as a Central Principal Applications Executive at McLaughlin, Nader and Purdy, your knowledge of change management is vital for our company's transformation. We request you to create a change management strategy.  \n",
       "1                                     Hello Hannah, would you please investigate the potential fallouts associated with the revisions in the Security department? Please incorporate your findings in your management strategy required previously.  \n",
       "2                     We also request a review of our policies with respect to the upcoming changes and to bring in your expertise in case a policy change is advised. You can communicate the updates via email at Bartholome_Goldner85@yahoo.com.  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 顯示前 first_n_data 筆資料\n",
    "first_n_data = 3\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.DataFrame(dataset['train'].select(range(first_n_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練設定\n",
    "\n",
    "![](https://miro.medium.com/v2/0*HapPSei5sok65wcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 訓練相關設定, 利用降低 batch size 提高 gradient accumulation steps 來節省記憶體\n",
    "class Config(BaseModel):\n",
    "  model_name: str = 'microsoft/Phi-3.5-mini-instruct'\n",
    "  torch_dtype: Any = torch.bfloat16 # 半精度浮點數\n",
    "  adam_epsilon: float = 1e-4 # 當使用半精度浮點數時，需要設定較大的 adam epsilon\n",
    "  saved_model_path: str = 'sample_data/saved_encoder_model' # path to save the trained model\n",
    "  saved_lora_path: str = 'sample_data/saved_lora_model' # path to save the trained LORA model\n",
    "  train_batch_size: int = 2 # size of the input batch in training\n",
    "  eval_batch_size: int = 2 # size of the input batch in evaluation\n",
    "  gradient_accumulation_steps: int = 2 # number of updates steps to accumulate before performing a backward/update pass\n",
    "  epochs: int = 1 # number of times to iterate over the entire training dataset\n",
    "  lr: float = 2e-5 # learning rate, controls how fast or slow the model learns\n",
    "  weight_decay: float = 0.01 # weight decay, helps the model stay simple and avoid overfitting by penalizing large weights.\n",
    "\n",
    "  # LORA 相關設定\n",
    "  rank: int = 128 # rank of the PEFT model\n",
    "\n",
    "config = Config(\n",
    "  torch_dtype=torch.bfloat16 if device.type != 'mps' else torch.float16 # MPS 需要使用 torch.float16\n",
    "  epochs=5 if device.type != 'mps' else 1 # 方便在 Apple Silicon 上快速測試\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 先觀察 Fine-tuning 前的表現\n",
    "\n",
    "### 載入 Tokenizer\n",
    "\n",
    "* 如果沒有定義 `pad_token`，請定義一個 `pad_token`，並將其加入 Tokenizer 中。\n",
    "* 如果 `padding_side` 不是 `right`，請將其設定為 `right`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaTokenizerFast(name_or_path='microsoft/Phi-3.5-mini-instruct', vocab_size=32000, model_max_length=131072, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '<|endoftext|>', 'unk_token': '<unk>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t32000: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32001: AddedToken(\"<|assistant|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32002: AddedToken(\"<|placeholder1|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32003: AddedToken(\"<|placeholder2|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32004: AddedToken(\"<|placeholder3|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32005: AddedToken(\"<|placeholder4|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32006: AddedToken(\"<|system|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32007: AddedToken(\"<|end|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32008: AddedToken(\"<|placeholder5|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32009: AddedToken(\"<|placeholder6|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32010: AddedToken(\"<|user|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 透過預訓練模型取得 Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  config.model_name,\n",
    ")\n",
    "# 檢視 Tokenizer，是否存在 padding token 及 padding side 等資訊\n",
    "pprint(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 設定 Padding Side ===\n",
      "LlamaTokenizerFast(name_or_path='microsoft/Phi-3.5-mini-instruct', vocab_size=32000, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '<|endoftext|>', 'unk_token': '<unk>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t32000: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32001: AddedToken(\"<|assistant|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32002: AddedToken(\"<|placeholder1|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32003: AddedToken(\"<|placeholder2|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32004: AddedToken(\"<|placeholder3|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32005: AddedToken(\"<|placeholder4|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32006: AddedToken(\"<|system|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32007: AddedToken(\"<|end|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32008: AddedToken(\"<|placeholder5|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32009: AddedToken(\"<|placeholder6|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32010: AddedToken(\"<|user|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Add pad_token to the tokenizer\n",
    "if tokenizer.pad_token is None:\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  print('=== 設定 Padding Token ===')\n",
    "  pprint(tokenizer)\n",
    "# Make sure padding_side is 'right'\n",
    "if tokenizer.padding_side != 'right':\n",
    "  tokenizer.padding_side = 'right'\n",
    "  print('=== 設定 Padding Side ===')\n",
    "  pprint(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入預訓練模型\n",
    "\n",
    "由於 GPU 記憶體有限，我們將使用半精度進行模型 Fine-tuning。這邊需要留意，使用半精度進行 Fine-tuning 時，`TrainingArguments` 中的 `adam_epsilon` 需要設定為 `1e-4`。預設的 `adam_epsilon` 是 `1e-8`，這個值在半精度訓練時會出現問題。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:14<00:00,  7.37s/it]\n"
     ]
    }
   ],
   "source": [
    "# 半精度浮點數訓練\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  config.model_name,\n",
    "  torch_dtype=config.torch_dtype,\n",
    "  low_cpu_mem_usage=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight: torch.float16\n",
      "model.layers.0.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.0.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.0.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.0.mlp.down_proj.weight: torch.float16\n",
      "model.layers.0.input_layernorm.weight: torch.float16\n",
      "model.layers.0.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.1.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.1.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.1.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.1.mlp.down_proj.weight: torch.float16\n",
      "model.layers.1.input_layernorm.weight: torch.float16\n",
      "model.layers.1.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.2.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.2.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.2.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.2.mlp.down_proj.weight: torch.float16\n",
      "model.layers.2.input_layernorm.weight: torch.float16\n",
      "model.layers.2.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.3.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.3.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.3.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.3.mlp.down_proj.weight: torch.float16\n",
      "model.layers.3.input_layernorm.weight: torch.float16\n",
      "model.layers.3.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.4.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.4.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.4.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.4.mlp.down_proj.weight: torch.float16\n",
      "model.layers.4.input_layernorm.weight: torch.float16\n",
      "model.layers.4.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.5.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.5.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.5.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.5.mlp.down_proj.weight: torch.float16\n",
      "model.layers.5.input_layernorm.weight: torch.float16\n",
      "model.layers.5.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.6.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.6.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.6.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.6.mlp.down_proj.weight: torch.float16\n",
      "model.layers.6.input_layernorm.weight: torch.float16\n",
      "model.layers.6.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.7.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.7.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.7.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.7.mlp.down_proj.weight: torch.float16\n",
      "model.layers.7.input_layernorm.weight: torch.float16\n",
      "model.layers.7.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.8.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.8.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.8.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.8.mlp.down_proj.weight: torch.float16\n",
      "model.layers.8.input_layernorm.weight: torch.float16\n",
      "model.layers.8.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.9.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.9.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.9.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.9.mlp.down_proj.weight: torch.float16\n",
      "model.layers.9.input_layernorm.weight: torch.float16\n",
      "model.layers.9.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.10.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.10.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.10.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.10.mlp.down_proj.weight: torch.float16\n",
      "model.layers.10.input_layernorm.weight: torch.float16\n",
      "model.layers.10.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.11.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.11.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.11.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.11.mlp.down_proj.weight: torch.float16\n",
      "model.layers.11.input_layernorm.weight: torch.float16\n",
      "model.layers.11.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.12.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.12.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.12.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.12.mlp.down_proj.weight: torch.float16\n",
      "model.layers.12.input_layernorm.weight: torch.float16\n",
      "model.layers.12.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.13.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.13.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.13.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.13.mlp.down_proj.weight: torch.float16\n",
      "model.layers.13.input_layernorm.weight: torch.float16\n",
      "model.layers.13.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.14.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.14.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.14.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.14.mlp.down_proj.weight: torch.float16\n",
      "model.layers.14.input_layernorm.weight: torch.float16\n",
      "model.layers.14.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.15.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.15.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.15.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.15.mlp.down_proj.weight: torch.float16\n",
      "model.layers.15.input_layernorm.weight: torch.float16\n",
      "model.layers.15.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.16.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.16.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.16.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.16.mlp.down_proj.weight: torch.float16\n",
      "model.layers.16.input_layernorm.weight: torch.float16\n",
      "model.layers.16.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.17.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.17.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.17.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.17.mlp.down_proj.weight: torch.float16\n",
      "model.layers.17.input_layernorm.weight: torch.float16\n",
      "model.layers.17.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.18.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.18.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.18.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.18.mlp.down_proj.weight: torch.float16\n",
      "model.layers.18.input_layernorm.weight: torch.float16\n",
      "model.layers.18.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.19.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.19.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.19.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.19.mlp.down_proj.weight: torch.float16\n",
      "model.layers.19.input_layernorm.weight: torch.float16\n",
      "model.layers.19.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.20.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.20.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.20.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.20.mlp.down_proj.weight: torch.float16\n",
      "model.layers.20.input_layernorm.weight: torch.float16\n",
      "model.layers.20.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.21.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.21.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.21.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.21.mlp.down_proj.weight: torch.float16\n",
      "model.layers.21.input_layernorm.weight: torch.float16\n",
      "model.layers.21.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.22.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.22.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.22.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.22.mlp.down_proj.weight: torch.float16\n",
      "model.layers.22.input_layernorm.weight: torch.float16\n",
      "model.layers.22.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.23.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.23.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.23.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.23.mlp.down_proj.weight: torch.float16\n",
      "model.layers.23.input_layernorm.weight: torch.float16\n",
      "model.layers.23.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.24.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.24.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.24.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.24.mlp.down_proj.weight: torch.float16\n",
      "model.layers.24.input_layernorm.weight: torch.float16\n",
      "model.layers.24.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.25.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.25.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.25.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.25.mlp.down_proj.weight: torch.float16\n",
      "model.layers.25.input_layernorm.weight: torch.float16\n",
      "model.layers.25.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.26.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.26.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.26.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.26.mlp.down_proj.weight: torch.float16\n",
      "model.layers.26.input_layernorm.weight: torch.float16\n",
      "model.layers.26.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.27.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.27.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.27.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.27.mlp.down_proj.weight: torch.float16\n",
      "model.layers.27.input_layernorm.weight: torch.float16\n",
      "model.layers.27.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.28.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.28.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.28.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.28.mlp.down_proj.weight: torch.float16\n",
      "model.layers.28.input_layernorm.weight: torch.float16\n",
      "model.layers.28.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.29.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.29.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.29.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.29.mlp.down_proj.weight: torch.float16\n",
      "model.layers.29.input_layernorm.weight: torch.float16\n",
      "model.layers.29.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.30.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.30.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.30.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.30.mlp.down_proj.weight: torch.float16\n",
      "model.layers.30.input_layernorm.weight: torch.float16\n",
      "model.layers.30.post_attention_layernorm.weight: torch.float16\n",
      "model.layers.31.self_attn.o_proj.weight: torch.float16\n",
      "model.layers.31.self_attn.qkv_proj.weight: torch.float16\n",
      "model.layers.31.mlp.gate_up_proj.weight: torch.float16\n",
      "model.layers.31.mlp.down_proj.weight: torch.float16\n",
      "model.layers.31.input_layernorm.weight: torch.float16\n",
      "model.layers.31.post_attention_layernorm.weight: torch.float16\n",
      "model.norm.weight: torch.float16\n",
      "lm_head.weight: torch.float16\n"
     ]
    }
   ],
   "source": [
    "# 獲取模型參數名稱及型態，確認是否使用半精度浮點數\n",
    "for name, param in model.named_parameters():\n",
    "  print(f'{name}: {param.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 詠唱格式化 (Prompt Formatting)\n",
    "\n",
    "先定義我們的詠唱 (Prompt) 格式。為此，我們將創建一個格式化函數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "system_message = 'Given the information below, mask the personal identifiable information.'\n",
    "\n",
    "def instruction_formatter(x):\n",
    "  text = f'''\n",
    "    <|system|> {system_message}.\n",
    "    <|user|> {x['unmasked_text']}\n",
    "    <|assistant|>\n",
    "  '''\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning 前的表現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 載入預訓練模型\n",
    "generator = pipeline(\n",
    "  task='text-generation',\n",
    "  model=model,\n",
    "  tokenizer=tokenizer,\n",
    "  device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonas_cheng/.pyenv/versions/learning-transformer-with-huggingface-3.11.2/lib/python3.11/site-packages/transformers/pytorch_utils.py:325: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    <|system|> Given the information below, mask the personal identifiable information..\n",
      "    <|user|> In order to finalize your registration for our Animal-assisted Therapy Program, please make the payment of Guarani 428.22. This can be made to our Auto Loan Account with account number 58114957. Once the payment is received, we will reach out to you with more information. Please use the provided contact (364) 467-8496 x139 for any queries related to your registration. Please use your GH5C2WDB7WVB30204 as a reference.\n",
      "    <|assistant|>\n",
      "   To complete participation in an animal assistance theraputic program: proceed by making remittance equivalent approximately USD $X amount [please convert currency if necessary]. Direct this transaction towards 'Auto loan' designated financial record identified under alphanumeric code YYYYMMMDDD_XXXXXX where X represents unique digits and MM/dd corresponds respectively herein omitted due privacy considerations; post settlement confirmation expect communication from coordinators regarding subsequent steps forwarded via specified channel ZZ - XXXXX XXX / Contact Number WXYZEFGHIJKLNMOPQRSTUV at extension NNN following\n"
     ]
    }
   ],
   "source": [
    "# 顯示預訓練模型預測結果\n",
    "input = instruction_formatter(dataset['test'][0])\n",
    "response = generator(\n",
    "  input,\n",
    "  max_new_tokens=128, # 限制最大生成字數\n",
    "  repetition_penalty=1.5, # 重複機率, 1~2 之間, 1.0 (no penalty), 2.0 (maximum penalty)\n",
    ")\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練模型\n",
    "\n",
    "隨著 `trl` 的最新版本發布，現在支持流行的指令 (instruction) 和對話 (conversation) 數據集格式。這意味著我們只需要將數據集轉換為支持的格式之一，`trl` 會處理其餘的部分。這些格式包括：\n",
    "\n",
    "* 指令格式 instruction format\n",
    "\n",
    "```json\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "```\n",
    "\n",
    "* 對話格式 conversational format\n",
    "\n",
    "```json\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料預處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def create_conversation(dataset):\n",
    "  rows = []\n",
    "  unmasked_texts = dataset['unmasked_text']\n",
    "  masked_texts = dataset['masked_text']\n",
    "  for unmasked_text, masked_text in zip(unmasked_texts, masked_texts):\n",
    "    rows.append([\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": unmasked_text},\n",
    "        {\"role\": \"assistant\", \"content\": masked_text}\n",
    "      ],)\n",
    "  return {'messages': rows}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 172/172 [00:00<00:00, 1246.56 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 43/43 [00:00<00:00, 2306.36 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 114.01 examples/s]\n"
     ]
    }
   ],
   "source": [
    "conversation_dataset = dataset.map(\n",
    "  create_conversation,\n",
    "  batched=True,\n",
    "  remove_columns=dataset['train'].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'content': 'Given the information below, mask the personal identifiable information.', 'role': 'system'}, {'content': 'Mr. Adolphus Reagan Ziemann, as a Central Principal Applications Executive at McLaughlin, Nader and Purdy, your knowledge of change management is vital for our company's transformation. We request you to create a change management strategy.', 'role': 'user'}, {'content': '[PREFIX_1] [FIRSTNAME_1] [MIDDLENAME_1] [LASTNAME_1], as a [JOBDESCRIPTOR_1] [JOBTITLE_1] at [COMPANY_NAME_1], your knowledge of change management is vital for our company's transformation. We request you to create a change management strategy.', 'role': 'assistant'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'content': 'Given the information below, mask the personal identifiable information.', 'role': 'system'}, {'content': 'Hello Hannah, would you please investigate the potential fallouts associated with the revisions in the Security department? Please incorporate your findings in your management strategy required previously.', 'role': 'user'}, {'content': 'Hello [FIRSTNAME_1], would you please investigate the potential fallouts associated with the revisions in the [JOBAREA_1] department? Please incorporate your findings in your management strategy required previously.', 'role': 'assistant'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'content': 'Given the information below, mask the personal identifiable information.', 'role': 'system'}, {'content': 'We also request a review of our policies with respect to the upcoming changes and to bring in your expertise in case a policy change is advised. You can communicate the updates via email at Bartholome_Goldner85@yahoo.com.', 'role': 'user'}, {'content': 'We also request a review of our policies with respect to the upcoming changes and to bring in your expertise in case a policy change is advised. You can communicate the updates via email at [EMAIL_1].', 'role': 'assistant'}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 messages\n",
       "0  [{'content': 'Given the information below, mask the personal identifiable information.', 'role': 'system'}, {'content': 'Mr. Adolphus Reagan Ziemann, as a Central Principal Applications Executive at McLaughlin, Nader and Purdy, your knowledge of change management is vital for our company's transformation. We request you to create a change management strategy.', 'role': 'user'}, {'content': '[PREFIX_1] [FIRSTNAME_1] [MIDDLENAME_1] [LASTNAME_1], as a [JOBDESCRIPTOR_1] [JOBTITLE_1] at [COMPANY_NAME_1], your knowledge of change management is vital for our company's transformation. We request you to create a change management strategy.', 'role': 'assistant'}]\n",
       "1                                                                  [{'content': 'Given the information below, mask the personal identifiable information.', 'role': 'system'}, {'content': 'Hello Hannah, would you please investigate the potential fallouts associated with the revisions in the Security department? Please incorporate your findings in your management strategy required previously.', 'role': 'user'}, {'content': 'Hello [FIRSTNAME_1], would you please investigate the potential fallouts associated with the revisions in the [JOBAREA_1] department? Please incorporate your findings in your management strategy required previously.', 'role': 'assistant'}]\n",
       "2                                                                 [{'content': 'Given the information below, mask the personal identifiable information.', 'role': 'system'}, {'content': 'We also request a review of our policies with respect to the upcoming changes and to bring in your expertise in case a policy change is advised. You can communicate the updates via email at Bartholome_Goldner85@yahoo.com.', 'role': 'user'}, {'content': 'We also request a review of our policies with respect to the upcoming changes and to bring in your expertise in case a policy change is advised. You can communicate the updates via email at [EMAIL_1].', 'role': 'assistant'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 顯示前 first_n_data 筆資料\n",
    "first_n_data = 3\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.DataFrame(conversation_dataset['train'].select(range(first_n_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'content': 'Given the information below, mask the personal '\n",
      "                          'identifiable information.',\n",
      "               'role': 'system'},\n",
      "              {'content': 'Mr. Adolphus Reagan Ziemann, as a Central Principal '\n",
      "                          'Applications Executive at McLaughlin, Nader and '\n",
      "                          'Purdy, your knowledge of change management is vital '\n",
      "                          \"for our company's transformation. We request you to \"\n",
      "                          'create a change management strategy.',\n",
      "               'role': 'user'},\n",
      "              {'content': '[PREFIX_1] [FIRSTNAME_1] [MIDDLENAME_1] '\n",
      "                          '[LASTNAME_1], as a [JOBDESCRIPTOR_1] [JOBTITLE_1] '\n",
      "                          'at [COMPANY_NAME_1], your knowledge of change '\n",
      "                          \"management is vital for our company's \"\n",
      "                          'transformation. We request you to create a change '\n",
      "                          'management strategy.',\n",
      "               'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "# 顯示單筆方便閱讀\n",
    "pprint(conversation_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA 的訓練策略 - 降維打擊\n",
    "\n",
    "LoRA（Low-Rank Adaptation）是一種用於訓練大型語言模型的技術，旨在提高訓練效率並減少計算資源的需求。以下是為何需要透過LoRA訓練的一些原因：\n",
    "\n",
    "降低計算成本：LoRA 通過將模型的權重矩陣分解為低秩矩陣，顯著減少了參數的數量，從而降低了計算成本和內存需求。\n",
    "\n",
    "加速訓練速度：由於參數數量減少，LoRA 可以加速模型的訓練過程，使得在相同的硬件資源下能夠更快地完成訓練。\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/lora_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 3,821,079,552, Trainable Parameters: 3,821,079,552\n"
     ]
    }
   ],
   "source": [
    "# 查看預訓練模型可訓練的參數量，其數量相當龐大，所以需要透過 Low Rank Approximation (LORA) 來降低參數量\n",
    "print('Parameters: {:,}, Trainable Parameters: {:,}'.format(\n",
    "  model.num_parameters(),\n",
    "  model.num_parameters(only_trainable=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PEFT 配置\n",
    "\n",
    "`target_module`: 要降維的模型層，可以透過 `model.named_parameters` 查看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoraConfig(task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>,\n",
      "           peft_type=<PeftType.LORA: 'LORA'>,\n",
      "           auto_mapping=None,\n",
      "           base_model_name_or_path=None,\n",
      "           revision=None,\n",
      "           inference_mode=False,\n",
      "           r=128,\n",
      "           target_modules={'qkv_proj'},\n",
      "           exclude_modules=None,\n",
      "           lora_alpha=8,\n",
      "           lora_dropout=0.0,\n",
      "           fan_in_fan_out=False,\n",
      "           bias='none',\n",
      "           use_rslora=False,\n",
      "           modules_to_save=None,\n",
      "           init_lora_weights=True,\n",
      "           layers_to_transform=None,\n",
      "           layers_pattern=None,\n",
      "           rank_pattern={},\n",
      "           alpha_pattern={},\n",
      "           megatron_config=None,\n",
      "           megatron_core='megatron.core',\n",
      "           loftq_config={},\n",
      "           eva_config=None,\n",
      "           use_dora=False,\n",
      "           layer_replication=None,\n",
      "           runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False),\n",
      "           lora_bias=False)\n"
     ]
    }
   ],
   "source": [
    "# PEFT 配置\n",
    "lora_config = LoraConfig(\n",
    "  task_type=TaskType.CAUSAL_LM,\n",
    "  r=config.rank,\n",
    "  target_modules=['qkv_proj'],\n",
    ")\n",
    "pprint(lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 取得 PEFT 模型\n",
    "\n",
    "搭配預訓模型及 PEFT 配置，我們可以取得 PEFT 模型。我們可以觀察受到降維影響的模型層。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonas_cheng/.pyenv/versions/learning-transformer-with-huggingface-3.11.2/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "# 取得 PEFT 模型\n",
    "peft_model = get_peft_model(\n",
    "  model, # 預訓練模型\n",
    "  lora_config, # PEFT 配置\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Phi3ForCausalLM(\n",
       "      (model): Phi3Model(\n",
       "        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "        (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x Phi3DecoderLayer(\n",
       "            (self_attn): Phi3SdpaAttention(\n",
       "              (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "              (qkv_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=9216, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Phi3MLP(\n",
       "              (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "              (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "              (activation_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取得 PEFT 模型, 觀察受 PEFT 影響的模型參數\n",
    "peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 調整 PEFT 模型精度\n",
    "\n",
    "PEFT 模型的精度是 `torch.float32`，我們可以透過 `model.half()` 將其轉換為半精度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight: torch.float16\n",
      "base_model.model.model.layers.0.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.0.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.0.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.0.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.0.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.0.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.0.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.1.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.1.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.1.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.1.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.1.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.1.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.1.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.2.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.2.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.2.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.2.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.2.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.2.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.2.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.3.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.3.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.3.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.3.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.3.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.3.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.3.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.4.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.4.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.4.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.4.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.4.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.4.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.4.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.5.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.5.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.5.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.5.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.5.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.5.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.5.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.6.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.6.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.6.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.6.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.6.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.6.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.6.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.7.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.7.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.7.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.7.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.7.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.7.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.7.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.8.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.8.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.8.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.8.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.8.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.8.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.8.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.9.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.9.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.9.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.9.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.9.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.9.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.9.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.10.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.10.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.10.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.10.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.10.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.10.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.10.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.11.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.11.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.11.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.11.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.11.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.11.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.11.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.12.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.12.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.12.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.12.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.12.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.12.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.12.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.13.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.13.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.13.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.13.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.13.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.13.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.13.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.14.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.14.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.14.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.14.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.14.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.14.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.14.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.15.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.15.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.15.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.15.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.15.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.15.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.15.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.16.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.16.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.16.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.16.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.16.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.16.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.16.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.16.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.17.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.17.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.17.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.17.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.17.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.17.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.17.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.17.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.18.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.18.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.18.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.18.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.18.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.18.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.18.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.18.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.19.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.19.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.19.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.19.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.19.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.19.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.19.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.19.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.20.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.20.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.20.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.20.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.20.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.20.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.20.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.20.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.21.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.21.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.21.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.21.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.21.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.21.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.21.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.21.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.22.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.22.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.22.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.22.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.22.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.22.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.22.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.22.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.23.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.23.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.23.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.23.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.23.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.23.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.23.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.23.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.24.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.24.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.24.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.24.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.24.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.24.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.24.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.24.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.25.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.25.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.25.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.25.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.25.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.25.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.25.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.25.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.26.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.26.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.26.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.26.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.26.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.26.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.26.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.26.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.27.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.27.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.27.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.27.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.27.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.27.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.27.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.27.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.28.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.28.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.28.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.28.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.28.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.28.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.28.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.28.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.29.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.29.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.29.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.29.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.29.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.29.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.29.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.29.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.30.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.30.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.30.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.30.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.30.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.30.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.30.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.30.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.31.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.31.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.31.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.31.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.31.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.31.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.31.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.31.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.norm.weight: torch.float16\n",
      "base_model.model.lm_head.weight: torch.float16\n"
     ]
    }
   ],
   "source": [
    "# 獲取 PERF 模型參數名稱及型態，確認是否使用半精度浮點數\n",
    "for name, param in peft_model.named_parameters():\n",
    "  print(f'{name}: {param.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 同樣採用半精度浮點數訓練\n",
    "peft_model = peft_model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 50,331,648 || all params: 3,871,411,200 || trainable%: 1.3001\n"
     ]
    }
   ],
   "source": [
    "# 查看可訓練的參數量\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定義訓練參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 172/172 [00:00<00:00, 1852.53 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 43/43 [00:00<00:00, 2401.40 examples/s]\n",
      "/Users/jonas_cheng/.pyenv/versions/learning-transformer-with-huggingface-3.11.2/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = SFTConfig(\n",
    "  output_dir='sample_data/train_output_pii_masking',\n",
    "  learning_rate=config.lr,\n",
    "  per_device_train_batch_size=config.train_batch_size,\n",
    "  per_device_eval_batch_size=config.eval_batch_size,\n",
    "  gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "  num_train_epochs=config.epochs,\n",
    "  weight_decay=config.weight_decay,\n",
    "  eval_strategy='epoch', # 每個 epoch 評估一次\n",
    "  save_strategy='epoch', # 每個 epoch 儲存一次\n",
    "  load_best_model_at_end=True,\n",
    "  report_to='none', # Disable wandb on colab\n",
    "  adam_epsilon=config.adam_epsilon, # 當使用半精度浮點數時，需要設定較大的 adam epsilon\n",
    "  packing=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=conversation_dataset['train'],\n",
    "    eval_dataset=conversation_dataset['validation'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 開始訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='87' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 87/129 3:39:26 < 1:48:25, 0.01 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.357999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='22' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5/22 01:48 < 07:41, 0.04 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 開始訓練，這可能需要一些時間\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存 LoRA 模型參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 保存 Lora 参数\n",
    "peft_model.save_pretrained(config.saved_lora_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 合併 LoRA 模型參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合併原始模型和 Lora 参数\n",
    "new_model = PeftModel.from_pretrained(model, config.saved_lora_path)\n",
    "\n",
    "print(\"=== 合併前的模型結構 ===\")\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 合併並卸載 Lora 参数\n",
    "new_model.merge_and_unload()\n",
    "\n",
    "print(\"=== 合併後的模型結構 ===\")\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 保存合併後的模型\n",
    "new_model.save_pretrained(config.saved_model_path)\n",
    "tokenizer.save_pretrained(config.saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 釋放資源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 釋放 GPU 記憶體\n",
    "del new_model\n",
    "del trainer\n",
    "\n",
    "peft_model.to('cpu')\n",
    "del peft_model\n",
    "\n",
    "model.to('cpu')\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評估模型\n",
    "\n",
    "### 載入微調後 Tokenizer\n",
    "\n",
    "從已經完成訓練的模型取得 Tokenizer，可以留意這個訓練時保存下來的 Tokenizer 仍保有訓練時的設定，包涵 `pad_token` 和 `padding_side`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  config.saved_model_path\n",
    ")\n",
    "# 檢視 Tokenizer\n",
    "pprint(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入微調後模型\n",
    "\n",
    "以半精度浮點數載入已經完成訓練的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 以半精度浮點數載入已經完成訓練的模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  config.saved_model_path,\n",
    "  low_cpu_mem_usage=True,\n",
    "  torch_dtype=config.torch_dtype,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning 後的表現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 載入新模型\n",
    "generator = pipeline(\n",
    "  task='text-generation',\n",
    "  model=model,\n",
    "  tokenizer=tokenizer,\n",
    "  device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 顯示新模型預測結果\n",
    "input = instruction_formatter(dataset['test'][0])\n",
    "response = generator(\n",
    "  input,\n",
    "  max_new_tokens=128, # 限制最大生成字數\n",
    "  repetition_penalty=1.5, # 重複機率, 1~2 之間, 1.0 (no penalty), 2.0 (maximum penalty)\n",
    ")\n",
    "print(response[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
