{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {},
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# transformers not support NumPy 2.0 yet\n",
    "!pip install -q numpy~=1.26.4 transformers~=4.46.2\n",
    "!pip install -q datasets pydantic peft\n",
    "# !pip install -q bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練 PII 遮掩模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonas_cheng/.pyenv/versions/learning-transformer-with-huggingface-3.11.2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "  AutoTokenizer,\n",
    "  AutoModelForCausalLM,\n",
    "  TrainingArguments,\n",
    "  Trainer,\n",
    ")\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "  DataCollatorForSeq2Seq,\n",
    "  BitsAndBytesConfig,\n",
    "  pipeline,\n",
    ")\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "\n",
    "# 載入 PEFT 相關套件\n",
    "from peft import LoraConfig, TaskType, PeftModel, get_peft_model\n",
    "\n",
    "# 檢查是否有 GPU 可以使用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下載資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# The full `train` split, only 50% of dataset\n",
    "immutable_dataset = load_dataset(\"ai4privacy/pii-masking-65k\", split=\"train[:50%]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料包含什麼？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['masked_text', 'unmasked_text', 'token_entity_labels', 'tokenised_unmasked_text'],\n",
       "        num_rows: 17261\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['masked_text', 'unmasked_text', 'token_entity_labels', 'tokenised_unmasked_text'],\n",
       "        num_rows: 4316\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['masked_text', 'unmasked_text', 'token_entity_labels', 'tokenised_unmasked_text'],\n",
       "        num_rows: 11\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reserve 0.05% of the training set for testing\n",
    "test_dataset = immutable_dataset.train_test_split(\n",
    "  test_size=0.0005, # 0.05% of the data is used for testing\n",
    "  shuffle=True, # Ensure that train and validation sets are the same across runs\n",
    "  )\n",
    "# Split into 80% training and 20% validation sets\n",
    "train_dataset = test_dataset['train'].train_test_split(\n",
    "  test_size=0.2, # 20% of the data is used for validation\n",
    "  shuffle=True, # Ensure that train and test sets are the same across runs\n",
    "  )\n",
    "immutable_dataset = DatasetDict({\n",
    "  'train': train_dataset['train'],\n",
    "  'validation': train_dataset['test'],\n",
    "  'test': test_dataset['test'],\n",
    "  })\n",
    "immutable_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['masked_text', 'unmasked_text'],\n",
       "        num_rows: 17261\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['masked_text', 'unmasked_text'],\n",
       "        num_rows: 4316\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['masked_text', 'unmasked_text'],\n",
       "        num_rows: 11\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保留必要 features: 'masked_text', 'unmasked_text'\n",
    "dataset = immutable_dataset.remove_columns(['token_entity_labels', 'tokenised_unmasked_text'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>masked_text</th>\n",
       "      <th>unmasked_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Could you please analyze if the website visited most frequently by our user agent software [USERAGENT_1] is necessary for our operation?</td>\n",
       "      <td>Could you please analyze if the website visited most frequently by our user agent software Mozilla/5.0 (Windows; U; Windows NT 5.0) AppleWebKit/535.1.1 (KHTML, like Gecko) Chrome/14.0.808.0 Safari/535.1.1 is necessary for our operation?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dear Team, we have received several reports about software issues within the vicinity of IP [IP_1]. Please investigate this matter urgently, and update [EMAIL_1] with your findings.</td>\n",
       "      <td>Dear Team, we have received several reports about software issues within the vicinity of IP 97fe:fef5:3941:ab2c:58dc:bb80:a8a6:03ac. Please investigate this matter urgently, and update Lilliana.Greenholt56@yahoo.com with your findings.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abbiamo individuato alcune comunità online che discutono attivamente dei nostri prodotti e servizi. Apprezzeremmo se potesse tenere d'occhio queste comunità e segnalare eventuali discussioni o feedback importanti. Prestare particolare attenzione a eventuali post degli utenti nella [STATE_1] [COUNTY_1]. Abbiamo bisogno di questi dati per perfezionare la nostra strategia di relazioni pubbliche.</td>\n",
       "      <td>Abbiamo individuato alcune comunità online che discutono attivamente dei nostri prodotti e servizi. Apprezzeremmo se potesse tenere d'occhio queste comunità e segnalare eventuali discussioni o feedback importanti. Prestare particolare attenzione a eventuali post degli utenti nella Illinois Buckinghamshire. Abbiamo bisogno di questi dati per perfezionare la nostra strategia di relazioni pubbliche.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                   masked_text  \\\n",
       "0                                                                                                                                                                                                                                                                     Could you please analyze if the website visited most frequently by our user agent software [USERAGENT_1] is necessary for our operation?   \n",
       "1                                                                                                                                                                                                                        Dear Team, we have received several reports about software issues within the vicinity of IP [IP_1]. Please investigate this matter urgently, and update [EMAIL_1] with your findings.   \n",
       "2  Abbiamo individuato alcune comunità online che discutono attivamente dei nostri prodotti e servizi. Apprezzeremmo se potesse tenere d'occhio queste comunità e segnalare eventuali discussioni o feedback importanti. Prestare particolare attenzione a eventuali post degli utenti nella [STATE_1] [COUNTY_1]. Abbiamo bisogno di questi dati per perfezionare la nostra strategia di relazioni pubbliche.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                     unmasked_text  \n",
       "0                                                                                                                                                                     Could you please analyze if the website visited most frequently by our user agent software Mozilla/5.0 (Windows; U; Windows NT 5.0) AppleWebKit/535.1.1 (KHTML, like Gecko) Chrome/14.0.808.0 Safari/535.1.1 is necessary for our operation?  \n",
       "1                                                                                                                                                                      Dear Team, we have received several reports about software issues within the vicinity of IP 97fe:fef5:3941:ab2c:58dc:bb80:a8a6:03ac. Please investigate this matter urgently, and update Lilliana.Greenholt56@yahoo.com with your findings.  \n",
       "2  Abbiamo individuato alcune comunità online che discutono attivamente dei nostri prodotti e servizi. Apprezzeremmo se potesse tenere d'occhio queste comunità e segnalare eventuali discussioni o feedback importanti. Prestare particolare attenzione a eventuali post degli utenti nella Illinois Buckinghamshire. Abbiamo bisogno di questi dati per perfezionare la nostra strategia di relazioni pubbliche.  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 顯示前 first_n_data 筆資料\n",
    "first_n_data = 3\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.DataFrame(dataset['train'].select(range(first_n_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 訓練相關設定, 利用降低 batch size 提高 gradient accumulation steps 來節省記憶體\n",
    "class Config(BaseModel):\n",
    "  model_name: str = 'microsoft/Phi-3.5-mini-instruct'\n",
    "  saved_model_path: str = 'sample_data/saved_encoder_model' # path to save the trained model\n",
    "  saved_lora_path: str = 'sample_data/saved_lora_model' # path to save the trained LORA model\n",
    "  train_batch_size: int = 2 # size of the input batch in training\n",
    "  eval_batch_size: int = 2 # size of the input batch in evaluation\n",
    "  gradient_accumulation_steps: int = 2 # number of updates steps to accumulate before performing a backward/update pass\n",
    "  epochs: int = 1 # 測試至少 5 個 epochs 才有成效\n",
    "  lr: float = 2e-5 # learning rate, controls how fast or slow the model learns\n",
    "  weight_decay: float = 0.01 # weight decay, helps the model stay simple and avoid overfitting by penalizing large weights.\n",
    "\n",
    "  # LORA 相關設定\n",
    "  rank: int = 128 # rank of the PEFT model\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Quantization 壓縮設定，將 model 壓縮至 8-bit\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#   load_in_8bit=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 先觀察 Fine-tuning 前的表現"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 詠唱格式化 (Prompt Formatting)\n",
    "\n",
    "先定義我們的詠唱 (Prompt) 格式。為此，我們將創建一個格式化函數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def instruction_formatter(x):\n",
    "  text = f'''\n",
    "    Given the information below, mask the personal identifiable information.\n",
    "\n",
    "    Input: {x['unmasked_text']}\n",
    "\n",
    "    Output:\n",
    "  '''\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='distilbert/distilgpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 透過預訓練模型取得 Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  config.model_name,\n",
    ")\n",
    "# 檢視 Tokenizer，是否存在 PADDING token?\n",
    "pprint(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='distilbert/distilgpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Add PADDING token to the tokenizer\n",
    "if tokenizer.pad_token is None:\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  print('=== 設定 Padding Token ===')\n",
    "  pprint(tokenizer)\n",
    "# Make sure padding_side is 'right'\n",
    "if tokenizer.padding_side != 'right':\n",
    "  tokenizer.padding_side = 'right'\n",
    "  print('=== 設定 Padding Side ===')\n",
    "  pprint(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m  \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m  \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/learning-transformer-with-huggingface-3.11.2/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/learning-transformer-with-huggingface-3.11.2/lib/python3.11/site-packages/transformers/modeling_utils.py:3657\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3654\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3657\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3659\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3660\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3661\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[0;32m~/.pyenv/versions/learning-transformer-with-huggingface-3.11.2/lib/python3.11/site-packages/transformers/quantizers/quantizer_bnb_8bit.py:73\u001b[0m, in \u001b[0;36mBnb8BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 8-bit quantization requires Accelerate: `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m     )\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_bitsandbytes_available():\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m     )\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_bnb_backend_availability\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n",
      "\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`"
     ]
    }
   ],
   "source": [
    "# 半精度浮點數訓練\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  config.model_name,\n",
    "  torch_dtype=torch.bfloat16,\n",
    "  low_cpu_mem_usage=True,\n",
    "#  quantization_config=quantization_config,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 獲取模型參數名稱及型態\n",
    "for name, param in model.named_parameters():\n",
    "  print(f'{name}: {param.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 載入預訓練模型\n",
    "generator = pipeline(\n",
    "  task='text-generation',\n",
    "  model=model,\n",
    "  tokenizer=tokenizer,\n",
    "  device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonas_cheng/.pyenv/versions/learning-transformer-with-huggingface-3.11.2/lib/python3.11/site-packages/transformers/pytorch_utils.py:325: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Given the information below, mask the personal identifiable information.\n",
      "\n",
      "    Input: To ensure network security in our Configuration, kindly change your system password to pEyDYbbpBRSs at your earliest convenience.\n",
      "\n",
      "    Output:\n",
      "   If my service is up and running on a new computer with multiple CPUs but you still need an old school backup solution before sending us copies of them that would be extremely important not only for yourself when I put this software over there –but also because it makes using Microsoft's Azure services much easier since they give more secure access as well by adding back-end encryption functionality which reduces all those extra bitbuckets going missing (unless we're working directly from Amazon).\n"
     ]
    }
   ],
   "source": [
    "# 顯示預訓練模型預測結果\n",
    "input = instruction_formatter(dataset['test'][0])\n",
    "response = generator(\n",
    "  input,\n",
    "  max_new_tokens=512, # 限制最大生成字數\n",
    "  repetition_penalty=1.5, # 重複機率, 1~2 之間, 1.0 (no penalty), 2.0 (maximum penalty)\n",
    ")\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料預處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def process_func(x):\n",
    "  token_ids, attention_mask, labels = [], [], []\n",
    "  instruction = tokenizer(\n",
    "    instruction_formatter(x),\n",
    "    truncation=True)\n",
    "  # Generate the response and add the end of string token\n",
    "  response = tokenizer(\n",
    "    x['masked_text'] + tokenizer.eos_token,\n",
    "    truncation=True,)\n",
    "  # Combine the instruction and response\n",
    "  token_ids = instruction['input_ids'] + response['input_ids']\n",
    "  attention_mask = instruction['attention_mask'] + response['attention_mask']\n",
    "  # The labels are the response token ids, but ignore the instruction token ids by setting them to -100\n",
    "  labels = [-100] * len(instruction['input_ids']) + response['input_ids']\n",
    "\n",
    "  return {\n",
    "    'input_ids': token_ids,\n",
    "    'attention_mask': attention_mask,\n",
    "    'labels': labels,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17261/17261 [00:07<00:00, 2286.91 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 4316/4316 [00:01<00:00, 2290.18 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 536.32 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "  process_func,\n",
    "  batched=False,\n",
    "  remove_columns=dataset['train'].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 輸入資料 ===\n",
      "\n",
      "    Given the information below, mask the personal identifiable information.\n",
      "\n",
      "    Input: Dans le cas où vous décideriez de procéder au paiement en cryptomonnaie, veuillez trouver notre adresse Ethereum 0xbf9555a9cc7d3ca2bfab02bfdf43210a8cc5bb0d pour des transactions rapides et faciles.\n",
      "\n",
      "    Output:\n",
      "  Dans le cas où vous décideriez de procéder au paiement en cryptomonnaie, veuillez trouver notre adresse Ethereum [ETHEREUMADDRESS_1] pour des transactions rapides et faciles.<|endoftext|>\n",
      "\n",
      "=== 標註資料 (非 -100) ===\n",
      "Dans le cas où vous décideriez de procéder au paiement en cryptomonnaie, veuillez trouver notre adresse Ethereum [ETHEREUMADDRESS_1] pour des transactions rapides et faciles.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# 檢視輸入資料\n",
    "print('=== 輸入資料 ===')\n",
    "print(tokenizer.decode(tokenized_dataset['train']['input_ids'][0]))\n",
    "print()\n",
    "print('=== 標註資料 (非 -100) ===')\n",
    "print(tokenizer.decode(\n",
    "  list(filter(lambda x: x != -100, tokenized_dataset['train']['labels'][0]))\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練模型\n",
    "\n",
    "您現在可以開始訓練您的模型了！使用 AutoModelForCausalLM 加載預訓練的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 81,912,576, Trainable Parameters: 81,912,576\n"
     ]
    }
   ],
   "source": [
    "# 查看可訓練的參數量\n",
    "print('Parameters: {:,}, Trainable Parameters: {:,}'.format(\n",
    "  model.num_parameters(),\n",
    "  model.num_parameters(only_trainable=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA 的訓練策略 - 降維打擊\n",
    "\n",
    "LoRA（Low-Rank Adaptation）是一種用於訓練大型語言模型的技術，旨在提高訓練效率並減少計算資源的需求。以下是為何需要透過LoRA訓練的一些原因：\n",
    "\n",
    "降低計算成本：LoRA 通過將模型的權重矩陣分解為低秩矩陣，顯著減少了參數的數量，從而降低了計算成本和內存需求。\n",
    "\n",
    "加速訓練速度：由於參數數量減少，LoRA 可以加速模型的訓練過程，使得在相同的硬件資源下能夠更快地完成訓練。\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/lora_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoraConfig(task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>,\n",
      "           peft_type=<PeftType.LORA: 'LORA'>,\n",
      "           auto_mapping=None,\n",
      "           base_model_name_or_path=None,\n",
      "           revision=None,\n",
      "           inference_mode=False,\n",
      "           r=256,\n",
      "           target_modules=None,\n",
      "           exclude_modules=None,\n",
      "           lora_alpha=8,\n",
      "           lora_dropout=0.0,\n",
      "           fan_in_fan_out=False,\n",
      "           bias='none',\n",
      "           use_rslora=False,\n",
      "           modules_to_save=None,\n",
      "           init_lora_weights=True,\n",
      "           layers_to_transform=None,\n",
      "           layers_pattern=None,\n",
      "           rank_pattern={},\n",
      "           alpha_pattern={},\n",
      "           megatron_config=None,\n",
      "           megatron_core='megatron.core',\n",
      "           loftq_config={},\n",
      "           eva_config=None,\n",
      "           use_dora=False,\n",
      "           layer_replication=None,\n",
      "           runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False),\n",
      "           lora_bias=False)\n"
     ]
    }
   ],
   "source": [
    "# PEFT 配置\n",
    "lora_config = LoraConfig(\n",
    "  task_type=TaskType.CAUSAL_LM,\n",
    "  r=config.rank,\n",
    "  target_modules=['qkv_proj'],\n",
    ")\n",
    "pprint(lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonas_cheng/.pyenv/versions/learning-transformer-with-huggingface-3.11.2/lib/python3.11/site-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 取得 PEFT 模型\n",
    "peft_model = get_peft_model(\n",
    "  model, # 預訓練模型\n",
    "  lora_config, # PEFT 配置\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2LMHeadModel(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-5): 6 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2SdpaAttention(\n",
       "              (c_attn): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=2304, nx=768)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=256, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (c_proj): Conv1D(nf=768, nx=768)\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D(nf=3072, nx=768)\n",
       "              (c_proj): Conv1D(nf=768, nx=3072)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取得 PEFT 模型, 觀察受 PEFT 影響的模型參數\n",
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 獲取模型參數名稱及型態\n",
    "for name, param in peft_model.named_parameters():\n",
    "  print(f'{name}: {param.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 同樣採用半精度浮點數訓練\n",
    "peft_model = peft_model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,718,592 || all params: 86,631,168 || trainable%: 5.4468\n"
     ]
    }
   ],
   "source": [
    "# 查看可訓練的參數量\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/78/7jt3l1x92pg6ddgbzjbj12lw0000gp/T/ipykernel_11899/482717621.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir='sample_data/train_output_pii_masking',\n",
    "  learning_rate=config.lr,\n",
    "  per_device_train_batch_size=config.train_batch_size,\n",
    "  per_device_eval_batch_size=config.eval_batch_size,\n",
    "  gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "  num_train_epochs=config.epochs,\n",
    "  weight_decay=config.weight_decay,\n",
    "  eval_strategy='epoch', # 每個 epoch 評估一次\n",
    "  save_strategy='epoch', # 每個 epoch 儲存一次\n",
    "  load_best_model_at_end=True,\n",
    "  report_to='none', # Disable wandb on colab\n",
    "  adam_epsilon=1e-4, # 當使用半精度浮點數時，需要設定較大的 adam epsilon\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "  model=peft_model,\n",
    "  args=training_args,\n",
    "  train_dataset=tokenized_dataset['train'],\n",
    "  eval_dataset=tokenized_dataset['validation'],\n",
    "  data_collator=DataCollatorForSeq2Seq(tokenizer, padding=True),\n",
    "  tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21580' max='21580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21580/21580 3:26:13, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.751200</td>\n",
       "      <td>0.562725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.583800</td>\n",
       "      <td>0.442927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.382811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.520500</td>\n",
       "      <td>0.354371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.460600</td>\n",
       "      <td>0.344917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=21580, training_loss=0.6572439197260986, metrics={'train_runtime': 12374.7877, 'train_samples_per_second': 6.974, 'train_steps_per_second': 1.744, 'total_flos': 5542076207185920.0, 'train_loss': 0.6572439197260986, 'epoch': 5.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 開始訓練，這可能需要一些時間\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 保存 Lora 参数\n",
    "peft_model.save_pretrained(f'{config.saved_lora_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 合併前的模型結構 ===\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPT2LMHeadModel(\n",
      "      (transformer): GPT2Model(\n",
      "        (wte): Embedding(50257, 768)\n",
      "        (wpe): Embedding(1024, 768)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-5): 6 x GPT2Block(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2SdpaAttention(\n",
      "              (c_attn): lora.Linear(\n",
      "                (base_layer): Conv1D(nf=2304, nx=768)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=256, out_features=2304, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (c_proj): Conv1D(nf=768, nx=768)\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): Conv1D(nf=3072, nx=768)\n",
      "              (c_proj): Conv1D(nf=768, nx=3072)\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "=== 合併後的模型結構 ===\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPT2LMHeadModel(\n",
      "      (transformer): GPT2Model(\n",
      "        (wte): Embedding(50257, 768)\n",
      "        (wpe): Embedding(1024, 768)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-5): 6 x GPT2Block(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2SdpaAttention(\n",
      "              (c_attn): Conv1D(nf=2304, nx=768)\n",
      "              (c_proj): Conv1D(nf=768, nx=768)\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): Conv1D(nf=3072, nx=768)\n",
      "              (c_proj): Conv1D(nf=768, nx=3072)\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 合併原始模型和 Lora 参数\n",
    "new_model = PeftModel.from_pretrained(model, config.saved_lora_path)\n",
    "\n",
    "print(\"=== 合併前的模型結構 ===\")\n",
    "print(new_model)\n",
    "\n",
    "# 合併並卸載 Lora 参数\n",
    "new_model.merge_and_unload()\n",
    "\n",
    "print(\"=== 合併後的模型結構 ===\")\n",
    "print(new_model)\n",
    "\n",
    "# 儲存模型\n",
    "# trainer.save_model(config.saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('sample_data/saved_encoder_model/tokenizer_config.json',\n",
       " 'sample_data/saved_encoder_model/special_tokens_map.json',\n",
       " 'sample_data/saved_encoder_model/vocab.json',\n",
       " 'sample_data/saved_encoder_model/merges.txt',\n",
       " 'sample_data/saved_encoder_model/added_tokens.json',\n",
       " 'sample_data/saved_encoder_model/tokenizer.json')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存合併後的模型\n",
    "new_model.save_pretrained(config.saved_model_path)\n",
    "tokenizer.save_pretrained(config.saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 載入新模型\n",
    "generator = pipeline(\n",
    "  task='text-generation',\n",
    "  model=config.saved_model_path,\n",
    "  device=device,\n",
    "  tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonas_cheng/.pyenv/versions/learning-transformer-with-huggingface-3.11.2/lib/python3.11/site-packages/transformers/pytorch_utils.py:325: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Given the information below, mask the personal identifiable information.\n",
      "\n",
      "    Input: To ensure network security in our Configuration, kindly change your system password to pEyDYbbpBRSs at your earliest convenience.\n",
      "\n",
      "    Output:\n",
      "   [email protected] Create a virtual address inside this computer before moving from an attacker mode or destination and re-enter each one of three ways by default using ssh : mbox -O \"https://localhost/1\" /etc/.ssh_hostname if desired Please enter such command on any terminal like sudo ssl5, ezm7r4f2d6ac8c9e0ba0801af3ec80eb35cfb50ddef86a00bd8319be40fa8868ca74aa052618257522dc1315573844476777293637892748703942deb12655954ea09043355cd4590df2028876664244623176978bf96db11ae06da491094248341407841643264309224799582796061ffab21fd31fb81fe53ad6312948870898605200411287985245953076092654072553043085630530640640532157608208258256254266278298297309335449732592387584081096484986062051997078091277061060009330310850567855962159508607018659679721053591653791683691073743553685217420164418564970969819518903229658169696228269239759188209204268299149686235289198148697288234267177197805249019478972853494878086890215611535499918713927418038417319218314616614429612517538535601639738951120486339761004003752574894552061031794487115329414515534623617637867716723713747949539435769524414740958744770524028624619634734845021455622616460415445710418499529519047627613475533637633415857827039039535317837713627524328413319411628735811466613879516064699722380664745936770439878920713566982193142132131227458694252122197991507685277496396117343996172186387756392143124010754684163203015386388776014998804337504668126113366111 (Note 2) http:/opt... 1\n"
     ]
    }
   ],
   "source": [
    "# 顯示新模型預測結果\n",
    "input = instruction_formatter(dataset['test'][0])\n",
    "response = generator(\n",
    "  input,\n",
    "  max_new_tokens=512, # 限制最大生成字數\n",
    "  repetition_penalty=1.5, # 重複機率, 1~2 之間, 1.0 (no penalty), 2.0 (maximum penalty)\n",
    ")\n",
    "print(response[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
