{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# transformers not support NumPy 2.0 yet\n",
    "!pip install -q numpy~=1.26.4 transformers~=4.46.2\n",
    "!pip install -q datasets~=3.2.0 pydantic~=2.10.4\n",
    "!pip install -q peft~=0.14.0 trl~=0.13.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練問答模型\n",
    "\n",
    "在這個筆記本中，我們將展示如何使用 `transformers` 套件訓練問答模型。我們將使用 `transformers` 套件中的 `SFTTrainer` ([Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/sft_trainer)) 類別來訓練模型，。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# import garbage collector\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "  AutoTokenizer,\n",
    "  AutoModelForCausalLM,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "  pipeline,\n",
    ")\n",
    "\n",
    "from typing import Any\n",
    "from pydantic import BaseModel\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "\n",
    "# 載入 PEFT 相關套件\n",
    "from peft import LoraConfig, TaskType, PeftModel, get_peft_model\n",
    "# 載入 SFTTrainer 相關套件\n",
    "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM, setup_chat_format\n",
    "\n",
    "# 檢查是否有 GPU 可以使用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下載資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# The full `train` split, only 1% of dataset\n",
    "immutable_dataset = load_dataset(\"philschmid/dolly-15k-oai-style\", split=\"train[:1%]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料包含什麼？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 顯示原始資料中包含的 features 以及筆數\n",
    "immutable_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 檢視資料集中的第一筆資料\n",
    "pprint(immutable_dataset[0]['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 將 messages 欄位分拆成 user 和 assistant 兩個欄位，方便演示\n",
    "dataset = immutable_dataset.map(\n",
    "  lambda x: {\n",
    "    \"user\": x[\"messages\"][0],\n",
    "    \"assistant\": x[\"messages\"][1],\n",
    "  }\n",
    ")\n",
    "# 顯示處理後的資料\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 顯示前 first_n_data 筆資料\n",
    "first_n_data = 3\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.DataFrame(dataset.select(range(first_n_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練設定\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 訓練相關設定, 利用降低 batch size 提高 gradient accumulation steps 來節省記憶體\n",
    "class Config(BaseModel):\n",
    "  model_name: str = 'facebook/opt-2.7b'\n",
    "  torch_dtype: Any = torch.bfloat16 # 半精度浮點數\n",
    "  adam_epsilon: float = 1e-4 # 當使用半精度浮點數時，需要設定較大的 adam epsilon\n",
    "  saved_model_path: str = 'sample_data/saved_encoder_model' # path to save the trained model\n",
    "  saved_lora_path: str = 'sample_data/saved_lora_model' # path to save the trained LORA model\n",
    "  train_batch_size: int = 2 # size of the input batch in training\n",
    "  gradient_accumulation_steps: int = 2 # number of updates steps to accumulate before performing a backward/update pass\n",
    "  epochs: int = 20 # number of times to iterate over the entire training dataset\n",
    "  lr: float = 1e-3 # learning rate, controls how fast or slow the model learns\n",
    "  weight_decay: float = 0.01 # weight decay, helps the model stay simple and avoid overfitting by penalizing large weights.\n",
    "\n",
    "  # Pipieline 相關設定\n",
    "  pipeline_name: str = 'text-generation' # pipeline name\n",
    "  temperature: float = 0.1 # temperature for sampling\n",
    "  max_new_tokens: int = 125 # 限制最大生成字數\n",
    "  repetition_penalty: float = 1.5 # 重複機率, 1~2 之間, 1.0 (no penalty), 2.0 (maximum penalty)\n",
    "\n",
    "  # LORA 相關設定\n",
    "  rank: int = 128 # rank of the LORA model\n",
    "  lora_alpha: int = 32 # alpha of the LORA model\n",
    "\n",
    "if device.type == 'mps': # 方便在 Apple Silicon 上快速測試\n",
    "  config = Config(\n",
    "    model_name='facebook/opt-125m',\n",
    "    torch_dtype=torch.float32, # 在 Apple Silicon 若使用預訓練模型 opt-125m 需要使用全精度浮點數，否則會出現錯誤\n",
    "    epochs=3,\n",
    "  )\n",
    "else:\n",
    "  config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 先觀察 Fine-tuning 前的表現\n",
    "\n",
    "### 載入 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 透過預訓練模型取得 Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  config.model_name,\n",
    ")\n",
    "# 檢視 Tokenizer，是否存在 padding token 及 padding side 等資訊\n",
    "pprint(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入預訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  config.model_name,\n",
    "  torch_dtype=config.torch_dtype,\n",
    "  low_cpu_mem_usage=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "pprint(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 詠唱格式化 (Prompt Formatting)\n",
    "\n",
    "先定義我們的詠唱 (Prompt) 格式。為此，我們將創建一個格式化函數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Set up the chat format with default 'chatml' format\n",
    "if tokenizer.chat_template is None:\n",
    "  model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "  print('=== 設定 chat format ===')\n",
    "  pprint(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def instruction_formatter(x):\n",
    "  return tokenizer.apply_chat_template(\n",
    "    [x['user']],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "input = instruction_formatter(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "pprint(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning 前的表現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 載入預訓練模型\n",
    "generator = pipeline(\n",
    "  task=config.pipeline_name,\n",
    "  model=model,\n",
    "  tokenizer=tokenizer,\n",
    "  device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 顯示預訓練模型預測結果\n",
    "response = generator(\n",
    "  input,\n",
    "  temperature=config.temperature,\n",
    "  max_new_tokens=config.max_new_tokens,\n",
    "  repetition_penalty=config.repetition_penalty,\n",
    ")\n",
    "pprint(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練模型\n",
    "\n",
    "隨著 `trl` 的最新版本發布，現在支持流行的指令 (instruction) 和對話 (conversation) 數據集格式。這意味著我們只需要將數據集轉換為支持的格式之一，`trl` 會處理其餘的部分。這些格式包括：\n",
    "\n",
    "* 指令格式 instruction format\n",
    "\n",
    "```json\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "```\n",
    "\n",
    "* 對話格式 conversational format\n",
    "\n",
    "```json\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "```\n",
    "\n",
    "我們所準備的資料集恰巧符合對話格式，因此我們可以直接使用它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 顯示單筆方便閱讀\n",
    "pprint(dataset[0]['messages'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA 的訓練策略 - 降維打擊\n",
    "\n",
    "LoRA（Low-Rank Adaptation）是一種用於訓練大型語言模型的技術，旨在提高訓練效率並減少計算資源的需求。以下是為何需要透過LoRA訓練的一些原因：\n",
    "\n",
    "降低計算成本：LoRA 通過將模型的權重矩陣分解為低秩矩陣，顯著減少了參數的數量，從而降低了計算成本和內存需求。\n",
    "\n",
    "加速訓練速度：由於參數數量減少，LoRA 可以加速模型的訓練過程，使得在相同的硬件資源下能夠更快地完成訓練。\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/lora_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 查看預訓練模型可訓練的參數量，其數量相當龐大，所以需要透過 Low Rank Approximation (LORA) 來降低參數量\n",
    "print('Parameters: {:,}, Trainable Parameters: {:,}'.format(\n",
    "  model.num_parameters(),\n",
    "  model.num_parameters(only_trainable=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PEFT 配置\n",
    "\n",
    "`lora_alpha`: 決定小模型的影響程度，也就是說 Alpha 值越高，越容易把大模型既有的能力給覆蓋掉。\n",
    "\n",
    "`target_module`: 要降維的模型層，可以透過 `model.named_parameters` 查看。\n",
    "\n",
    "對於許多語言微調任務，由於引入了新標記 (Token)，擴展模型的詞彙表是必要的。這需要擴展嵌入層 (Embedding) 以考慮新標記 (Token)，並在保存適配器時存儲嵌入層 (Embedding) 以及適配器權重。\n",
    "\n",
    "通過將嵌入層 (Embedding) 添加到配置的 `target_modules` 中來保存嵌入層 (Embedding)。嵌入層 (Embedding) 的名稱必須遵循 Transformers 的標準命名方案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# PEFT 配置\n",
    "lora_config = LoraConfig(\n",
    "  task_type=TaskType.CAUSAL_LM,\n",
    "  r=config.rank,\n",
    "  lora_alpha=config.lora_alpha,\n",
    "  # target_modules=[\n",
    "  #   'embed_tokens', 'lm_head', # embedding layer\n",
    "  #   'q_proj', 'k_proj', 'v_proj', 'out_proj', # attention layer\n",
    "  # ],\n",
    ")\n",
    "pprint(lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 取得 PEFT 模型\n",
    "\n",
    "搭配預訓模型及 PEFT 配置，我們可以取得 PEFT 模型。我們可以觀察受到降維影響的模型層。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 取得 PEFT 模型\n",
    "peft_model = get_peft_model(\n",
    "  model, # 預訓練模型\n",
    "  lora_config, # PEFT 配置\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "pprint(lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 取得 PEFT 模型, 觀察受 PEFT 影響的模型參數\n",
    "peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 調整 PEFT 模型精度\n",
    "\n",
    "PEFT 模型的精度是 `torch.float32`，我們可以透過 `model.half()` 將其轉換為半精度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 獲取 PERF 模型參數名稱及型態，確認是否使用半精度浮點數\n",
    "for name, param in peft_model.named_parameters():\n",
    "  print(f'{name}: {param.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "if config.torch_dtype == torch.float16 or config.torch_dtype == torch.bfloat16:\n",
    "  peft_model = peft_model.half() # 轉換為半精度浮點數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 查看可訓練的參數量\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定義訓練參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def formatting_prompts_func(x):\n",
    "  return tokenizer.apply_chat_template(\n",
    "    x['messages'],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=False,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "input = formatting_prompts_func(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "pprint(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "response_template = '<|im_start|>assistant'\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "  tokenizer=tokenizer,\n",
    "  response_template=response_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "training_args = SFTConfig(\n",
    "  output_dir='sample_data/train_output_pii_masking',\n",
    "  learning_rate=config.lr,\n",
    "  per_device_train_batch_size=config.train_batch_size,\n",
    "  gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "  num_train_epochs=config.epochs,\n",
    "  weight_decay=config.weight_decay,\n",
    "  save_strategy='epoch', # 每個 epoch 儲存一次\n",
    "  report_to='none', # Disable wandb on colab\n",
    "  adam_epsilon=config.adam_epsilon, # 當使用半精度浮點數時，需要設定較大的 adam epsilon\n",
    "  packing=False, # Disable packing when using DataCollatorForCompletionOnlyLM\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    data_collator=collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 開始訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 開始訓練，這可能需要一些時間\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存 LoRA 模型參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# # 保存 Lora 参数\n",
    "# peft_model.save_pretrained(\n",
    "#   config.saved_lora_path,\n",
    "#   # warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
    "#   save_embedding_layers=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 合併 LoRA 模型參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# # 合併原始模型和 Lora 参数\n",
    "# new_model = PeftModel.from_pretrained(model, config.saved_lora_path)\n",
    "\n",
    "# # print(\"=== 合併前的模型結構 ===\")\n",
    "# print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# # 合併並卸載 Lora 参数\n",
    "# new_model.merge_and_unload()\n",
    "\n",
    "# # print(\"=== 合併後的模型結構 ===\")\n",
    "# print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# # 保存合併後的模型\n",
    "# new_model.save_pretrained(config.saved_model_path)\n",
    "# tokenizer.save_pretrained(config.saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 釋放資源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# # 釋放 GPU 記憶體\n",
    "# del new_model\n",
    "# del trainer\n",
    "\n",
    "# peft_model.to('cpu')\n",
    "# del peft_model\n",
    "\n",
    "# model.to('cpu')\n",
    "# del model\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評估模型\n",
    "\n",
    "### 載入微調後 Tokenizer\n",
    "\n",
    "從已經完成訓練的模型取得 Tokenizer，可以留意這個訓練時保存下來的 Tokenizer 仍保有訓練時的設定，包涵 `pad_token` 和 `padding_side`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#   config.saved_model_path\n",
    "# )\n",
    "# # 檢視 Tokenizer\n",
    "# pprint(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入微調後模型\n",
    "\n",
    "以半精度浮點數載入已經完成訓練的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 以半精度浮點數載入已經完成訓練的模型\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#   config.saved_model_path,\n",
    "#   low_cpu_mem_usage=True,\n",
    "#   torch_dtype=config.torch_dtype,\n",
    "# ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning 後的表現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 載入新模型\n",
    "generator = pipeline(\n",
    "  task=config.pipeline_name,\n",
    "  model=model,\n",
    "  tokenizer=tokenizer,\n",
    "  device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "pipeline.model = peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 顯示新模型預測結果\n",
    "input = instruction_formatter(dataset[0])\n",
    "response = generator(\n",
    "  input,\n",
    "  temperature=config.temperature,\n",
    "  max_new_tokens=config.max_new_tokens,\n",
    "  repetition_penalty=config.repetition_penalty,\n",
    ")\n",
    "pprint(response[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
