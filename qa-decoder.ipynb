{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# transformers not support NumPy 2.0 yet\n",
    "!pip install -q numpy~=1.26.4 transformers~=4.46.2\n",
    "!pip install -q datasets~=3.2.0 pydantic~=2.10.4\n",
    "!pip install -q peft~=0.14.0 trl~=0.13.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練問答模型\n",
    "\n",
    "在這個筆記本中，我們將展示如何使用 `transformers` 套件訓練問答模型。我們將使用 `transformers` 套件中的 `SFTTrainer` ([Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/sft_trainer)) 類別來微調一個 Decoder-Only 架構的 Phi-3.5 模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonas/.pyenv/versions/learning-transformer-with-huggingface-3.11.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "  AutoTokenizer,\n",
    "  AutoModelForCausalLM,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "from typing import Any\n",
    "from pydantic import BaseModel\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "\n",
    "# 載入 PEFT 相關套件\n",
    "from peft import LoraConfig, TaskType, PeftModel, get_peft_model\n",
    "# 載入 SFTTrainer 相關套件\n",
    "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM, setup_chat_format\n",
    "\n",
    "# 檢查是否有 GPU 可以使用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下載資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# The full `train` split, only 1% of dataset\n",
    "immutable_dataset = load_dataset(\"philschmid/dolly-15k-oai-style\", split=\"train[:1%]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### 資料包含什麼？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages'],\n",
       "    num_rows: 150\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 顯示原始資料中包含的 features 以及筆數\n",
    "immutable_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'When did Virgin Australia start operating?\\n'\n",
      "             'Virgin Australia, the trading name of Virgin Australia Airlines '\n",
      "             'Pty Ltd, is an Australian-based airline. It is the largest '\n",
      "             'airline by fleet size to use the Virgin brand. It commenced '\n",
      "             'services on 31 August 2000 as Virgin Blue, with two aircraft on '\n",
      "             'a single route. It suddenly found itself as a major airline in '\n",
      "             \"Australia's domestic market after the collapse of Ansett \"\n",
      "             'Australia in September 2001. The airline has since grown to '\n",
      "             'directly serve 32 cities in Australia, from hubs in Brisbane, '\n",
      "             'Melbourne and Sydney.',\n",
      "  'role': 'user'},\n",
      " {'content': 'Virgin Australia commenced services on 31 August 2000 as Virgin '\n",
      "             'Blue, with two aircraft on a single route.',\n",
      "  'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "# 檢視資料集中的第一筆資料\n",
    "pprint(immutable_dataset[0]['messages'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這個 JSON 資料結構是一個列表，包含兩個字典，每個字典代表一個對話的訊息。每個字典有兩個 Key：\n",
    "\n",
    "* `role`: 表示訊息的角色，是一個字符串，可以是 `user` 或 `assistant`。\n",
    "\n",
    "* `content`: 表示訊息的內容，是一個字符串。\n",
    "\n",
    "具體結構如下：\n",
    "\n",
    "第一個字典：\n",
    "`role`: `user`，表示這是使用者的訊息。\n",
    "`content`: 包含使用者提問和相關資訊。\n",
    "\n",
    "第二個字典：\n",
    "`role`: `assistant`，表示這是助理的訊息。\n",
    "`content`: 包含助理的回答。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料前處理\n",
    "\n",
    "方便演示及加快訓練速度，我們將對資料進行以下前處理：\n",
    "\n",
    "1. 將 `messages` 欄位分拆成 `user` 和 `assistant` 兩個欄位，方便演示。\n",
    "2. 將 `user` 或 `assistant` 欄位中的 `content` 長於 512 的資料過濾掉。\n",
    "3. 將 `assistant` 欄位中的 `content` 短於 128 的資料過濾掉。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages', 'user', 'assistant'],\n",
       "    num_rows: 42\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 將 messages 欄位分拆成 user 和 assistant 兩個欄位，方便演示。\n",
    "dataset = immutable_dataset.map(\n",
    "  lambda x: {\n",
    "    \"user\": x[\"messages\"][0],\n",
    "    \"assistant\": x[\"messages\"][1],\n",
    "  }\n",
    ")\n",
    "# 將 user 或 assistant 欄位中過長的 content 資料過濾掉\n",
    "dataset = dataset.filter(\n",
    "  lambda x: len(x[\"user\"][\"content\"]) <= 512 and len(x[\"assistant\"][\"content\"]) <= 512\n",
    ")\n",
    "# 將 assistant 欄位中過短的 content 資料過濾掉\n",
    "dataset = dataset.filter(\n",
    "  lambda x: len(x[\"assistant\"][\"content\"]) >= 128\n",
    ")\n",
    "# 顯示處理後的資料\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messages</th>\n",
       "      <th>user</th>\n",
       "      <th>assistant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'content': 'What is a polygon?', 'role': 'user'}, {'content': 'A polygon is a form in Geometry.  It is a single dimensional plane made of connecting lines and any number of vertices.  It is a closed chain of connected line segments or edges.  The vertices of the polygon are formed where two edges meet.  Examples of polygons are hexagons, pentagons, and octagons.  Any plane that does not contain edges or vertices is not a polygon.  An example of a non-polygon is a circle.', 'role': 'assistant'}]</td>\n",
       "      <td>{'content': 'What is a polygon?', 'role': 'user'}</td>\n",
       "      <td>{'content': 'A polygon is a form in Geometry.  It is a single dimensional plane made of connecting lines and any number of vertices.  It is a closed chain of connected line segments or edges.  The vertices of the polygon are formed where two edges meet.  Examples of polygons are hexagons, pentagons, and octagons.  Any plane that does not contain edges or vertices is not a polygon.  An example of a non-polygon is a circle.', 'role': 'assistant'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'content': 'How do I start running?', 'role': 'user'}, {'content': 'Make sure you get comfortable running shoes and attire. Start with achievable goal in mind like a 5K race. If you never ran before, start gradually from a walk, to brisk walk, light jog aiming for 15-30mins initially. Slowly increase your running time and distance as your fitness level improves. One of the most important things is cool down and gentle stretching. Always listen to your body, and take rest days when needed to prevent injury.', 'role': 'assistant'}]</td>\n",
       "      <td>{'content': 'How do I start running?', 'role': 'user'}</td>\n",
       "      <td>{'content': 'Make sure you get comfortable running shoes and attire. Start with achievable goal in mind like a 5K race. If you never ran before, start gradually from a walk, to brisk walk, light jog aiming for 15-30mins initially. Slowly increase your running time and distance as your fitness level improves. One of the most important things is cool down and gentle stretching. Always listen to your body, and take rest days when needed to prevent injury.', 'role': 'assistant'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    messages  \\\n",
       "0                                      [{'content': 'What is a polygon?', 'role': 'user'}, {'content': 'A polygon is a form in Geometry.  It is a single dimensional plane made of connecting lines and any number of vertices.  It is a closed chain of connected line segments or edges.  The vertices of the polygon are formed where two edges meet.  Examples of polygons are hexagons, pentagons, and octagons.  Any plane that does not contain edges or vertices is not a polygon.  An example of a non-polygon is a circle.', 'role': 'assistant'}]   \n",
       "1  [{'content': 'How do I start running?', 'role': 'user'}, {'content': 'Make sure you get comfortable running shoes and attire. Start with achievable goal in mind like a 5K race. If you never ran before, start gradually from a walk, to brisk walk, light jog aiming for 15-30mins initially. Slowly increase your running time and distance as your fitness level improves. One of the most important things is cool down and gentle stretching. Always listen to your body, and take rest days when needed to prevent injury.', 'role': 'assistant'}]   \n",
       "\n",
       "                                                     user  \\\n",
       "0       {'content': 'What is a polygon?', 'role': 'user'}   \n",
       "1  {'content': 'How do I start running?', 'role': 'user'}   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         assistant  \n",
       "0                                 {'content': 'A polygon is a form in Geometry.  It is a single dimensional plane made of connecting lines and any number of vertices.  It is a closed chain of connected line segments or edges.  The vertices of the polygon are formed where two edges meet.  Examples of polygons are hexagons, pentagons, and octagons.  Any plane that does not contain edges or vertices is not a polygon.  An example of a non-polygon is a circle.', 'role': 'assistant'}  \n",
       "1  {'content': 'Make sure you get comfortable running shoes and attire. Start with achievable goal in mind like a 5K race. If you never ran before, start gradually from a walk, to brisk walk, light jog aiming for 15-30mins initially. Slowly increase your running time and distance as your fitness level improves. One of the most important things is cool down and gentle stretching. Always listen to your body, and take rest days when needed to prevent injury.', 'role': 'assistant'}  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 顯示前 first_n_data 筆資料\n",
    "first_n_data = 2\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.DataFrame(dataset.select(range(first_n_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練參數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批次大小 (Batch Size) 和 梯度累積步數 (Gradient Accumulation Steps)\n",
    "\n",
    "批次大小（batch size）和梯度累積步數（gradient accumulation steps）之間的關係可以簡單地說明如下：\n",
    "\n",
    "* 批次大小（batch size）：每次訓練迭代中使用的樣本數量。較大的批次大小通常需要更多的內存。\n",
    "* 梯度累積步數（gradient accumulation steps）：在更新模型權重之前累積梯度的迭代次數。這允許使用較小的批次大小來模擬較大的批次大小。\n",
    "\n",
    "當內存限制無法直接使用大批次大小時，可以通過梯度累積來實現。例如：\n",
    "\n",
    "* 如果批次大小是 8，梯度累積步數是 4，這相當於使用批次大小為 32（8 * 4）進行訓練。\n",
    "\n",
    "這樣可以在內存有限的情況下實現大批次大小的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 半精度浮點數\n",
    "\n",
    "半精度訓練（Half-Precision Training）是一種使用 16 位浮點數（FP16）而不是 32 位浮點數（FP32）來訓練神經網絡的方法。這種方法的主要優點包括：\n",
    "\n",
    "* 減少內存使用：FP16 數據類型佔用的內存比 FP32 少一半，允許在相同的硬件上訓練更大的模型或使用更大的批次大小。\n",
    "* 加速計算：許多現代 GPU 對 FP16 計算進行了優化，可以更快地執行 FP16 運算，從而加速訓練過程。\n",
    "* 節省帶寬：減少內存和帶寬的使用，有助於提高數據傳輸效率。\n",
    "\n",
    "BFP16 (Brain Floating Point 16)\n",
    "BFP16 是一種 16 位浮點數格式，主要由 Google 用於其 TPU（Tensor Processing Unit）。BFP16 的優點是它保留了與 FP32 相同的指數範圍，但尾數精度較低，這在某些情況下可以提供更好的數值穩定性。\n",
    "\n",
    "FP16 (Half-Precision Floating Point)\n",
    "FP16 是一種標準的 16 位浮點數格式，廣泛用於 GPU 加速的深度學習訓練。FP16 的優點是內存佔用少，計算速度快，但指數範圍和尾數精度都比 FP32 小。\n",
    "\n",
    "![](https://miro.medium.com/v2/0*HapPSei5sok65wcv)\n",
    "\n",
    "總體來說，半精度訓練可以在不顯著影響模型性能的情況下，提高訓練效率和資源利用率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 訓練相關設定\n",
    "class Config(BaseModel):\n",
    "  model_name: str = 'microsoft/Phi-3.5-mini-instruct'\n",
    "  torch_dtype: Any = torch.bfloat16 # 半精度浮點數\n",
    "  adam_epsilon: float = 1e-4 # 當使用半精度浮點數時，需要設定較大的 adam epsilon\n",
    "  saved_model_path: str = 'sample_data/saved_encoder_model' # path to save the trained model\n",
    "  saved_lora_path: str = 'sample_data/saved_lora_model' # path to save the trained LORA model\n",
    "  batch_size: int = 2 # size of the input batch in training and evaluation\n",
    "  gradient_accumulation_steps: int = 2 # number of updates steps to accumulate before performing a backward/update pass\n",
    "  epochs: int = 50 # number of times to iterate over the entire training dataset\n",
    "  lr: float = 2e-4 # learning rate, controls how fast or slow the model learns\n",
    "  weight_decay: float = 0.01 # weight decay, helps the model stay simple and avoid overfitting by penalizing large weights.\n",
    "\n",
    "  # 文本生成相關設定\n",
    "  temperature: float = 0.1 # temperature for sampling\n",
    "  max_new_tokens: int = 125 # 限制最大生成字數\n",
    "  repetition_penalty: float = 1.5 # 重複機率, 1~2 之間, 1.0 (no penalty), 2.0 (maximum penalty)\n",
    "\n",
    "  # LORA 相關設定\n",
    "  rank: int = 128 # rank of the Lora layers\n",
    "  lora_alpha: int = rank * 2 # alpha for Lora scaling.\n",
    "  lora_dropout: float = 0.05 # dropout probability for Lora layers\n",
    "\n",
    "if device.type == 'mps': # 方便在 Apple Silicon 上快速測試\n",
    "  config = Config(\n",
    "    torch_dtype=torch.float16, # 在 Apple Silicon 若使用預訓練模型 opt-125m 需要使用全精度浮點數，否則會出現錯誤\n",
    "  )\n",
    "else:\n",
    "  config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning 前的表現"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入預訓練分詞器 (Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaTokenizerFast(name_or_path='microsoft/Phi-3.5-mini-instruct', vocab_size=32000, model_max_length=131072, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '<|endoftext|>', 'unk_token': '<unk>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t32000: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32001: AddedToken(\"<|assistant|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32002: AddedToken(\"<|placeholder1|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32003: AddedToken(\"<|placeholder2|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32004: AddedToken(\"<|placeholder3|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32005: AddedToken(\"<|placeholder4|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32006: AddedToken(\"<|system|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32007: AddedToken(\"<|end|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32008: AddedToken(\"<|placeholder5|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32009: AddedToken(\"<|placeholder6|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32010: AddedToken(\"<|user|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 透過預訓練模型取得 Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  config.model_name,\n",
    ")\n",
    "pprint(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<|endoftext|>'\n"
     ]
    }
   ],
   "source": [
    "# 檢視 Tokenizer，是否存在 padding token 及 padding side 等資訊\n",
    "pprint(tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'left'\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 如果沒有定義 `pad_token`，請定義一個 `pad_token`，並將其加入 Tokenizer 中。\n",
    "* 如果 `padding_side` 不是 `right`，請將其設定為 `right`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 設定 Padding Side ===\n",
      "LlamaTokenizerFast(name_or_path='microsoft/Phi-3.5-mini-instruct', vocab_size=32000, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '<|endoftext|>', 'unk_token': '<unk>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t32000: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32001: AddedToken(\"<|assistant|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32002: AddedToken(\"<|placeholder1|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32003: AddedToken(\"<|placeholder2|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32004: AddedToken(\"<|placeholder3|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32005: AddedToken(\"<|placeholder4|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32006: AddedToken(\"<|system|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32007: AddedToken(\"<|end|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32008: AddedToken(\"<|placeholder5|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32009: AddedToken(\"<|placeholder6|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32010: AddedToken(\"<|user|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Add pad_token to the tokenizer\n",
    "if tokenizer.pad_token is None:\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  print('=== 設定 Padding Token ===')\n",
    "  pprint(tokenizer)\n",
    "# Make sure padding_side is 'right'\n",
    "if tokenizer.padding_side != 'right':\n",
    "  tokenizer.padding_side = 'right'\n",
    "  print('=== 設定 Padding Side ===')\n",
    "  pprint(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入預訓練模型\n",
    "\n",
    "由於 GPU 記憶體有限，我們將使用半精度進行模型 Fine-tuning。這邊需要留意，使用半精度進行 Fine-tuning 時，`TrainingArguments` 中的 `adam_epsilon` 需要設定為 `1e-4`。預設的 `adam_epsilon` 是 `1e-8`，這個值在半精度訓練時會出現問題。\n",
    "\n",
    "透過 `AutoModelForCausalLM` 用於因果語言建模的自動類別，它可以載入不同的預訓練模型進行文本生成任務。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.94s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  config.model_name,\n",
    "  torch_dtype=config.torch_dtype,\n",
    "  # 這個參數用於優化內存使用，減少模型加載時的 CPU 內存佔用，特別是在內存有限的環境中非常有用。\n",
    "  low_cpu_mem_usage=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phi3ForCausalLM(\n",
      "  (model): Phi3Model(\n",
      "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
      "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x Phi3DecoderLayer(\n",
      "        (self_attn): Phi3SdpaAttention(\n",
      "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "          (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Phi3MLP(\n",
      "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "          (activation_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
      "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "pprint(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這是一個典型的 Decoder-Only Transformer 模型。\n",
    "\n",
    "```json\n",
    "      (0-31): 32 x Phi3DecoderLayer(\n",
    "        (self_attn): Phi3SdpaAttention(\n",
    "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
    "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
    "          (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n",
    "        )\n",
    "        (mlp): Phi3MLP(\n",
    "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
    "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
    "          (activation_fn): SiLU()\n",
    "        )\n",
    "        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
    "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
    "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
    "        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
    "      )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 配置聊天樣本 (Chat Template)\n",
    "\n",
    "在語言模型中添加特殊標記對於訓練聊天模型至關重要。這些標記被添加在對話中不同角色之間，例如 `user`、`assistant` 和 `system`，幫助模型識別對話的結構和流程。這種設置對於使模型在聊天環境中生成連貫且上下文適當的回應是必不可少的。\n",
    "\n",
    "`trl` 中的 `setup_chat_format()` 函數可以輕鬆地為對話式 AI 任務設置模型和分詞器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Set up the chat format with default 'chatml' format\n",
    "if tokenizer.chat_template is None:\n",
    "  model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "  print('=== 設定 chat format ===')\n",
    "  pprint(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由於我們使用的模型已經是一個聊天模型，我們不需要再次設置對話格式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 詠唱格式化 (Prompt Formatting)\n",
    "\n",
    "定義詠唱 (Prompt) 格式，我們將創建一個格式化函數。\n",
    "\n",
    "請注意，這次我們指定 `add_generation_prompt` 為 `True`，表示回應開始的標記。這確保了當模型生成文本時，它會寫出機器人的回應，而不是做一些意想不到的事情，比如繼續用戶的訊息。請記住，聊天模型仍然只是語言模型，它們被訓練來玩文字接龍，而聊天對它們來說只是一種特殊的文本！你需要用適當的控制標記來引導它們，讓它們知道應該做什麼。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def instruction_formatter(x, tokenize):\n",
    "  if tokenize:\n",
    "    return tokenizer.apply_chat_template(\n",
    "      [x['user']],\n",
    "      tokenize=tokenize,\n",
    "      add_generation_prompt=True,\n",
    "      return_tensors='pt',\n",
    "      return_dict=True,\n",
    "    ).to(device)\n",
    "  else:\n",
    "    return tokenizer.apply_chat_template(\n",
    "      [x['user']],\n",
    "      tokenize=tokenize,\n",
    "      add_generation_prompt=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<|user|>\\nWhat is a polygon?<|end|>\\n<|assistant|>\\n'\n"
     ]
    }
   ],
   "source": [
    "# tokenize=False 代表不進行 Tokenize，直接回傳原始文字\n",
    "input = instruction_formatter(dataset[0], tokenize=False)\n",
    "pprint(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='mps:0'),\n",
      " 'input_ids': tensor([[32010,  1724,   338,   263, 29807, 29973, 32007, 32001]],\n",
      "       device='mps:0')}\n"
     ]
    }
   ],
   "source": [
    "# tokenize=True 代表進行 Tokenize，回傳 Tokenize 後的 ID 及 attention mask tensors\n",
    "tokenized_input = instruction_formatter(dataset[0], tokenize=True)\n",
    "pprint(tokenized_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer 回傳內容包含兩個主要部分：`input_ids` 和 `attention_mask`。以下是詳細解釋：\n",
    "\n",
    "* `input_ids`: 是一個張量 (tensor)，包含了輸入文本的 token IDs。這些 IDs 是由 tokenizer 將文本轉換為數字表示後得到的。\n",
    "\n",
    "* `attention_mask`: 同樣是一個張量，用於指示模型應該關注哪些位置。值為 1 的位置表示應該關注，值為 0 的位置表示應該忽略。在這個例子中，`attention_mask` 的值全為 1，表示模型應該關注所有位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32010 -> <|user|>\n",
      "1724 -> What\n",
      "338 -> is\n",
      "263 -> a\n",
      "29807 -> polygon\n",
      "29973 -> ?\n",
      "32007 -> <|end|>\n",
      "32001 -> <|assistant|>\n"
     ]
    }
   ],
   "source": [
    "# 透過 Tokenizer 的 decode 方法將 ID 轉換回文字，並列顯示出來\n",
    "for id in tokenized_input['input_ids'][0]:\n",
    "  print(f'{id} -> {tokenizer.decode([id])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning 前的表現"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 單筆演示生成回應"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonas/.pyenv/versions/learning-transformer-with-huggingface-3.11.11/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/Users/jonas/.pyenv/versions/learning-transformer-with-huggingface-3.11.11/lib/python3.11/site-packages/transformers/pytorch_utils.py:325: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n"
     ]
    }
   ],
   "source": [
    "# 透過預訓練模型生成回應\n",
    "output_ids = model.generate(\n",
    "  **tokenized_input,\n",
    "  temperature=config.temperature,\n",
    "  max_new_tokens=config.max_new_tokens,\n",
    "  repetition_penalty=config.repetition_penalty,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[32010,  1724,   338,   263, 29807, 29973, 32007, 32001,   319,  3579,\n",
       "          3733, 17125,  1068, 14637,   304,   738,  1023, 29899, 12531, 26224,\n",
       "          4377,   393, 11624,   310,  7812, 29892,  1661,  1639,  8803,   292,\n",
       "          1196, 24611,   470,   376, 29879,  2247, 29908,   607,  2094,  2226,\n",
       "          8162, 29889,  4525, 11192,   526,  6631,   472,  1009,  1095,  9748,\n",
       "          2000, 13791,   313,  2976,  1070, 29901, 12688,   467,   450, 13290,\n",
       "          5120,  8429,   491,  1438,  3454,   322,   278,   427, 15603,  2913,\n",
       "          2768,   372,   508,   367, 10423,   411,  2927,   565,  7429,   363,\n",
       "          7604,  2133, 11976, 29936,   445,  4038,  2629,   541,   451,  3704,\n",
       "           967, 10452, 17645,   825,   591,  1246,   525,  1552, 10694, 29915,\n",
       "           297, 16303,  4958,   746,  5353,  1048,  1248,  4790,   787, 10816,\n",
       "           373, 12151, 28001,   763,  5650, 11053,  2992,  1696,  2466,  5722,\n",
       "          1711, 13590,   896,  1863, 16978,   408,  1472,   727, 30010,   276,\n",
       "           694,  3151,  1490]], device='mps:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將 output_ids 轉換為文字\n",
    "output = tokenizer.decode(\n",
    "  output_ids[0],\n",
    "  skip_special_tokens=False, # 決定是否跳過特殊 token（例如，開始和結束標記）。\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<|user|> What is a polygon?<|end|><|assistant|> A **polygon** refers to any '\n",
      " 'two-dimensional geometric figure that consists of straight, nonintersecting '\n",
      " 'line segments or \"sides\" which enclose spaces. These sides are connected at '\n",
      " 'their endpoints called vertices (singular: vertex). The interior region '\n",
      " 'formed by these lines and the enclosed space inside it can be filled with '\n",
      " 'color if desired for visualization purposes; this area within but not '\n",
      " \"including its boundary defines what we call 'the plane' in geometry terms \"\n",
      " 'when discuss about polygons specifically on flat surfaces like paper maps '\n",
      " 'etc., though technically speaking they exist everywhere as long there’re no '\n",
      " 'curved')\n"
     ]
    }
   ],
   "source": [
    "pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A **polygon** refers to any two-dimensional geometric figure that consists '\n",
      " 'of straight, nonintersecting line segments or \"sides\" which enclose spaces. '\n",
      " 'These sides are connected at their endpoints called vertices (singular: '\n",
      " 'vertex). The interior region formed by these lines and the enclosed space '\n",
      " 'inside it can be filled with color if desired for visualization purposes; '\n",
      " \"this area within but not including its boundary defines what we call 'the \"\n",
      " \"plane' in geometry terms when discuss about polygons specifically on flat \"\n",
      " 'surfaces like paper maps etc., though technically speaking they exist '\n",
      " 'everywhere as long there’re no curved')\n"
     ]
    }
   ],
   "source": [
    "# 只取得生成的文字, 即 <|assistant|> 之後的文字\n",
    "pprint(output.split('<|assistant|>')[1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 批次處理模型表現\n",
    "\n",
    "初步了解如何生成模型的回應，我們將定義一個 `generate()` 函數來生成模型的回應。這個函數接受一個輸入文本，並生成模型的回應。藉由這個函數，我們可以批次處理資料。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將以上程式碼整理成一個函式，方便我們批次處理資料\n",
    "def generator(x, model):\n",
    "  tokenized_input = instruction_formatter(x, tokenize=True)\n",
    "  output_ids = model.generate(\n",
    "    **tokenized_input,\n",
    "    temperature=config.temperature,\n",
    "    max_new_tokens=config.max_new_tokens,\n",
    "    repetition_penalty=config.repetition_penalty,\n",
    "  )\n",
    "  output = tokenizer.decode(output_ids[0], skip_special_tokens=False)\n",
    "  return output.split('<|assistant|>')[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonas/.pyenv/versions/learning-transformer-with-huggingface-3.11.11/lib/python3.11/site-packages/dill/_dill.py:414: PicklingWarning: Cannot locate reference to <class '__main__.Config'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "/Users/jonas/.pyenv/versions/learning-transformer-with-huggingface-3.11.11/lib/python3.11/site-packages/dill/_dill.py:414: PicklingWarning: Cannot pickle <class '__main__.Config'>: __main__.Config has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████████| 2/2 [01:06<00:00, 33.14s/ examples]\n"
     ]
    }
   ],
   "source": [
    "# 這個步驟可能會花費一些時間，所以我們只處理前 first_n_data 筆資料\n",
    "first_n_dataset = dataset.select(range(first_n_data))\n",
    "\n",
    "# 移除 messages 欄位\n",
    "first_n_dataset = first_n_dataset.remove_columns('messages')\n",
    "\n",
    "# 透過預訓練模型生成回應，將其新增到 first_n_dataset 的 pt_response 欄位中\n",
    "first_n_dataset = first_n_dataset.map(\n",
    "  lambda x: {\n",
    "    **x,\n",
    "    \"pt_response\": generator(x, model),\n",
    "  },\n",
    "  batched=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>assistant</th>\n",
       "      <th>pt_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'content': 'What is a polygon?', 'role': 'user'}</td>\n",
       "      <td>{'content': 'A polygon is a form in Geometry.  It is a single dimensional plane made of connecting lines and any number of vertices.  It is a closed chain of connected line segments or edges.  The vertices of the polygon are formed where two edges meet.  Examples of polygons are hexagons, pentagons, and octagons.  Any plane that does not contain edges or vertices is not a polygon.  An example of a non-polygon is a circle.', 'role': 'assistant'}</td>\n",
       "      <td>A **polygon** refers to any two-dimensional geometric figure that consists of straight, nonintersecting line segments or \"sides\" which enclose spaces. These sides are connected at their endpoints called vertices (singular: vertex). The interior region formed by these lines and the enclosed space inside it can be filled with color if desired for visualization purposes; this area within but not including its boundary defines what we call 'the plane' in geometry terms when discuss about polygons specifically on flat surfaces like paper maps etc., though technically speaking they exist everywhere as long there’re no curved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'content': 'How do I start running?', 'role': 'user'}</td>\n",
       "      <td>{'content': 'Make sure you get comfortable running shoes and attire. Start with achievable goal in mind like a 5K race. If you never ran before, start gradually from a walk, to brisk walk, light jog aiming for 15-30mins initially. Slowly increase your running time and distance as your fitness level improves. One of the most important things is cool down and gentle stretching. Always listen to your body, and take rest days when needed to prevent injury.', 'role': 'assistant'}</td>\n",
       "      <td>Starting to run is a great way for improving your fitness, mental health and overall well-being. Here's how you can begin:\\n 1) Set realistic goals - Determine what distance or time frame works best with the current level of physical activity in mind (either beginner mileage guidelines like Couch25K/Coworkout30k if starting from scratch). Remember that progress takes patience!  \\t    * Choose appropriate footwear – Investment into good quality shoes will help prevent injuries downline by providing proper support during</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     user  \\\n",
       "0       {'content': 'What is a polygon?', 'role': 'user'}   \n",
       "1  {'content': 'How do I start running?', 'role': 'user'}   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         assistant  \\\n",
       "0                                 {'content': 'A polygon is a form in Geometry.  It is a single dimensional plane made of connecting lines and any number of vertices.  It is a closed chain of connected line segments or edges.  The vertices of the polygon are formed where two edges meet.  Examples of polygons are hexagons, pentagons, and octagons.  Any plane that does not contain edges or vertices is not a polygon.  An example of a non-polygon is a circle.', 'role': 'assistant'}   \n",
       "1  {'content': 'Make sure you get comfortable running shoes and attire. Start with achievable goal in mind like a 5K race. If you never ran before, start gradually from a walk, to brisk walk, light jog aiming for 15-30mins initially. Slowly increase your running time and distance as your fitness level improves. One of the most important things is cool down and gentle stretching. Always listen to your body, and take rest days when needed to prevent injury.', 'role': 'assistant'}   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          pt_response  \n",
       "0  A **polygon** refers to any two-dimensional geometric figure that consists of straight, nonintersecting line segments or \"sides\" which enclose spaces. These sides are connected at their endpoints called vertices (singular: vertex). The interior region formed by these lines and the enclosed space inside it can be filled with color if desired for visualization purposes; this area within but not including its boundary defines what we call 'the plane' in geometry terms when discuss about polygons specifically on flat surfaces like paper maps etc., though technically speaking they exist everywhere as long there’re no curved  \n",
       "1                                                                                                        Starting to run is a great way for improving your fitness, mental health and overall well-being. Here's how you can begin:\\n 1) Set realistic goals - Determine what distance or time frame works best with the current level of physical activity in mind (either beginner mileage guidelines like Couch25K/Coworkout30k if starting from scratch). Remember that progress takes patience!  \\t    * Choose appropriate footwear – Investment into good quality shoes will help prevent injuries downline by providing proper support during  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 顯示預訓練模型預測結果\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.DataFrame(first_n_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練資料格式\n",
    "\n",
    "隨著 `trl` 的最新版本發布，現在支持流行的指令 (instruction) 和對話 (conversation) 數據集格式。這意味著我們只需要將數據集轉換為支持的格式之一，`trl` 會處理其餘的部分。這些格式包括：\n",
    "\n",
    "* 指令格式 instruction format\n",
    "\n",
    "```json\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "```\n",
    "\n",
    "* 對話格式 conversational format\n",
    "\n",
    "```json\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "```\n",
    "\n",
    "我們所準備的資料集恰巧符合對話格式，因此我們可以直接使用它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'What is a polygon?', 'role': 'user'},\n",
      " {'content': 'A polygon is a form in Geometry.  It is a single dimensional '\n",
      "             'plane made of connecting lines and any number of vertices.  It '\n",
      "             'is a closed chain of connected line segments or edges.  The '\n",
      "             'vertices of the polygon are formed where two edges meet.  '\n",
      "             'Examples of polygons are hexagons, pentagons, and octagons.  Any '\n",
      "             'plane that does not contain edges or vertices is not a polygon.  '\n",
      "             'An example of a non-polygon is a circle.',\n",
      "  'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "# 顯示單筆方便閱讀\n",
    "pprint(dataset[0]['messages'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA 的訓練策略 - 降維打擊\n",
    "\n",
    "LoRA（Low-Rank Adaptation）是一種用於訓練大型語言模型的技術，旨在提高訓練效率並減少計算資源的需求。以下是為何需要透過LoRA訓練的一些原因：\n",
    "\n",
    "* 降低計算成本：LoRA 通過將模型的權重矩陣分解為低秩矩陣，顯著減少了參數的數量，從而降低了計算成本和內存需求。\n",
    "\n",
    "* 加速訓練速度：由於參數數量減少，LoRA 可以加速模型的訓練過程，使得在相同的硬件資源下能夠更快地完成訓練。\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/lora_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 3,821,079,552, Trainable Parameters: 3,821,079,552\n"
     ]
    }
   ],
   "source": [
    "# 查看預訓練模型可訓練的參數量，其數量相當龐大，所以需要透過 Low Rank Adaptation (LoRA) 來降低參數量\n",
    "print('Parameters: {:,}, Trainable Parameters: {:,}'.format(\n",
    "  model.num_parameters(),\n",
    "  model.num_parameters(only_trainable=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LoRA 配置\n",
    "\n",
    "* `task_type`: TaskType.CAUSAL_LM 指定任務類型為因果語言模型 (Causal Language Model)。\n",
    "\n",
    "* `rank`: 是低秩矩陣的秩(rank)，它決定了 LoRA 層的參數數量。較低的 `r` 值意味著較少的參數，從而減少了模型的計算和存儲需求。具體來說，LoRA 通過將全連接層的權重矩陣分解為兩個低秩矩陣來實現參數高效化。`r` 值越小，這兩個低秩矩陣的維度越小，這個練習我們採用 128。\n",
    "\n",
    "* `lora_alpha`: 是一個縮放因子，用於調整 LoRA 層的輸出。它控制了低秩矩陣的影響力。較高的 `lora_alpha` 值會增加 LoRA 層的影響力，也就是說值越高，越容易把大模型既有的能力給覆蓋掉。具體來說，LoRA 層的輸出會乘以這個縮放因子，這個練習我們採用常見的比例為 `rank` 的兩倍。\n",
    "\n",
    "* `lora_dropout`: 是一個丟棄率，用於在訓練過程中隨機丟棄 LoRA 層的一部分輸出。這有助於防止過擬合，並提高模型的泛化能力。例如，`lora_dropout` 設置為 0.1 表示在每次前向傳播中，有 10% 的 LoRA 層輸出會被隨機設置為零。\n",
    "\n",
    "* `target_module`: 指定了應用 LoRA 的目標模塊。這通常是模型中的某些特定層或子模塊，例如 Transformer 模型中的注意力層，可以透過 `model.named_parameters` 查看。通過指定 `target_module`，你可以靈活地選擇在哪些層應用 LoRA，以便在保持模型性能的同時減少參數數量。\n",
    "\n",
    "> 廣為周知的模型當未指定 `target_module`，透過 `get_peft_model` 加載 Lora 適配模型時，會自動設定。\n",
    "> 可以先嘗試不指定，若出現錯誤再試著設定注意力相關的參數層。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoraConfig(task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>,\n",
      "           peft_type=<PeftType.LORA: 'LORA'>,\n",
      "           auto_mapping=None,\n",
      "           base_model_name_or_path=None,\n",
      "           revision=None,\n",
      "           inference_mode=False,\n",
      "           r=128,\n",
      "           target_modules={'qkv_proj'},\n",
      "           exclude_modules=None,\n",
      "           lora_alpha=256,\n",
      "           lora_dropout=0.05,\n",
      "           fan_in_fan_out=False,\n",
      "           bias='none',\n",
      "           use_rslora=False,\n",
      "           modules_to_save=None,\n",
      "           init_lora_weights=True,\n",
      "           layers_to_transform=None,\n",
      "           layers_pattern=None,\n",
      "           rank_pattern={},\n",
      "           alpha_pattern={},\n",
      "           megatron_config=None,\n",
      "           megatron_core='megatron.core',\n",
      "           loftq_config={},\n",
      "           eva_config=None,\n",
      "           use_dora=False,\n",
      "           layer_replication=None,\n",
      "           runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False),\n",
      "           lora_bias=False)\n"
     ]
    }
   ],
   "source": [
    "# LoRA 配置\n",
    "lora_config = LoraConfig(\n",
    "  task_type=TaskType.CAUSAL_LM,\n",
    "  r=config.rank,\n",
    "  lora_alpha=config.lora_alpha,\n",
    "  lora_dropout=config.lora_dropout,\n",
    "  # Phi3ForCausalLM need to specify the target_modules beforehand\n",
    "  target_modules=['qkv_proj'],\n",
    ")\n",
    "\n",
    "pprint(lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加載 LoRA 適配模型\n",
    "\n",
    "搭配預訓模型及 LoRA 配置，我們可以加載 LoRA 適配模型。我們可以觀察受到降維影響的模型層。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 加載 LoRA 適配模型\n",
    "peft_model = get_peft_model(\n",
    "  model, # 預訓練模型\n",
    "  lora_config, # LoRA 配置\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoraConfig(task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>,\n",
      "           peft_type=<PeftType.LORA: 'LORA'>,\n",
      "           auto_mapping=None,\n",
      "           base_model_name_or_path='microsoft/Phi-3.5-mini-instruct',\n",
      "           revision=None,\n",
      "           inference_mode=False,\n",
      "           r=128,\n",
      "           target_modules={'qkv_proj'},\n",
      "           exclude_modules=None,\n",
      "           lora_alpha=256,\n",
      "           lora_dropout=0.05,\n",
      "           fan_in_fan_out=False,\n",
      "           bias='none',\n",
      "           use_rslora=False,\n",
      "           modules_to_save=None,\n",
      "           init_lora_weights=True,\n",
      "           layers_to_transform=None,\n",
      "           layers_pattern=None,\n",
      "           rank_pattern={},\n",
      "           alpha_pattern={},\n",
      "           megatron_config=None,\n",
      "           megatron_core='megatron.core',\n",
      "           loftq_config={},\n",
      "           eva_config=None,\n",
      "           use_dora=False,\n",
      "           layer_replication=None,\n",
      "           runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False),\n",
      "           lora_bias=False)\n"
     ]
    }
   ],
   "source": [
    "pprint(lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LoRA 適配模型\n",
    "\n",
    "加載 LoRA 適配模型後, 觀察受 LoRA 影響的模型參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Phi3ForCausalLM(\n",
       "      (model): Phi3Model(\n",
       "        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "        (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x Phi3DecoderLayer(\n",
       "            (self_attn): Phi3SdpaAttention(\n",
       "              (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "              (qkv_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=9216, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Phi3MLP(\n",
       "              (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "              (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "              (activation_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由於我們指定的 `target_module` 是 `qkv_proj`, 因此所有注意力層受到 LoRA 的影響。\n",
    "\n",
    "```json\n",
    "  (qkv_proj): lora.Linear(\n",
    "    (base_layer): Linear(in_features=3072, out_features=9216, bias=False)\n",
    "    (lora_dropout): ModuleDict(\n",
    "      (default): Dropout(p=0.05, inplace=False)\n",
    "    )\n",
    "    (lora_A): ModuleDict(\n",
    "      (default): Linear(in_features=3072, out_features=128, bias=False)\n",
    "    )\n",
    "    (lora_B): ModuleDict(\n",
    "      (default): Linear(in_features=128, out_features=9216, bias=False)\n",
    "    )\n",
    "    (lora_embedding_A): ParameterDict()\n",
    "    (lora_embedding_B): ParameterDict()\n",
    "    (lora_magnitude_vector): ModuleDict()\n",
    "  )\n",
    "```              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 調整 LoRA 精度\n",
    "\n",
    "LoRA 適配模型的精度是 `torch.float32`，我們可以透過 `model.half()` 將其轉換為半精度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight: torch.float16\n",
      "base_model.model.model.layers.0.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.0.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.0.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.0.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.0.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.0.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.0.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.1.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.1.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.1.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.1.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.1.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.1.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.1.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.2.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.2.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.2.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.2.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.2.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.2.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.2.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.3.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.3.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.3.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.3.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.3.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.3.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.3.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.4.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.4.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.4.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.4.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.4.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.4.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.4.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.5.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.5.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.5.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.5.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.5.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.5.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.5.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.6.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.6.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.6.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.6.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.6.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.6.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.6.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.7.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.7.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.7.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.7.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.7.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.7.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.7.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.8.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.8.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.8.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.8.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.8.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.8.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.8.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.9.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.9.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.9.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.9.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.9.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.9.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.9.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.10.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.10.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.10.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.10.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.10.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.10.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.10.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.11.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.11.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.11.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.11.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.11.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.11.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.11.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.12.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.12.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.12.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.12.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.12.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.12.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.12.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.13.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.13.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.13.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.13.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.13.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.13.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.13.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.14.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.14.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.14.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.14.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.14.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.14.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.14.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.15.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.15.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.15.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.15.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.15.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.15.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.15.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.16.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.16.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.16.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.16.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.16.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.16.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.16.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.16.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.17.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.17.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.17.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.17.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.17.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.17.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.17.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.17.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.18.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.18.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.18.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.18.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.18.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.18.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.18.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.18.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.19.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.19.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.19.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.19.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.19.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.19.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.19.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.19.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.20.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.20.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.20.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.20.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.20.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.20.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.20.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.20.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.21.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.21.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.21.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.21.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.21.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.21.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.21.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.21.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.22.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.22.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.22.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.22.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.22.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.22.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.22.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.22.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.23.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.23.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.23.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.23.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.23.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.23.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.23.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.23.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.24.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.24.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.24.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.24.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.24.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.24.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.24.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.24.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.25.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.25.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.25.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.25.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.25.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.25.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.25.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.25.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.26.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.26.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.26.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.26.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.26.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.26.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.26.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.26.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.27.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.27.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.27.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.27.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.27.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.27.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.27.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.27.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.28.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.28.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.28.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.28.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.28.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.28.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.28.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.28.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.29.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.29.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.29.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.29.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.29.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.29.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.29.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.29.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.30.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.30.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.30.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.30.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.30.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.30.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.30.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.30.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.31.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.31.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.31.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
      "base_model.model.model.layers.31.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
      "base_model.model.model.layers.31.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.31.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.31.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.31.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.norm.weight: torch.float16\n",
      "base_model.model.lm_head.weight: torch.float16\n"
     ]
    }
   ],
   "source": [
    "# 獲取 LoRA 模型參數名稱及型態，確認是否使用半精度浮點數\n",
    "for name, param in peft_model.named_parameters():\n",
    "  print(f'{name}: {param.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以發現除了預訓練模型的權重是半精度外，LoRA 適配模型的權重仍然是全精度。\n",
    "\n",
    "```shell\n",
    "base_model.model.model.layers.0.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
    "base_model.model.model.layers.0.self_attn.qkv_proj.lora_A.default.weight: torch.float32\n",
    "base_model.model.model.layers.0.self_attn.qkv_proj.lora_B.default.weight: torch.float32\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "if config.torch_dtype == torch.float16 or config.torch_dtype == torch.bfloat16:\n",
    "  peft_model = peft_model.half() # 轉換為半精度浮點數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight: torch.float16\n",
      "base_model.model.model.layers.0.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.0.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.0.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
      "base_model.model.model.layers.0.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
      "base_model.model.model.layers.0.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.0.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.0.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.1.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.1.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.1.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
      "base_model.model.model.layers.1.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
      "base_model.model.model.layers.1.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.1.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.1.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.2.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.2.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.2.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
      "base_model.model.model.layers.2.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
      "base_model.model.model.layers.2.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.2.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.2.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.3.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.3.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.3.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
      "base_model.model.model.layers.3.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
      "base_model.model.model.layers.3.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.3.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.3.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.4.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.4.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.4.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
      "base_model.model.model.layers.4.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
      "base_model.model.model.layers.4.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.4.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.4.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.5.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.5.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.5.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
      "base_model.model.model.layers.5.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
      "base_model.model.model.layers.5.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.5.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.5.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.6.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.6.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.6.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
      "base_model.model.model.layers.6.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
      "base_model.model.model.layers.6.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.6.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.6.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.7.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.7.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.7.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
      "base_model.model.model.layers.7.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
      "base_model.model.model.layers.7.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.7.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.7.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.8.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.8.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.8.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
      "base_model.model.model.layers.8.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
      "base_model.model.model.layers.8.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.8.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.8.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.9.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.9.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.9.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
      "base_model.model.model.layers.9.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
      "base_model.model.model.layers.9.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.9.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.9.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.10.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.10.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.10.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
      "base_model.model.model.layers.10.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
      "base_model.model.model.layers.10.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.10.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.10.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.11.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.11.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.11.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
      "base_model.model.model.layers.11.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
      "base_model.model.model.layers.11.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.11.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.11.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.12.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.12.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.12.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
      "base_model.model.model.layers.12.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
      "base_model.model.model.layers.12.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.12.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.12.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.13.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.13.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.13.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
      "base_model.model.model.layers.13.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
      "base_model.model.model.layers.13.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.13.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.13.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.14.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.14.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.14.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
      "base_model.model.model.layers.14.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
      "base_model.model.model.layers.14.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.14.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.14.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.15.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.15.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.15.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
      "base_model.model.model.layers.15.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
      "base_model.model.model.layers.15.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.15.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.15.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.16.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.16.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.16.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
      "base_model.model.model.layers.16.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
      "base_model.model.model.layers.16.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.16.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.16.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.16.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.17.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.17.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.17.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
      "base_model.model.model.layers.17.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
      "base_model.model.model.layers.17.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.17.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.17.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.17.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.18.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.18.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.18.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
      "base_model.model.model.layers.18.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
      "base_model.model.model.layers.18.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.18.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.18.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.18.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.19.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.19.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.19.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
      "base_model.model.model.layers.19.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
      "base_model.model.model.layers.19.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.19.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.19.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.19.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.20.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.20.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.20.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
      "base_model.model.model.layers.20.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
      "base_model.model.model.layers.20.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.20.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.20.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.20.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.21.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.21.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.21.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
      "base_model.model.model.layers.21.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
      "base_model.model.model.layers.21.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.21.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.21.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.21.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.22.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.22.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.22.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
      "base_model.model.model.layers.22.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
      "base_model.model.model.layers.22.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.22.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.22.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.22.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.23.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.23.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.23.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
      "base_model.model.model.layers.23.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
      "base_model.model.model.layers.23.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.23.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.23.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.23.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.24.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.24.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.24.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
      "base_model.model.model.layers.24.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
      "base_model.model.model.layers.24.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.24.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.24.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.24.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.25.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.25.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.25.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
      "base_model.model.model.layers.25.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
      "base_model.model.model.layers.25.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.25.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.25.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.25.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.26.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.26.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.26.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
      "base_model.model.model.layers.26.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
      "base_model.model.model.layers.26.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.26.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.26.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.26.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.27.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.27.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.27.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
      "base_model.model.model.layers.27.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
      "base_model.model.model.layers.27.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.27.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.27.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.27.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.28.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.28.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.28.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
      "base_model.model.model.layers.28.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
      "base_model.model.model.layers.28.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.28.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.28.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.28.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.29.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.29.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.29.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
      "base_model.model.model.layers.29.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
      "base_model.model.model.layers.29.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.29.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.29.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.29.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.30.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.30.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.30.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
      "base_model.model.model.layers.30.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
      "base_model.model.model.layers.30.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.30.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.30.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.30.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.31.self_attn.o_proj.weight: torch.float16\n",
      "base_model.model.model.layers.31.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
      "base_model.model.model.layers.31.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
      "base_model.model.model.layers.31.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
      "base_model.model.model.layers.31.mlp.gate_up_proj.weight: torch.float16\n",
      "base_model.model.model.layers.31.mlp.down_proj.weight: torch.float16\n",
      "base_model.model.model.layers.31.input_layernorm.weight: torch.float16\n",
      "base_model.model.model.layers.31.post_attention_layernorm.weight: torch.float16\n",
      "base_model.model.model.norm.weight: torch.float16\n",
      "base_model.model.lm_head.weight: torch.float16\n"
     ]
    }
   ],
   "source": [
    "# 獲取 LoRA 模型參數名稱及型態，確認是否使用半精度浮點數\n",
    "for name, param in peft_model.named_parameters():\n",
    "  print(f'{name}: {param.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "經過 `model.half()` 轉換後，LoRA 適配模型的權重也變成半精度。\n",
    "\n",
    "```shell\n",
    "base_model.model.model.layers.0.self_attn.qkv_proj.base_layer.weight: torch.float16\n",
    "base_model.model.model.layers.0.self_attn.qkv_proj.lora_A.default.weight: torch.float16\n",
    "base_model.model.model.layers.0.self_attn.qkv_proj.lora_B.default.weight: torch.float16\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練參數量也從原先 3B 大大減少為 50M。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 50,331,648 || all params: 3,871,411,200 || trainable%: 1.3001\n"
     ]
    }
   ],
   "source": [
    "# 查看可訓練的參數量\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 詠唱格式化 (Prompt Formatting)\n",
    "\n",
    "有別於先前的詠唱格式，這次我們將包含 `assistant` 的回應，以便作為標注資料供模型訓練。由於已經包含 `assistant`，這次我們指定 `add_generation_prompt` 為 `False`，省卻回應開始的標記。\n",
    "\n",
    "另一個差異是，這個函式預設不會進行 tokenize，會直接回傳原始文字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def instruction_completion_formatter(x, tokenize: bool = False):\n",
    "  return tokenizer.apply_chat_template(\n",
    "    x['messages'],\n",
    "    tokenize=tokenize,\n",
    "    add_generation_prompt=False,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<|user|>\\n'\n",
      " 'What is a polygon?<|end|>\\n'\n",
      " '<|assistant|>\\n'\n",
      " 'A polygon is a form in Geometry.  It is a single dimensional plane made of '\n",
      " 'connecting lines and any number of vertices.  It is a closed chain of '\n",
      " 'connected line segments or edges.  The vertices of the polygon are formed '\n",
      " 'where two edges meet.  Examples of polygons are hexagons, pentagons, and '\n",
      " 'octagons.  Any plane that does not contain edges or vertices is not a '\n",
      " 'polygon.  An example of a non-polygon is a circle.<|end|>\\n'\n",
      " '<|endoftext|>')\n"
     ]
    }
   ],
   "source": [
    "pprint(instruction_completion_formatter(dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料校對器 (Data Collator)\n",
    "\n",
    "在微調語言模型時，使用 data collator 是為了有效地準備和處理批次數據。以下是使用 data collator 的幾個主要原因：\n",
    "\n",
    "* 動態填充 (Dynamic Padding): 不同長度的序列需要填充到相同的長度，以便能夠在同一批次中進行處理。Data collator 可以自動計算每個批次的最大長度，並對序列進行適當的填充。\n",
    "\n",
    "* 批次處理 (Batch Processing): Data collator 可以將多個樣本組合成一個批次，這樣可以更高效地利用計算資源，特別是在使用 GPU 或 TPU 時。\n",
    "\n",
    "* 生成注意力掩碼 (Attention Masks): 在填充序列時，data collator 會生成相應的注意力掩碼 (attention masks)，以確保模型只關注實際的數據部分，而忽略填充部分。\n",
    "\n",
    "* 簡化代碼 (Code Simplification): 使用 data collator 可以簡化數據處理的代碼，減少手動處理數據的繁瑣步驟，讓開發者專注於模型設計和訓練。\n",
    "\n",
    "總之，data collator 在微調語言模型時提供了便利和效率，確保數據能夠以一致且高效的方式進行處理。\n",
    "\n",
    "在這邊我們使用 `DataCollatorForCompletionOnlyLM` 是一個專門用於處理語言模型補全任務的數據整理器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 定義回應開始的標記\n",
    "response_template = '<|assistant|>'\n",
    "\n",
    "# 設定 DataCollatorForCompletionOnlyLM\n",
    "data_collator = DataCollatorForCompletionOnlyLM(\n",
    "  tokenizer=tokenizer,\n",
    "  response_template=response_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[32010,  1724,   338,   263, 29807, 29973, 32007, 32001,   319, 29807,\n",
      "           338,   263,   883,   297,  1879,  7843, 29889, 29871,   739,   338,\n",
      "           263,  2323, 22112, 10694,  1754,   310, 16791,  3454,   322,   738,\n",
      "          1353,   310, 13791, 29889, 29871,   739,   338,   263,  5764,  9704,\n",
      "           310,  6631,  1196, 24611,   470, 12770, 29889, 29871,   450, 13791,\n",
      "           310,   278, 29807,   526,  8429,   988,  1023, 12770,  5870, 29889,\n",
      "         29871,  1222,  9422,   310,  1248,  4790,   787,   526, 15090,   351,\n",
      "           787, 29892, 11137,   351,   787, 29892,   322,  4725,   351,   787,\n",
      "         29889, 29871,  3139, 10694,   393,   947,   451,  1712, 12770,   470,\n",
      "         13791,   338,   451,   263, 29807, 29889, 29871,   530,  1342,   310,\n",
      "           263,  1661, 29899,  3733, 17125,   338,   263,  8607, 29889, 32007,\n",
      "         32000, 32000, 32000, 32000, 32000, 32000],\n",
      "        [32010,  1128,   437,   306,  1369,  2734, 29973, 32007, 32001,  8561,\n",
      "          1854,   366,   679, 25561,  2734, 17394,   267,   322,  1098,   533,\n",
      "         29889,  7370,   411, 27012,   519,  7306,   297,  3458,   763,   263,\n",
      "         29871, 29945, 29968,  8175, 29889,   960,   366,  2360,  6350,  1434,\n",
      "         29892,  1369, 22020,   515,   263,  6686, 29892,   304,  1506,  3873,\n",
      "          6686, 29892,  3578, 16812, 12242,   292,   363, 29871, 29896, 29945,\n",
      "         29899, 29941, 29900, 29885,  1144, 12919, 29889,   317,   677,   368,\n",
      "          7910,   596,  2734,   931,   322,  5418,   408,   596,  6216,  2264,\n",
      "          3233,  4857,  1960, 29889,  3118,   310,   278,  1556,  4100,  2712,\n",
      "           338, 12528,  1623,   322,  9914, 16116,   292, 29889, 29849, 11621,\n",
      "           304,   596,  3573, 29892,   322,  2125,  1791,  3841,   746,  4312,\n",
      "           304,  5557, 24092, 29889, 32007, 32000]]),\n",
      " 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   319, 29807,\n",
      "           338,   263,   883,   297,  1879,  7843, 29889, 29871,   739,   338,\n",
      "           263,  2323, 22112, 10694,  1754,   310, 16791,  3454,   322,   738,\n",
      "          1353,   310, 13791, 29889, 29871,   739,   338,   263,  5764,  9704,\n",
      "           310,  6631,  1196, 24611,   470, 12770, 29889, 29871,   450, 13791,\n",
      "           310,   278, 29807,   526,  8429,   988,  1023, 12770,  5870, 29889,\n",
      "         29871,  1222,  9422,   310,  1248,  4790,   787,   526, 15090,   351,\n",
      "           787, 29892, 11137,   351,   787, 29892,   322,  4725,   351,   787,\n",
      "         29889, 29871,  3139, 10694,   393,   947,   451,  1712, 12770,   470,\n",
      "         13791,   338,   451,   263, 29807, 29889, 29871,   530,  1342,   310,\n",
      "           263,  1661, 29899,  3733, 17125,   338,   263,  8607, 29889, 32007,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  8561,\n",
      "          1854,   366,   679, 25561,  2734, 17394,   267,   322,  1098,   533,\n",
      "         29889,  7370,   411, 27012,   519,  7306,   297,  3458,   763,   263,\n",
      "         29871, 29945, 29968,  8175, 29889,   960,   366,  2360,  6350,  1434,\n",
      "         29892,  1369, 22020,   515,   263,  6686, 29892,   304,  1506,  3873,\n",
      "          6686, 29892,  3578, 16812, 12242,   292,   363, 29871, 29896, 29945,\n",
      "         29899, 29941, 29900, 29885,  1144, 12919, 29889,   317,   677,   368,\n",
      "          7910,   596,  2734,   931,   322,  5418,   408,   596,  6216,  2264,\n",
      "          3233,  4857,  1960, 29889,  3118,   310,   278,  1556,  4100,  2712,\n",
      "           338, 12528,  1623,   322,  9914, 16116,   292, 29889, 29849, 11621,\n",
      "           304,   596,  3573, 29892,   322,  2125,  1791,  3841,   746,  4312,\n",
      "           304,  5557, 24092, 29889, 32007,  -100]])}\n"
     ]
    }
   ],
   "source": [
    "# 展示 DataCollatorForCompletionOnlyLM 的輸出, 標籤以 -100 表示在損失函數中不會被考慮\n",
    "batch = data_collator([instruction_completion_formatter(dataset[i], True) for i in range(first_n_data)])\n",
    "pprint(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"input: ['<|user|>', 'What', 'is', 'a', 'polygon', '?', '<|end|>', \"\n",
      " \"'<|assistant|>', 'A', 'polygon', 'is', 'a', 'form', 'in', 'Ge', 'ometry', \"\n",
      " \"'.', '', 'It', 'is', 'a', 'single', 'dimensional', 'plane', 'made', 'of', \"\n",
      " \"'connecting', 'lines', 'and', 'any', 'number', 'of', 'vertices', '.', '', \"\n",
      " \"'It', 'is', 'a', 'closed', 'chain', 'of', 'connected', 'line', 'segments', \"\n",
      " \"'or', 'edges', '.', '', 'The', 'vertices', 'of', 'the', 'polygon', 'are', \"\n",
      " \"'formed', 'where', 'two', 'edges', 'meet', '.', '', 'Ex', 'amples', 'of', \"\n",
      " \"'pol', 'yg', 'ons', 'are', 'hex', 'ag', 'ons', ',', 'pent', 'ag', 'ons', \"\n",
      " \"',', 'and', 'oct', 'ag', 'ons', '.', '', 'Any', 'plane', 'that', 'does', \"\n",
      " \"'not', 'contain', 'edges', 'or', 'vertices', 'is', 'not', 'a', 'polygon', \"\n",
      " \"'.', '', 'An', 'example', 'of', 'a', 'non', '-', 'pol', 'ygon', 'is', 'a', \"\n",
      " \"'circle', '.', '<|end|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', \"\n",
      " \"'<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\")\n",
      "(\"label: ['-', '-', '-', '-', '-', '-', '-', '-', 'A', 'polygon', 'is', 'a', \"\n",
      " \"'form', 'in', 'Ge', 'ometry', '.', '', 'It', 'is', 'a', 'single', \"\n",
      " \"'dimensional', 'plane', 'made', 'of', 'connecting', 'lines', 'and', 'any', \"\n",
      " \"'number', 'of', 'vertices', '.', '', 'It', 'is', 'a', 'closed', 'chain', \"\n",
      " \"'of', 'connected', 'line', 'segments', 'or', 'edges', '.', '', 'The', \"\n",
      " \"'vertices', 'of', 'the', 'polygon', 'are', 'formed', 'where', 'two', \"\n",
      " \"'edges', 'meet', '.', '', 'Ex', 'amples', 'of', 'pol', 'yg', 'ons', 'are', \"\n",
      " \"'hex', 'ag', 'ons', ',', 'pent', 'ag', 'ons', ',', 'and', 'oct', 'ag', \"\n",
      " \"'ons', '.', '', 'Any', 'plane', 'that', 'does', 'not', 'contain', 'edges', \"\n",
      " \"'or', 'vertices', 'is', 'not', 'a', 'polygon', '.', '', 'An', 'example', \"\n",
      " \"'of', 'a', 'non', '-', 'pol', 'ygon', 'is', 'a', 'circle', '.', '<|end|>', \"\n",
      " \"'-', '-', '-', '-', '-', '-']\")\n",
      "(\"input: ['<|user|>', 'How', 'do', 'I', 'start', 'running', '?', '<|end|>', \"\n",
      " \"'<|assistant|>', 'Make', 'sure', 'you', 'get', 'comfortable', 'running', \"\n",
      " \"'sho', 'es', 'and', 'att', 'ire', '.', 'Start', 'with', 'achiev', 'able', \"\n",
      " \"'goal', 'in', 'mind', 'like', 'a', '', '5', 'K', 'race', '.', 'If', 'you', \"\n",
      " \"'never', 'ran', 'before', ',', 'start', 'gradually', 'from', 'a', 'walk', \"\n",
      " \"',', 'to', 'br', 'isk', 'walk', ',', 'light', 'jog', 'aim', 'ing', 'for', \"\n",
      " \"'', '1', '5', '-', '3', '0', 'm', 'ins', 'initially', '.', 'S', 'low', 'ly', \"\n",
      " \"'increase', 'your', 'running', 'time', 'and', 'distance', 'as', 'your', \"\n",
      " \"'fit', 'ness', 'level', 'impro', 'ves', '.', 'One', 'of', 'the', 'most', \"\n",
      " \"'important', 'things', 'is', 'cool', 'down', 'and', 'gentle', 'stretch', \"\n",
      " \"'ing', '.', 'Always', 'listen', 'to', 'your', 'body', ',', 'and', 'take', \"\n",
      " \"'rest', 'days', 'when', 'needed', 'to', 'prevent', 'injury', '.', '<|end|>', \"\n",
      " \"'<|endoftext|>']\")\n",
      "(\"label: ['-', '-', '-', '-', '-', '-', '-', '-', '-', 'Make', 'sure', 'you', \"\n",
      " \"'get', 'comfortable', 'running', 'sho', 'es', 'and', 'att', 'ire', '.', \"\n",
      " \"'Start', 'with', 'achiev', 'able', 'goal', 'in', 'mind', 'like', 'a', '', \"\n",
      " \"'5', 'K', 'race', '.', 'If', 'you', 'never', 'ran', 'before', ',', 'start', \"\n",
      " \"'gradually', 'from', 'a', 'walk', ',', 'to', 'br', 'isk', 'walk', ',', \"\n",
      " \"'light', 'jog', 'aim', 'ing', 'for', '', '1', '5', '-', '3', '0', 'm', \"\n",
      " \"'ins', 'initially', '.', 'S', 'low', 'ly', 'increase', 'your', 'running', \"\n",
      " \"'time', 'and', 'distance', 'as', 'your', 'fit', 'ness', 'level', 'impro', \"\n",
      " \"'ves', '.', 'One', 'of', 'the', 'most', 'important', 'things', 'is', 'cool', \"\n",
      " \"'down', 'and', 'gentle', 'stretch', 'ing', '.', 'Always', 'listen', 'to', \"\n",
      " \"'your', 'body', ',', 'and', 'take', 'rest', 'days', 'when', 'needed', 'to', \"\n",
      " \"'prevent', 'injury', '.', '<|end|>', '-']\")\n"
     ]
    }
   ],
   "source": [
    "# 透過 Tokenizer 的 decode 方法將 ID 轉換回文字，並列標籤顯示出來\n",
    "for idx in range(first_n_data):\n",
    "  input_ids = batch['input_ids'][idx]\n",
    "  labels_ids = batch['labels'][idx]\n",
    "  input = [tokenizer.decode(id) for id in input_ids]\n",
    "  labels = ['-'] * len(input_ids)\n",
    "  for i, id in enumerate(labels_ids):\n",
    "    if id != -100:\n",
    "      labels[i] = tokenizer.decode(id)\n",
    "  pprint(f'input: {input}')\n",
    "  pprint(f'label: {labels}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以清楚觀察到，損失函數不會去關注包含 `<|assistant|>` 之前的部分，這樣可以讓模型專注於生成 `<|assistant|>` 之後的回應。\n",
    "\n",
    "考量批次訓練不同長度的序列需要填充到相同的長度，以便能夠在同一批次中進行處理。Data collator 自動序列進行適當的填充，填充的部分亦不會參與損失計算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練參數設定\n",
    "\n",
    "用於設定訓練過程中的各種參數，如學習率、批次大小、梯度累積步數、訓練 epoch 數、權重衰減等。\n",
    "\n",
    "* `output_dir` 指定了訓練輸出的目錄。\n",
    "* `eval_strategy` 和 `save_strategy` 設定為 'epoch'，表示每個 epoch 都會進行評估和儲存。\n",
    "* `load_best_model_at_end` 設定為 `True`，表示訓練結束後會載入最佳模型。\n",
    "* `report_to` 設定為 'none'，禁用了 wandb 報告。\n",
    "* `adam_epsilon` 設定了 Adam 優化器的 epsilon 值。\n",
    "* `packing` 設定為 `False`，當使用 `DataCollatorForCompletionOnlyLM` 時禁用 packing。\n",
    "* `save_total_limit` 設定了最多儲存 5 個 checkpoints。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "training_args = SFTConfig(\n",
    "  output_dir='sample_data/train_output_qa', # 訓練輸出目錄\n",
    "  learning_rate=config.lr, # 學習率\n",
    "  per_device_train_batch_size=config.batch_size, # 每個設備的訓練批次大小\n",
    "  per_device_eval_batch_size=config.batch_size, # 每個設備的評估批次大小\n",
    "  gradient_accumulation_steps=config.gradient_accumulation_steps, # 梯度累積步數\n",
    "  num_train_epochs=config.epochs, # 訓練的總 epoch 數\n",
    "  weight_decay=config.weight_decay, # 權重衰減\n",
    "  eval_strategy='epoch', # 每個 epoch 評估一次\n",
    "  save_strategy='epoch', # 每個 epoch 儲存一次\n",
    "  load_best_model_at_end=True, # 訓練完後載入最佳模型\n",
    "  report_to='none', # 禁用 wandb 報告 (Colab 環境預設需要 wandb)\n",
    "  adam_epsilon=config.adam_epsilon, # 當使用半精度浮點數時，需要設定較大的 adam epsilon\n",
    "  packing=False, # 當使用 DataCollatorForCompletionOnlyLM 時禁用 packing\n",
    "  save_total_limit=5, # 最多儲存 5 個 checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練器初始化\n",
    "\n",
    "用於初始化訓練器，並開始訓練模型。\n",
    "\n",
    "* `model` 是要訓練的模型。\n",
    "* `tokenizer` 是用於處理文本的分詞器。\n",
    "* `train_dataset` 和 `eval_dataset` 是訓練和評估數據集。\n",
    "* `formatting_func` 是用於格式化數據的函數。\n",
    "* `data_collator` 是用於整理數據的數據整理器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j8/hkxkfjqd58j9t_718vfr4tjw0000gn/T/ipykernel_38818/3065814488.py:1: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=peft_model, # 要訓練的模型\n",
    "    tokenizer=tokenizer, # 使用的分詞器\n",
    "    args=training_args, # 訓練參數\n",
    "    train_dataset=dataset, # 訓練數據集\n",
    "    eval_dataset=dataset, # 評估數據集\n",
    "    formatting_func=instruction_completion_formatter, # 格式化函數\n",
    "    data_collator=data_collator, # 數據整理器\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 開始訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 42:32, Epoch 47/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.436862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.876358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.387336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.156587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.084450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.065174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.035158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.015012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.005875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.006033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.001445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.161100</td>\n",
       "      <td>0.000265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.16114239501953126, metrics={'train_runtime': 2556.785, 'train_samples_per_second': 0.821, 'train_steps_per_second': 0.196, 'total_flos': 5647413307244544.0, 'train_loss': 0.16114239501953126, 'epoch': 47.61904761904762})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 開始訓練，這可能需要一些時間\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練完成後，您可以通過運行 `Trainer.evaluate()` 查看最終的分數:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存 LoRA 模型參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 保存 Lora 参数\n",
    "peft_model.save_pretrained(\n",
    "  config.saved_lora_path,\n",
    "  # warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
    "  save_embedding_layers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('sample_data/saved_encoder_model/tokenizer_config.json',\n",
       " 'sample_data/saved_encoder_model/special_tokens_map.json',\n",
       " 'sample_data/saved_encoder_model/tokenizer.json')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存 Tokenizer\n",
    "tokenizer.save_pretrained(config.saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 釋放資源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import garbage collector\n",
    "import gc\n",
    "\n",
    "# 釋放 GPU 記憶體\n",
    "del trainer\n",
    "del tokenizer\n",
    "\n",
    "peft_model.to('cpu')\n",
    "del peft_model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評估微調模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入微調分詞器 (Tokenizer)\n",
    "\n",
    "從已經完成訓練的模型取得 Tokenizer，可以留意這個訓練時保存下來的 Tokenizer 仍保有訓練時的設定，包涵 `pad_token` 和 `padding_side`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  config.saved_model_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<|endoftext|>'\n"
     ]
    }
   ],
   "source": [
    "# 檢視 Tokenizer 是否存在 padding token 及 padding side 等資訊\n",
    "pprint(tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'right'\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入微調後模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "ft_model = PeftModel.from_pretrained(\n",
    "  model, # 預訓練模型\n",
    "  config.saved_lora_path, # LoRA 適配模型\n",
    "  # 這個參數用於優化內存使用，減少模型加載時的 CPU 內存佔用，特別是在內存有限的環境中非常有用。\n",
    "  low_cpu_mem_usage=True,\n",
    "  torch_dtype=config.torch_dtype,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning 後的表現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonas/.pyenv/versions/learning-transformer-with-huggingface-3.11.11/lib/python3.11/site-packages/dill/_dill.py:414: PicklingWarning: Cannot locate reference to <class '__main__.Config'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "/Users/jonas/.pyenv/versions/learning-transformer-with-huggingface-3.11.11/lib/python3.11/site-packages/dill/_dill.py:414: PicklingWarning: Cannot pickle <class '__main__.Config'>: __main__.Config has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "Map:   0%|                                                                              | 0/2 [00:00<?, ? examples/s]/Users/jonas/.pyenv/versions/learning-transformer-with-huggingface-3.11.11/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/Users/jonas/.pyenv/versions/learning-transformer-with-huggingface-3.11.11/lib/python3.11/site-packages/transformers/pytorch_utils.py:325: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████████| 2/2 [01:02<00:00, 31.17s/ examples]\n"
     ]
    }
   ],
   "source": [
    "# 透過微調模型生成回應，將其新增到 first_n_dataset 的 ft_response 欄位中\n",
    "first_n_dataset = first_n_dataset.map(\n",
    "  lambda x: {\n",
    "    **x,\n",
    "    \"ft_response\": generator(x, ft_model),\n",
    "  },\n",
    "  batched=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>assistant</th>\n",
       "      <th>pt_response</th>\n",
       "      <th>ft_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'content': 'What is a polygon?', 'role': 'user'}</td>\n",
       "      <td>{'content': 'A polygon is a form in Geometry.  It is a single dimensional plane made of connecting lines and any number of vertices.  It is a closed chain of connected line segments or edges.  The vertices of the polygon are formed where two edges meet.  Examples of polygons are hexagons, pentagons, and octagons.  Any plane that does not contain edges or vertices is not a polygon.  An example of a non-polygon is a circle.', 'role': 'assistant'}</td>\n",
       "      <td>A **polygon** refers to any two-dimensional geometric figure that consists of straight, nonintersecting line segments or \"sides\" which enclose spaces. These sides are connected at their endpoints called vertices (singular: vertex). The interior region formed by these lines and the enclosed space inside it can be filled with color if desired for visualization purposes; this area within but not including its boundary defines what we call 'the plane' in geometry terms when discuss about polygons specifically on flat surfaces like paper maps etc., though technically speaking they exist everywhere as long there’re no curved</td>\n",
       "      <td>A polygon is a form in Geometry.  It is a single dimensional plane made of connecting lines and any number of vertices.  It has sides which are formed by joining two adjacent vertices together.  Examples of polygons are hexagons, pentagon, or octogon.  Any closed chain of connected line segments or edges makes up the boundary of a polygon.  The interior space within this boundary is called one side of the Polygon.    Although it may seem strange, but anti-polyglots exist who dislike Polygon shapes.\\nAlthough technically starting from Medieval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'content': 'How do I start running?', 'role': 'user'}</td>\n",
       "      <td>{'content': 'Make sure you get comfortable running shoes and attire. Start with achievable goal in mind like a 5K race. If you never ran before, start gradually from a walk, to brisk walk, light jog aiming for 15-30mins initially. Slowly increase your running time and distance as your fitness level improves. One of the most important things is cool down and gentle stretching. Always listen to your body, and take rest days when needed to prevent injury.', 'role': 'assistant'}</td>\n",
       "      <td>Starting to run is a great way for improving your fitness, mental health and overall well-being. Here's how you can begin:\\n 1) Set realistic goals - Determine what distance or time frame works best with the current level of physical activity in mind (either beginner mileage guidelines like Couch25K/Coworkout30k if starting from scratch). Remember that progress takes patience!  \\t    * Choose appropriate footwear – Investment into good quality shoes will help prevent injuries downline by providing proper support during</td>\n",
       "      <td>Make sure you get comfortable shoes and attire. Start with achievable goal in mind like a 5K race. If you never ran before, gradually build up your training program starting slow miles such as1-2 easy mileutes until reaching target distancegoal for the day or week. Include rest days to recover\\n. One of the most important things is warmup and cool down routines. Always listen to your body's signals and take breaks when needed. Cool Down: Slow jogging walk followed by stretching exercises focusing on legs and lower back. Warm Up: E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     user  \\\n",
       "0       {'content': 'What is a polygon?', 'role': 'user'}   \n",
       "1  {'content': 'How do I start running?', 'role': 'user'}   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         assistant  \\\n",
       "0                                 {'content': 'A polygon is a form in Geometry.  It is a single dimensional plane made of connecting lines and any number of vertices.  It is a closed chain of connected line segments or edges.  The vertices of the polygon are formed where two edges meet.  Examples of polygons are hexagons, pentagons, and octagons.  Any plane that does not contain edges or vertices is not a polygon.  An example of a non-polygon is a circle.', 'role': 'assistant'}   \n",
       "1  {'content': 'Make sure you get comfortable running shoes and attire. Start with achievable goal in mind like a 5K race. If you never ran before, start gradually from a walk, to brisk walk, light jog aiming for 15-30mins initially. Slowly increase your running time and distance as your fitness level improves. One of the most important things is cool down and gentle stretching. Always listen to your body, and take rest days when needed to prevent injury.', 'role': 'assistant'}   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          pt_response  \\\n",
       "0  A **polygon** refers to any two-dimensional geometric figure that consists of straight, nonintersecting line segments or \"sides\" which enclose spaces. These sides are connected at their endpoints called vertices (singular: vertex). The interior region formed by these lines and the enclosed space inside it can be filled with color if desired for visualization purposes; this area within but not including its boundary defines what we call 'the plane' in geometry terms when discuss about polygons specifically on flat surfaces like paper maps etc., though technically speaking they exist everywhere as long there’re no curved   \n",
       "1                                                                                                        Starting to run is a great way for improving your fitness, mental health and overall well-being. Here's how you can begin:\\n 1) Set realistic goals - Determine what distance or time frame works best with the current level of physical activity in mind (either beginner mileage guidelines like Couch25K/Coworkout30k if starting from scratch). Remember that progress takes patience!  \\t    * Choose appropriate footwear – Investment into good quality shoes will help prevent injuries downline by providing proper support during   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ft_response  \n",
       "0  A polygon is a form in Geometry.  It is a single dimensional plane made of connecting lines and any number of vertices.  It has sides which are formed by joining two adjacent vertices together.  Examples of polygons are hexagons, pentagon, or octogon.  Any closed chain of connected line segments or edges makes up the boundary of a polygon.  The interior space within this boundary is called one side of the Polygon.    Although it may seem strange, but anti-polyglots exist who dislike Polygon shapes.\\nAlthough technically starting from Medieval  \n",
       "1              Make sure you get comfortable shoes and attire. Start with achievable goal in mind like a 5K race. If you never ran before, gradually build up your training program starting slow miles such as1-2 easy mileutes until reaching target distancegoal for the day or week. Include rest days to recover\\n. One of the most important things is warmup and cool down routines. Always listen to your body's signals and take breaks when needed. Cool Down: Slow jogging walk followed by stretching exercises focusing on legs and lower back. Warm Up: E  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 顯示微調模型預測結果\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.DataFrame(first_n_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Download files from Colab workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "metadata": {},
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "![[ ! -z \"${COLAB_GPU}\" ]] && tar cvzf saved_encoder_model.tgz sample_data/saved_encoder_model/\n",
    "![[ ! -z \"${COLAB_GPU}\" ]] && tar cvzf saved_lora_model.tgz sample_data/saved_lora_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "  from google.colab import files\n",
    "  files.download('saved_encoder_model.tgz')\n",
    "  files.download('saved_lora_model.tgz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
